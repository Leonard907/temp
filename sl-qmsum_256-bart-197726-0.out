2022-06-15 06:34:46 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2022-06-15 06:34:48 | WARNING | __main__ | Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
2022-06-15 06:34:48 | INFO | __main__ | Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.98,
adam_epsilon=1e-06,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fp16_padding=False,
gradient_accumulation_steps=16,
greater_is_better=False,
group_by_length=True,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/allenai-led-base-16384_global_1024_16_1e-3_4096_scrolls_qmsum_coffee-second-33/runs/Jun15_06-34-47_arnold.inf.ed.ac.uk,
logging_first_step=False,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=20.0,
output_dir=/disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/allenai-led-base-16384_global_1024_16_1e-3_4096_scrolls_qmsum_coffee-second-33,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=8,
per_device_train_max_batch_size=None,
predict_with_generate=False,
prediction_loss_only=True,
push_to_hub=False,
push_to_hub_model_id=allenai-led-base-16384_global_1024_16_1e-3_4096_scrolls_qmsum_coffee-second-33,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=allenai-led-base-16384_global_1024_16_1e-3_4096_scrolls_qmsum_coffee-second-33,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_max_tokens=4096,
use_legacy_prediction_loop=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.001,
)
2022-06-15 06:34:50 | INFO | datasets.utils.filelock | Lock 140087496999360 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py.lock
2022-06-15 06:34:50 | INFO | datasets.utils.file_utils | https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py not found in cache or force_download set to True, downloading to /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/experiments/data/qmsum_led-1024/huggingface/datasets/downloads/tmp_uil9kc8
Downloading:   0%|          | 0.00/18.5k [00:00<?, ?B/s]Downloading: 100%|██████████| 18.5k/18.5k [00:00<00:00, 207kB/s]
2022-06-15 06:34:50 | INFO | datasets.utils.file_utils | storing https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py in cache at experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py
2022-06-15 06:34:50 | INFO | datasets.utils.file_utils | creating metadata file for experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py
2022-06-15 06:34:50 | INFO | datasets.utils.filelock | Lock 140087496999360 released on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py.lock
2022-06-15 06:34:51 | INFO | datasets.load | Checking experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py for additional imports.
2022-06-15 06:34:51 | INFO | datasets.utils.filelock | Lock 140087496999648 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py.lock
2022-06-15 06:34:51 | INFO | datasets.load | Creating main folder for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls
2022-06-15 06:34:51 | INFO | datasets.load | Creating specific version folder for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
2022-06-15 06:34:51 | INFO | datasets.load | Copying script file from https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py to experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac/scrolls.py
2022-06-15 06:34:51 | INFO | datasets.load | Couldn't find dataset infos file at https://huggingface.co/datasets/tau/scrolls/resolve/main/dataset_infos.json
2022-06-15 06:34:51 | INFO | datasets.load | Creating metadata file for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac/scrolls.json
2022-06-15 06:34:51 | INFO | datasets.utils.filelock | Lock 140087496999648 released on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py.lock
2022-06-15 06:34:52 | INFO | datasets.utils.filelock | Lock 140087496999360 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 06:34:52 | INFO | datasets.utils.filelock | Lock 140087496999360 released on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 06:34:52 | INFO | datasets.utils.filelock | Lock 140087497002288 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 06:34:52 | INFO | datasets.builder | Generating dataset scrolls (experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)
2022-06-15 06:34:52 | INFO | datasets.builder | Dataset not on Hf google storage. Downloading and preparing it from source
2022-06-15 06:34:53 | INFO | datasets.utils.filelock | Lock 140087496601408 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/641a07da07fe32597e78e307ebcf6e41920db618be347fa0265b5fe1024dc05f.lock
2022-06-15 06:34:53 | INFO | datasets.utils.file_utils | https://scrolls-tau.s3.us-east-2.amazonaws.com/qmsum.zip not found in cache or force_download set to True, downloading to /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/experiments/data/qmsum_led-1024/huggingface/datasets/downloads/tmptd8_9vyo
Downloading:   0%|          | 0.00/27.3M [00:00<?, ?B/s]Downloading:   0%|          | 52.2k/27.3M [00:00<01:43, 264kB/s]Downloading:   1%|          | 141k/27.3M [00:00<01:21, 335kB/s] Downloading:   1%|          | 249k/27.3M [00:00<01:04, 422kB/s]Downloading:   1%|▏         | 355k/27.3M [00:00<00:52, 515kB/s]Downloading:   2%|▏         | 461k/27.3M [00:00<00:44, 608kB/s]Downloading:   2%|▏         | 568k/27.3M [00:00<00:38, 699kB/s]Downloading:   2%|▏         | 679k/27.3M [00:00<00:33, 785kB/s]Downloading:   3%|▎         | 801k/27.3M [00:00<00:30, 876kB/s]Downloading:   3%|▎         | 924k/27.3M [00:01<00:27, 959kB/s]Downloading:   4%|▍         | 1.05M/27.3M [00:01<00:25, 1.03MB/s]Downloading:   4%|▍         | 1.18M/27.3M [00:01<00:23, 1.10MB/s]Downloading:   5%|▍         | 1.31M/27.3M [00:01<00:22, 1.14MB/s]Downloading:   5%|▌         | 1.43M/27.3M [00:01<00:22, 1.16MB/s]Downloading:   6%|▌         | 1.55M/27.3M [00:01<00:21, 1.19MB/s]Downloading:   6%|▌         | 1.68M/27.3M [00:01<00:21, 1.20MB/s]Downloading:   7%|▋         | 1.81M/27.3M [00:01<00:20, 1.23MB/s]Downloading:   7%|▋         | 1.95M/27.3M [00:01<00:19, 1.27MB/s]Downloading:   8%|▊         | 2.09M/27.3M [00:01<00:19, 1.32MB/s]Downloading:   8%|▊         | 2.23M/27.3M [00:02<00:19, 1.32MB/s]Downloading:   9%|▊         | 2.37M/27.3M [00:02<00:18, 1.35MB/s]Downloading:   9%|▉         | 2.51M/27.3M [00:02<00:18, 1.35MB/s]Downloading:  10%|▉         | 2.65M/27.3M [00:02<00:18, 1.36MB/s]Downloading:  10%|█         | 2.78M/27.3M [00:02<00:38, 631kB/s] Downloading:  11%|█         | 2.89M/27.3M [00:03<00:47, 510kB/s]Downloading:  11%|█         | 2.98M/27.3M [00:03<00:41, 584kB/s]Downloading:  11%|█         | 3.06M/27.3M [00:04<01:48, 223kB/s]Downloading:  11%|█▏        | 3.13M/27.3M [00:04<01:50, 219kB/s]Downloading:  12%|█▏        | 3.18M/27.3M [00:04<01:31, 263kB/s]Downloading:  12%|█▏        | 3.25M/27.3M [00:04<01:14, 323kB/s]Downloading:  12%|█▏        | 3.30M/27.3M [00:04<01:17, 309kB/s]Downloading:  12%|█▏        | 3.35M/27.3M [00:05<01:38, 242kB/s]Downloading:  12%|█▏        | 3.39M/27.3M [00:05<01:45, 226kB/s]Downloading:  13%|█▎        | 3.42M/27.3M [00:05<01:57, 203kB/s]Downloading:  13%|█▎        | 3.45M/27.3M [00:05<02:12, 181kB/s]Downloading:  13%|█▎        | 3.48M/27.3M [00:05<02:01, 196kB/s]Downloading:  13%|█▎        | 3.50M/27.3M [00:05<01:54, 208kB/s]Downloading:  13%|█▎        | 3.53M/27.3M [00:06<02:02, 194kB/s]Downloading:  13%|█▎        | 3.57M/27.3M [00:06<01:46, 223kB/s]Downloading:  13%|█▎        | 3.60M/27.3M [00:06<01:42, 230kB/s]Downloading:  13%|█▎        | 3.64M/27.3M [00:06<01:44, 227kB/s]Downloading:  13%|█▎        | 3.67M/27.3M [00:06<01:33, 252kB/s]Downloading:  14%|█▎        | 3.71M/27.3M [00:06<01:33, 251kB/s]Downloading:  14%|█▎        | 3.74M/27.3M [00:06<01:37, 242kB/s]Downloading:  14%|█▍        | 3.78M/27.3M [00:07<01:28, 265kB/s]Downloading:  14%|█▍        | 3.81M/27.3M [00:07<01:22, 285kB/s]Downloading:  14%|█▍        | 3.85M/27.3M [00:07<01:17, 301kB/s]Downloading:  14%|█▍        | 3.88M/27.3M [00:07<01:14, 312kB/s]Downloading:  14%|█▍        | 3.92M/27.3M [00:07<01:20, 292kB/s]Downloading:  14%|█▍        | 3.95M/27.3M [00:07<01:16, 307kB/s]Downloading:  15%|█▍        | 3.99M/27.3M [00:07<01:12, 320kB/s]Downloading:  15%|█▍        | 4.02M/27.3M [00:07<01:09, 333kB/s]Downloading:  15%|█▍        | 4.06M/27.3M [00:07<01:06, 351kB/s]Downloading:  15%|█▌        | 4.10M/27.3M [00:07<01:05, 353kB/s]Downloading:  15%|█▌        | 4.14M/27.3M [00:08<01:03, 366kB/s]Downloading:  15%|█▌        | 4.18M/27.3M [00:08<01:02, 369kB/s]Downloading:  15%|█▌        | 4.22M/27.3M [00:08<01:02, 372kB/s]Downloading:  16%|█▌        | 4.26M/27.3M [00:08<01:00, 380kB/s]Downloading:  16%|█▌        | 4.30M/27.3M [00:08<01:06, 344kB/s]Downloading:  16%|█▌        | 4.35M/27.3M [00:08<01:00, 382kB/s]Downloading:  16%|█▌        | 4.40M/27.3M [00:08<01:00, 379kB/s]Downloading:  16%|█▋        | 4.46M/27.3M [00:08<00:55, 412kB/s]Downloading:  17%|█▋        | 4.51M/27.3M [00:09<00:57, 400kB/s]Downloading:  17%|█▋        | 4.56M/27.3M [00:09<00:55, 412kB/s]Downloading:  17%|█▋        | 4.61M/27.3M [00:09<00:51, 439kB/s]Downloading:  17%|█▋        | 4.67M/27.3M [00:09<00:49, 458kB/s]Downloading:  17%|█▋        | 4.72M/27.3M [00:09<00:52, 431kB/s]Downloading:  17%|█▋        | 4.77M/27.3M [00:09<00:48, 462kB/s]Downloading:  18%|█▊        | 4.83M/27.3M [00:09<00:46, 481kB/s]Downloading:  18%|█▊        | 4.88M/27.3M [00:09<00:46, 486kB/s]Downloading:  18%|█▊        | 4.93M/27.3M [00:09<00:43, 509kB/s]Downloading:  18%|█▊        | 4.99M/27.3M [00:09<00:43, 516kB/s]Downloading:  18%|█▊        | 5.04M/27.3M [00:10<00:43, 516kB/s]Downloading:  19%|█▊        | 5.10M/27.3M [00:10<00:47, 472kB/s]Downloading:  19%|█▉        | 5.16M/27.3M [00:10<00:44, 494kB/s]Downloading:  19%|█▉        | 5.22M/27.3M [00:10<00:41, 532kB/s]Downloading:  19%|█▉        | 5.28M/27.3M [00:10<00:40, 538kB/s]Downloading:  20%|█▉        | 5.34M/27.3M [00:10<00:42, 519kB/s]Downloading:  20%|█▉        | 5.41M/27.3M [00:10<00:39, 559kB/s]Downloading:  20%|██        | 5.48M/27.3M [00:10<00:40, 540kB/s]Downloading:  20%|██        | 5.55M/27.3M [00:11<00:37, 579kB/s]Downloading:  21%|██        | 5.61M/27.3M [00:11<00:36, 586kB/s]Downloading:  21%|██        | 5.69M/27.3M [00:11<00:34, 622kB/s]Downloading:  21%|██        | 5.76M/27.3M [00:11<00:32, 655kB/s]Downloading:  21%|██▏       | 5.83M/27.3M [00:11<00:32, 669kB/s]Downloading:  22%|██▏       | 5.92M/27.3M [00:11<00:29, 717kB/s]Downloading:  22%|██▏       | 6.01M/27.3M [00:11<00:28, 757kB/s]Downloading:  22%|██▏       | 6.10M/27.3M [00:11<00:26, 795kB/s]Downloading:  23%|██▎       | 6.20M/27.3M [00:11<00:25, 821kB/s]Downloading:  23%|██▎       | 6.30M/27.3M [00:11<00:24, 874kB/s]Downloading:  23%|██▎       | 6.41M/27.3M [00:12<00:22, 919kB/s]Downloading:  24%|██▍       | 6.51M/27.3M [00:12<00:21, 956kB/s]Downloading:  24%|██▍       | 6.63M/27.3M [00:12<00:20, 1.02MB/s]Downloading:  25%|██▍       | 6.75M/27.3M [00:12<00:19, 1.07MB/s]Downloading:  25%|██▌       | 6.88M/27.3M [00:12<00:18, 1.11MB/s]Downloading:  26%|██▌       | 7.02M/27.3M [00:12<00:18, 1.08MB/s]Downloading:  26%|██▌       | 7.16M/27.3M [00:12<00:17, 1.16MB/s]Downloading:  27%|██▋       | 7.30M/27.3M [00:12<00:16, 1.22MB/s]Downloading:  27%|██▋       | 7.45M/27.3M [00:12<00:15, 1.30MB/s]Downloading:  28%|██▊       | 7.61M/27.3M [00:12<00:14, 1.38MB/s]Downloading:  28%|██▊       | 7.77M/27.3M [00:13<00:13, 1.43MB/s]Downloading:  29%|██▉       | 7.94M/27.3M [00:13<00:12, 1.50MB/s]Downloading:  30%|██▉       | 8.11M/27.3M [00:13<00:12, 1.57MB/s]Downloading:  30%|███       | 8.29M/27.3M [00:13<00:11, 1.62MB/s]Downloading:  31%|███       | 8.48M/27.3M [00:13<00:11, 1.70MB/s]Downloading:  32%|███▏      | 8.69M/27.3M [00:13<00:10, 1.79MB/s]Downloading:  33%|███▎      | 8.90M/27.3M [00:13<00:09, 1.87MB/s]Downloading:  33%|███▎      | 9.11M/27.3M [00:13<00:09, 1.95MB/s]Downloading:  34%|███▍      | 9.33M/27.3M [00:13<00:08, 2.01MB/s]Downloading:  35%|███▌      | 9.56M/27.3M [00:13<00:08, 2.08MB/s]Downloading:  36%|███▌      | 9.78M/27.3M [00:14<00:08, 2.12MB/s]Downloading:  37%|███▋      | 10.0M/27.3M [00:14<00:07, 2.17MB/s]Downloading:  38%|███▊      | 10.2M/27.3M [00:14<00:07, 2.18MB/s]Downloading:  38%|███▊      | 10.5M/27.3M [00:14<00:07, 2.21MB/s]Downloading:  39%|███▉      | 10.7M/27.3M [00:14<00:07, 2.23MB/s]Downloading:  40%|████      | 10.9M/27.3M [00:14<00:07, 2.30MB/s]Downloading:  41%|████      | 11.2M/27.3M [00:14<00:06, 2.30MB/s]Downloading:  42%|████▏     | 11.4M/27.3M [00:14<00:07, 2.07MB/s]Downloading:  43%|████▎     | 11.6M/27.3M [00:14<00:07, 2.13MB/s]Downloading:  44%|████▎     | 11.9M/27.3M [00:15<00:06, 2.20MB/s]Downloading:  44%|████▍     | 12.1M/27.3M [00:15<00:06, 2.22MB/s]Downloading:  45%|████▌     | 12.3M/27.3M [00:15<00:06, 2.24MB/s]Downloading:  46%|████▌     | 12.6M/27.3M [00:15<00:06, 2.30MB/s]Downloading:  47%|████▋     | 12.8M/27.3M [00:15<00:06, 2.31MB/s]Downloading:  48%|████▊     | 13.0M/27.3M [00:15<00:06, 2.30MB/s]Downloading:  49%|████▊     | 13.3M/27.3M [00:15<00:05, 2.34MB/s]Downloading:  50%|████▉     | 13.5M/27.3M [00:15<00:05, 2.34MB/s]Downloading:  50%|█████     | 13.8M/27.3M [00:15<00:05, 2.34MB/s]Downloading:  51%|█████▏    | 14.0M/27.3M [00:15<00:05, 2.37MB/s]Downloading:  52%|█████▏    | 14.2M/27.3M [00:16<00:05, 2.40MB/s]Downloading:  53%|█████▎    | 14.5M/27.3M [00:16<00:05, 2.39MB/s]Downloading:  54%|█████▍    | 14.7M/27.3M [00:16<00:05, 2.42MB/s]Downloading:  55%|█████▍    | 15.0M/27.3M [00:16<00:05, 2.42MB/s]Downloading:  56%|█████▌    | 15.2M/27.3M [00:16<00:05, 2.41MB/s]Downloading:  57%|█████▋    | 15.5M/27.3M [00:16<00:04, 2.43MB/s]Downloading:  58%|█████▊    | 15.7M/27.3M [00:16<00:04, 2.44MB/s]Downloading:  58%|█████▊    | 16.0M/27.3M [00:16<00:04, 2.45MB/s]Downloading:  59%|█████▉    | 16.2M/27.3M [00:16<00:04, 2.45MB/s]Downloading:  60%|██████    | 16.5M/27.3M [00:16<00:04, 2.41MB/s]Downloading:  61%|██████    | 16.7M/27.3M [00:17<00:04, 2.42MB/s]Downloading:  62%|██████▏   | 17.0M/27.3M [00:17<00:04, 2.44MB/s]Downloading:  63%|██████▎   | 17.2M/27.3M [00:17<00:04, 2.44MB/s]Downloading:  64%|██████▍   | 17.5M/27.3M [00:17<00:03, 2.48MB/s]Downloading:  65%|██████▍   | 17.7M/27.3M [00:17<00:03, 2.48MB/s]Downloading:  66%|██████▌   | 18.0M/27.3M [00:17<00:03, 2.50MB/s]Downloading:  67%|██████▋   | 18.2M/27.3M [00:17<00:03, 2.52MB/s]Downloading:  68%|██████▊   | 18.5M/27.3M [00:17<00:03, 2.56MB/s]Downloading:  69%|██████▊   | 18.8M/27.3M [00:17<00:03, 2.56MB/s]Downloading:  70%|██████▉   | 19.0M/27.3M [00:17<00:03, 2.56MB/s]Downloading:  71%|███████   | 19.3M/27.3M [00:18<00:03, 2.62MB/s]Downloading:  72%|███████▏  | 19.6M/27.3M [00:18<00:02, 2.62MB/s]Downloading:  73%|███████▎  | 19.8M/27.3M [00:18<00:02, 2.65MB/s]Downloading:  74%|███████▎  | 20.1M/27.3M [00:18<00:02, 2.70MB/s]Downloading:  75%|███████▍  | 20.4M/27.3M [00:18<00:02, 2.74MB/s]Downloading:  76%|███████▌  | 20.7M/27.3M [00:18<00:02, 2.75MB/s]Downloading:  77%|███████▋  | 21.0M/27.3M [00:18<00:02, 2.77MB/s]Downloading:  78%|███████▊  | 21.2M/27.3M [00:18<00:02, 2.79MB/s]Downloading:  79%|███████▉  | 21.5M/27.3M [00:18<00:02, 2.83MB/s]Downloading:  80%|████████  | 21.8M/27.3M [00:18<00:01, 2.88MB/s]Downloading:  81%|████████  | 22.1M/27.3M [00:19<00:01, 2.92MB/s]Downloading:  82%|████████▏ | 22.4M/27.3M [00:19<00:01, 2.94MB/s]Downloading:  83%|████████▎ | 22.8M/27.3M [00:19<00:01, 2.99MB/s]Downloading:  85%|████████▍ | 23.1M/27.3M [00:19<00:01, 3.03MB/s]Downloading:  86%|████████▌ | 23.4M/27.3M [00:19<00:01, 3.09MB/s]Downloading:  87%|████████▋ | 23.7M/27.3M [00:19<00:01, 3.11MB/s]Downloading:  88%|████████▊ | 24.0M/27.3M [00:19<00:01, 3.18MB/s]Downloading:  89%|████████▉ | 24.4M/27.3M [00:19<00:00, 3.24MB/s]Downloading:  91%|█████████ | 24.7M/27.3M [00:19<00:00, 3.12MB/s]Downloading:  92%|█████████▏| 25.1M/27.3M [00:19<00:00, 3.22MB/s]Downloading:  93%|█████████▎| 25.5M/27.3M [00:20<00:00, 3.32MB/s]Downloading:  95%|█████████▍| 25.8M/27.3M [00:20<00:00, 3.40MB/s]Downloading:  96%|█████████▌| 26.2M/27.3M [00:20<00:00, 3.51MB/s]Downloading:  97%|█████████▋| 26.6M/27.3M [00:20<00:00, 3.59MB/s]Downloading:  99%|█████████▉| 27.0M/27.3M [00:20<00:00, 3.67MB/s]Downloading: 100%|██████████| 27.3M/27.3M [00:20<00:00, 1.33MB/s]
2022-06-15 06:35:32 | INFO | datasets.utils.file_utils | storing https://scrolls-tau.s3.us-east-2.amazonaws.com/qmsum.zip in cache at experiments/data/qmsum_led-1024/huggingface/datasets/downloads/641a07da07fe32597e78e307ebcf6e41920db618be347fa0265b5fe1024dc05f
2022-06-15 06:35:32 | INFO | datasets.utils.file_utils | creating metadata file for experiments/data/qmsum_led-1024/huggingface/datasets/downloads/641a07da07fe32597e78e307ebcf6e41920db618be347fa0265b5fe1024dc05f
2022-06-15 06:35:32 | INFO | datasets.utils.filelock | Lock 140087496601408 released on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/641a07da07fe32597e78e307ebcf6e41920db618be347fa0265b5fe1024dc05f.lock
2022-06-15 06:35:32 | INFO | datasets.utils.download_manager | Downloading took 0.0 min
2022-06-15 06:35:32 | INFO | datasets.utils.download_manager | Checksum Computation took 0.0 min
2022-06-15 06:35:32 | INFO | datasets.utils.filelock | Lock 140087497087344 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/641a07da07fe32597e78e307ebcf6e41920db618be347fa0265b5fe1024dc05f.lock
2022-06-15 06:36:21 | INFO | datasets.utils.filelock | Lock 140087497087344 released on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/641a07da07fe32597e78e307ebcf6e41920db618be347fa0265b5fe1024dc05f.lock
2022-06-15 06:36:21 | INFO | datasets.builder | Generating split train
0 examples [00:00, ? examples/s]124 examples [00:00, 1239.28 examples/s]394 examples [00:00, 1478.69 examples/s]564 examples [00:00, 1538.17 examples/s]767 examples [00:00, 1658.19 examples/s]979 examples [00:00, 1477.63 examples/s]1119 examples [00:00, 1071.78 examples/s]                                         2022-06-15 06:37:01 | INFO | datasets.arrow_writer | Done writing 1257 examples in 70158328 bytes experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.incomplete/scrolls-train.arrow.
2022-06-15 06:37:01 | INFO | datasets.builder | Generating split validation
0 examples [00:00, ? examples/s]143 examples [00:00, 1428.06 examples/s]                                        2022-06-15 06:37:14 | INFO | datasets.arrow_writer | Done writing 272 examples in 15953268 bytes experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.incomplete/scrolls-validation.arrow.
2022-06-15 06:37:14 | INFO | datasets.builder | Generating split test
0 examples [00:00, ? examples/s]134 examples [00:00, 1337.82 examples/s]                                        2022-06-15 06:37:23 | INFO | datasets.arrow_writer | Done writing 281 examples in 16406660 bytes experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.incomplete/scrolls-test.arrow.
2022-06-15 06:37:23 | INFO | datasets.utils.filelock | Lock 140087497076304 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.incomplete.lock
2022-06-15 06:37:23 | INFO | datasets.utils.filelock | Lock 140087497076304 released on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.incomplete.lock
2022-06-15 06:37:23 | INFO | datasets.utils.filelock | Lock 140087497002288 released on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 06:37:23 | INFO | datasets.builder | Constructing Dataset for split train, validation, test, from experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:01,  1.53it/s] 67%|██████▋   | 2/3 [00:00<00:00,  1.98it/s]100%|██████████| 3/3 [00:00<00:00,  2.49it/s]100%|██████████| 3/3 [00:00<00:00,  3.09it/s]
[INFO|file_utils.py:1632] 2022-06-15 06:37:24,997 >> https://huggingface.co/allenai/led-base-16384/resolve/main/config.json not found in cache or force_download set to True, downloading to /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/experiments/data/qmsum_led-1024/huggingface/transformers/tmp9jl3yxlf
Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]Downloading: 100%|██████████| 1.09k/1.09k [00:00<00:00, 488kB/s]
[INFO|file_utils.py:1636] 2022-06-15 06:37:25,466 >> storing https://huggingface.co/allenai/led-base-16384/resolve/main/config.json in cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|file_utils.py:1644] 2022-06-15 06:37:25,468 >> creating metadata file for experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:561] 2022-06-15 06:37:25,471 >> loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:598] 2022-06-15 06:37:25,477 >> Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": 1024,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": true,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|file_utils.py:1632] 2022-06-15 06:37:25,957 >> https://huggingface.co/allenai/led-base-16384/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/experiments/data/qmsum_led-1024/huggingface/transformers/tmp18vivzic
Downloading:   0%|          | 0.00/27.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 27.0/27.0 [00:00<00:00, 12.7kB/s]
[INFO|file_utils.py:1636] 2022-06-15 06:37:26,371 >> storing https://huggingface.co/allenai/led-base-16384/resolve/main/tokenizer_config.json in cache at experiments/data/qmsum_led-1024/huggingface/transformers/86288ba22bce9550d76e9b26722ee92ae5921ae9285ccbc2904e9a5ad7199b73.cfc08f03f72cde495bd6b3dd3252bca130b3437de370856d084d1453c58b6fea
[INFO|file_utils.py:1644] 2022-06-15 06:37:26,372 >> creating metadata file for experiments/data/qmsum_led-1024/huggingface/transformers/86288ba22bce9550d76e9b26722ee92ae5921ae9285ccbc2904e9a5ad7199b73.cfc08f03f72cde495bd6b3dd3252bca130b3437de370856d084d1453c58b6fea
[INFO|configuration_utils.py:561] 2022-06-15 06:37:26,794 >> loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:598] 2022-06-15 06:37:26,796 >> Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": [
    1024,
    1024,
    1024,
    1024,
    1024,
    1024
  ],
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|file_utils.py:1632] 2022-06-15 06:37:27,638 >> https://huggingface.co/allenai/led-base-16384/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/experiments/data/qmsum_led-1024/huggingface/transformers/tmpch_e2nk7
Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]Downloading:   9%|▉         | 81.9k/899k [00:00<00:01, 470kB/s]Downloading:  42%|████▏     | 377k/899k [00:00<00:00, 600kB/s] Downloading: 100%|██████████| 899k/899k [00:00<00:00, 2.02MB/s]
[INFO|file_utils.py:1636] 2022-06-15 06:37:28,661 >> storing https://huggingface.co/allenai/led-base-16384/resolve/main/vocab.json in cache at experiments/data/qmsum_led-1024/huggingface/transformers/4fb25bb1f9a942a2e2930029211b4a7deaeb18b62f6e5ce6d59730c90da51373.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
[INFO|file_utils.py:1644] 2022-06-15 06:37:28,663 >> creating metadata file for experiments/data/qmsum_led-1024/huggingface/transformers/4fb25bb1f9a942a2e2930029211b4a7deaeb18b62f6e5ce6d59730c90da51373.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
[INFO|file_utils.py:1632] 2022-06-15 06:37:29,075 >> https://huggingface.co/allenai/led-base-16384/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/experiments/data/qmsum_led-1024/huggingface/transformers/tmp9bnltkdp
Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]Downloading:  20%|█▉        | 90.1k/456k [00:00<00:00, 506kB/s]Downloading:  66%|██████▋   | 303k/456k [00:00<00:00, 611kB/s] Downloading: 100%|██████████| 456k/456k [00:00<00:00, 1.25MB/s]
[INFO|file_utils.py:1636] 2022-06-15 06:37:30,371 >> storing https://huggingface.co/allenai/led-base-16384/resolve/main/merges.txt in cache at experiments/data/qmsum_led-1024/huggingface/transformers/087e8f4306cbf22e21907929074344a3b0a46bd680a118eb6267cd5a2bcec5b2.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|file_utils.py:1644] 2022-06-15 06:37:30,372 >> creating metadata file for experiments/data/qmsum_led-1024/huggingface/transformers/087e8f4306cbf22e21907929074344a3b0a46bd680a118eb6267cd5a2bcec5b2.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|file_utils.py:1632] 2022-06-15 06:37:31,666 >> https://huggingface.co/allenai/led-base-16384/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/experiments/data/qmsum_led-1024/huggingface/transformers/tmp4s5ubd35
Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]Downloading: 100%|██████████| 772/772 [00:00<00:00, 363kB/s]
[INFO|file_utils.py:1636] 2022-06-15 06:37:32,076 >> storing https://huggingface.co/allenai/led-base-16384/resolve/main/special_tokens_map.json in cache at experiments/data/qmsum_led-1024/huggingface/transformers/05da652a7fca41c1c18027c1201e473217bb373e370d1283e3de49d5880cbf0c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0
[INFO|file_utils.py:1644] 2022-06-15 06:37:32,078 >> creating metadata file for experiments/data/qmsum_led-1024/huggingface/transformers/05da652a7fca41c1c18027c1201e473217bb373e370d1283e3de49d5880cbf0c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:37:32,478 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/vocab.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/4fb25bb1f9a942a2e2930029211b4a7deaeb18b62f6e5ce6d59730c90da51373.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:37:32,479 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/merges.txt from cache at experiments/data/qmsum_led-1024/huggingface/transformers/087e8f4306cbf22e21907929074344a3b0a46bd680a118eb6267cd5a2bcec5b2.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:37:32,479 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:37:32,479 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:37:32,479 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/special_tokens_map.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/05da652a7fca41c1c18027c1201e473217bb373e370d1283e3de49d5880cbf0c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:37:32,479 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/tokenizer_config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/86288ba22bce9550d76e9b26722ee92ae5921ae9285ccbc2904e9a5ad7199b73.cfc08f03f72cde495bd6b3dd3252bca130b3437de370856d084d1453c58b6fea
[INFO|configuration_utils.py:561] 2022-06-15 06:37:32,897 >> loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:598] 2022-06-15 06:37:32,899 >> Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": [
    1024,
    1024,
    1024,
    1024,
    1024,
    1024
  ],
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:561] 2022-06-15 06:37:33,430 >> loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:598] 2022-06-15 06:37:33,432 >> Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": [
    1024,
    1024,
    1024,
    1024,
    1024,
    1024
  ],
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|file_utils.py:1632] 2022-06-15 06:37:33,969 >> https://huggingface.co/allenai/led-base-16384/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/experiments/data/qmsum_led-1024/huggingface/transformers/tmpahe3j0bg
Downloading:   0%|          | 0.00/648M [00:00<?, ?B/s]Downloading:   0%|          | 53.2k/648M [00:00<30:04, 359kB/s]Downloading:   0%|          | 314k/648M [00:00<22:55, 471kB/s] Downloading:   0%|          | 1.20M/648M [00:00<16:34, 650kB/s]Downloading:   1%|          | 3.30M/648M [00:00<11:43, 917kB/s]Downloading:   1%|          | 6.27M/648M [00:00<08:18, 1.29MB/s]Downloading:   2%|▏         | 9.87M/648M [00:00<05:52, 1.81MB/s]Downloading:   2%|▏         | 14.7M/648M [00:00<04:08, 2.55MB/s]Downloading:   3%|▎         | 18.1M/648M [00:00<02:58, 3.52MB/s]Downloading:   3%|▎         | 22.1M/648M [00:01<02:08, 4.85MB/s]Downloading:   4%|▍         | 27.7M/648M [00:01<01:33, 6.61MB/s]Downloading:   5%|▌         | 32.4M/648M [00:01<01:09, 8.91MB/s]Downloading:   6%|▌         | 37.7M/648M [00:01<00:51, 11.9MB/s]Downloading:   6%|▋         | 42.0M/648M [00:01<00:40, 14.9MB/s]Downloading:   7%|▋         | 47.0M/648M [00:01<00:31, 18.9MB/s]Downloading:   8%|▊         | 53.9M/648M [00:01<00:25, 23.4MB/s]Downloading:   9%|▉         | 58.7M/648M [00:01<00:21, 27.6MB/s]Downloading:  10%|▉         | 64.0M/648M [00:01<00:18, 32.3MB/s]Downloading:  11%|█         | 69.9M/648M [00:02<00:15, 36.2MB/s]Downloading:  12%|█▏        | 74.9M/648M [00:02<00:14, 39.2MB/s]Downloading:  12%|█▏        | 80.7M/648M [00:02<00:13, 43.4MB/s]Downloading:  13%|█▎        | 86.0M/648M [00:02<00:12, 44.4MB/s]Downloading:  14%|█▍        | 91.0M/648M [00:02<00:13, 41.8MB/s]Downloading:  14%|█▍        | 91.0M/648M [00:46<00:13, 41.8MB/s]Downloading:  14%|█▍        | 91.0M/648M [00:55<164:38:19, 939B/s]Downloading:  14%|█▍        | 91.0M/648M [00:55<531:05:22, 291B/s]Downloading:  15%|█▍        | 95.1M/648M [00:55<369:02:51, 416B/s]Downloading:  15%|█▍        | 96.2M/648M [00:55<257:48:58, 594B/s]Downloading:  16%|█▌        | 105M/648M [00:55<177:34:44, 849B/s] Downloading:  18%|█▊        | 114M/648M [00:55<122:08:22, 1.21kB/s]Downloading:  19%|█▉        | 123M/648M [00:55<84:03:28, 1.73kB/s] Downloading:  21%|██        | 133M/648M [00:55<57:43:49, 2.47kB/s]Downloading:  22%|██▏       | 144M/648M [00:56<39:34:05, 3.54kB/s]Downloading:  24%|██▍       | 155M/648M [00:56<27:05:42, 5.05kB/s]Downloading:  25%|██▌       | 164M/648M [00:56<18:36:28, 7.21kB/s]Downloading:  27%|██▋       | 173M/648M [00:56<12:47:31, 10.3kB/s]Downloading:  28%|██▊       | 181M/648M [00:56<8:48:46, 14.7kB/s] Downloading:  29%|██▉       | 187M/648M [00:56<6:04:59, 21.0kB/s]Downloading:  30%|██▉       | 193M/648M [00:56<4:12:08, 30.0kB/s]Downloading:  31%|███       | 199M/648M [00:57<2:54:21, 42.9kB/s]Downloading:  32%|███▏      | 205M/648M [00:57<2:00:28, 61.3kB/s]Downloading:  32%|███▏      | 210M/648M [00:57<1:23:19, 87.5kB/s]Downloading:  33%|███▎      | 217M/648M [00:57<57:31, 125kB/s]   Downloading:  34%|███▍      | 222M/648M [00:57<39:46, 178kB/s]Downloading:  35%|███▌      | 228M/648M [00:57<27:32, 254kB/s]Downloading:  36%|███▌      | 233M/648M [00:57<19:05, 362kB/s]Downloading:  37%|███▋      | 238M/648M [00:57<13:14, 516kB/s]Downloading:  38%|███▊      | 243M/648M [00:57<09:11, 734kB/s]Downloading:  38%|███▊      | 249M/648M [00:58<06:22, 1.04MB/s]Downloading:  39%|███▉      | 254M/648M [00:58<04:27, 1.47MB/s]Downloading:  40%|████      | 259M/648M [00:58<03:06, 2.08MB/s]Downloading:  41%|████      | 265M/648M [00:58<02:12, 2.88MB/s]Downloading:  41%|████      | 265M/648M [01:12<02:12, 2.88MB/s]Downloading:  41%|████      | 265M/648M [01:28<64:45:20, 1.64kB/s]Downloading:  41%|████      | 265M/648M [01:47<1136:07:50, 93.7B/s]Downloading:  41%|████      | 265M/648M [02:14<1136:07:50, 93.7B/s]Downloading:  41%|████      | 265M/648M [02:25<1979:16:53, 53.8B/s]Downloading:  41%|████      | 265M/648M [02:27<1979:16:53, 53.8B/s]Downloading:  41%|████      | 265M/648M [02:27<1443:15:02, 73.7B/s]Downloading:  41%|████▏     | 269M/648M [02:27<999:09:05, 105B/s]  Downloading:  42%|████▏     | 270M/648M [02:28<697:26:53, 150B/s]Downloading:  43%|████▎     | 278M/648M [02:28<477:17:43, 215B/s]Downloading:  44%|████▍     | 287M/648M [02:28<325:57:53, 307B/s]Downloading:  46%|████▌     | 296M/648M [02:28<222:43:07, 439B/s]Downloading:  47%|████▋     | 305M/648M [02:29<151:51:39, 627B/s]Downloading:  49%|████▊     | 315M/648M [02:29<103:16:25, 895B/s]Downloading:  50%|█████     | 325M/648M [02:29<70:06:33, 1.28kB/s]Downloading:  52%|█████▏    | 335M/648M [02:29<47:32:19, 1.83kB/s]Downloading:  53%|█████▎    | 345M/648M [02:29<32:10:57, 2.61kB/s]Downloading:  55%|█████▍    | 355M/648M [02:29<21:49:52, 3.73kB/s]Downloading:  56%|█████▌    | 363M/648M [02:29<14:51:32, 5.33kB/s]Downloading:  57%|█████▋    | 370M/648M [02:29<10:08:31, 7.61kB/s]Downloading:  58%|█████▊    | 376M/648M [02:30<6:56:11, 10.9kB/s] Downloading:  59%|█████▉    | 382M/648M [02:30<4:45:00, 15.5kB/s]Downloading:  60%|█████▉    | 388M/648M [02:30<3:15:08, 22.2kB/s]Downloading:  61%|██████    | 395M/648M [02:30<2:13:01, 31.7kB/s]Downloading:  62%|██████▏   | 401M/648M [02:30<1:31:02, 45.2kB/s]Downloading:  63%|██████▎   | 406M/648M [02:30<1:02:23, 64.6kB/s]Downloading:  64%|██████▎   | 412M/648M [02:30<42:34, 92.2kB/s]  Downloading:  64%|██████▍   | 418M/648M [02:30<29:07, 132kB/s] Downloading:  65%|██████▌   | 423M/648M [02:31<19:56, 188kB/s]Downloading:  66%|██████▌   | 428M/648M [02:31<13:38, 268kB/s]Downloading:  67%|██████▋   | 434M/648M [02:31<09:20, 382kB/s]Downloading:  68%|██████▊   | 439M/648M [02:31<06:23, 544kB/s]Downloading:  69%|██████▊   | 445M/648M [02:31<04:21, 774kB/s]Downloading:  70%|██████▉   | 451M/648M [02:31<02:59, 1.10MB/s]Downloading:  70%|███████   | 456M/648M [02:31<02:03, 1.55MB/s]Downloading:  71%|███████   | 461M/648M [02:31<01:25, 2.18MB/s]Downloading:  72%|███████▏  | 466M/648M [02:31<00:59, 3.06MB/s]Downloading:  73%|███████▎  | 472M/648M [02:32<00:41, 4.24MB/s]Downloading:  74%|███████▎  | 477M/648M [02:32<00:29, 5.84MB/s]Downloading:  75%|███████▍  | 483M/648M [02:32<00:20, 8.03MB/s]Downloading:  75%|███████▌  | 488M/648M [02:32<00:14, 10.7MB/s]Downloading:  76%|███████▌  | 493M/648M [02:32<00:11, 13.7MB/s]Downloading:  77%|███████▋  | 498M/648M [02:32<00:08, 17.2MB/s]Downloading:  78%|███████▊  | 504M/648M [02:32<00:06, 22.1MB/s]Downloading:  79%|███████▊  | 509M/648M [02:32<00:05, 25.3MB/s]Downloading:  79%|███████▉  | 514M/648M [02:32<00:04, 28.6MB/s]Downloading:  80%|████████  | 519M/648M [02:33<00:05, 25.1MB/s]Downloading:  81%|████████  | 522M/648M [02:33<00:04, 27.4MB/s]Downloading:  82%|████████▏ | 531M/648M [02:33<00:03, 34.2MB/s]Downloading:  83%|████████▎ | 536M/648M [02:33<00:03, 37.2MB/s]Downloading:  84%|████████▍ | 543M/648M [02:33<00:02, 42.2MB/s]Downloading:  85%|████████▍ | 548M/648M [02:33<00:02, 46.0MB/s]Downloading:  86%|████████▌ | 555M/648M [02:33<00:02, 46.5MB/s]Downloading:  87%|████████▋ | 560M/648M [02:33<00:01, 49.0MB/s]Downloading:  87%|████████▋ | 567M/648M [02:34<00:01, 52.4MB/s]Downloading:  88%|████████▊ | 572M/648M [02:34<00:01, 48.3MB/s]Downloading:  89%|████████▉ | 579M/648M [02:34<00:01, 49.3MB/s]Downloading:  90%|█████████ | 584M/648M [02:34<00:01, 40.1MB/s]Downloading:  91%|█████████ | 588M/648M [02:34<00:01, 30.9MB/s]Downloading:  91%|█████████▏| 592M/648M [02:34<00:01, 32.8MB/s]Downloading:  92%|█████████▏| 596M/648M [02:34<00:01, 32.8MB/s]Downloading:  93%|█████████▎| 599M/648M [02:35<00:01, 29.7MB/s]Downloading:  93%|█████████▎| 605M/648M [02:35<00:01, 34.6MB/s]Downloading:  94%|█████████▍| 609M/648M [02:35<00:01, 35.6MB/s]Downloading:  95%|█████████▌| 616M/648M [02:35<00:00, 41.6MB/s]Downloading:  96%|█████████▌| 621M/648M [02:35<00:00, 41.6MB/s]Downloading:  97%|█████████▋| 625M/648M [02:35<00:00, 40.1MB/s]Downloading:  97%|█████████▋| 630M/648M [02:35<00:00, 35.6MB/s]Downloading:  98%|█████████▊| 633M/648M [02:35<00:00, 30.7MB/s]Downloading:  98%|█████████▊| 633M/648M [02:49<00:00, 30.7MB/s]Downloading:  98%|█████████▊| 633M/648M [03:01<57:59, 4.08kB/s]Downloading:  98%|█████████▊| 633M/648M [03:13<28:58:05, 136B/s]Downloading:  98%|█████████▊| 633M/648M [06:20<28:58:05, 136B/s]Downloading:  98%|█████████▊| 633M/648M [06:20<235:44:13, 16.7B/s]Downloading:  99%|█████████▊| 638M/648M [06:20<107:20:33, 23.9B/s]Downloading:  99%|█████████▊| 639M/648M [06:22<73:27:19, 34.1B/s] Downloading:  99%|█████████▉| 640M/648M [06:23<44:05:58, 48.8B/s]Downloading:  99%|█████████▉| 641M/648M [06:23<28:35:30, 69.7B/s]Downloading:  99%|█████████▉| 641M/648M [06:23<19:05:53, 99.5B/s]Downloading: 100%|█████████▉| 645M/648M [06:25<5:30:16, 142B/s]  Downloading: 100%|██████████| 648M/648M [06:25<00:00, 1.68MB/s]
[INFO|file_utils.py:1636] 2022-06-15 06:44:01,950 >> storing https://huggingface.co/allenai/led-base-16384/resolve/main/pytorch_model.bin in cache at experiments/data/qmsum_led-1024/huggingface/transformers/c8f7e4603efbc329ce921b34057d78880dead50f45b2a1648b3a06ca6eb17f51.201222b06d46289037a8dccc57548abc8eb81ba042d3762214ac15c9691ff8c7
[INFO|file_utils.py:1644] 2022-06-15 06:44:01,953 >> creating metadata file for experiments/data/qmsum_led-1024/huggingface/transformers/c8f7e4603efbc329ce921b34057d78880dead50f45b2a1648b3a06ca6eb17f51.201222b06d46289037a8dccc57548abc8eb81ba042d3762214ac15c9691ff8c7
[INFO|modeling_utils.py:1279] 2022-06-15 06:44:01,955 >> loading weights file https://huggingface.co/allenai/led-base-16384/resolve/main/pytorch_model.bin from cache at experiments/data/qmsum_led-1024/huggingface/transformers/c8f7e4603efbc329ce921b34057d78880dead50f45b2a1648b3a06ca6eb17f51.201222b06d46289037a8dccc57548abc8eb81ba042d3762214ac15c9691ff8c7
[INFO|modeling_utils.py:1524] 2022-06-15 06:44:09,376 >> All model checkpoint weights were used when initializing LEDForConditionalGeneration.

[INFO|modeling_utils.py:1532] 2022-06-15 06:44:09,376 >> All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at allenai/led-base-16384.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.
2022-06-15 06:44:09 | INFO | __main__ | 
2022-06-15 06:44:09 | INFO | __main__ | Training examples before tokenization:
2022-06-15 06:44:09 | INFO | __main__ | input #0: How Did Project Manager and User Interface introduce the prototype of the remote control?

Project Manager: Yep . Soon as I get this . Okay . This is our last meeting . Um I'll go ahead and go through the minutes from the previous meeting . Uh and then we'll have a , the prototype presentation . {vocalsound} Um then we will um do an evaluation . Uh or we'll see what , what we need to have under the criteria for the evaluation . Then we'll go through the finance and see if we fall within the budget . Um then we'll do the evaluation , and then we can finish up after that with um any changes that we'll need to make , or hopefully everything will fall right in line . Um let's see , minutes from the last meeting . Um we looked at uh the the trends . We had uh the fashion trends that people want a fancy look-and-feel . It was twice as important as anything else . Um they liked fruit and vegetables in the new styles . Um and a spongy feel . So we were talking about trying to incorporate those into our prototype . Um they wanted limited buttons and simplicity . Um then we looked at the uh the method for coming up with our own remote . Um looking at other other devices . Um the iPod , we really liked the look of that . Um we also had uh the kid's remote for a simple idea . Um a two part remote , which was what were were originally looking at . Uh and then um there was talk of spee uh speech recognition um becoming more uh predominant and easier to use . But I think we've still decided not to go with that . {vocalsound} Then we looked at the components um the materials for the case , the different energy sources , the different types of chips , um and made a decision on what we were going to use to make our remote . Um and basically how , what were making for the prototype . So I'm going to leave it at that and let you guys take over .
User Interface: The prototype discussion .
Project Manager: The prototype yeah . Do you need a {disfmarker} this ?
User Interface: No . {vocalsound}
Project Manager: Okay .
Industrial Designer: {vocalsound} Can try to plug that in there
User Interface: There is our remo {gap} the banana .
Marketing: {vocalsound}
Industrial Designer: but {disfmarker}
User Interface: Um {vocalsound} yeah basically we we st went with the colour yellow . Um working on the principle of a fruit which was mentioned , it's basically designed around a banana .
Project Manager: {vocalsound}
User Interface: Um but it would be held in such a fashion ,
Marketing: {vocalsound}
User Interface: where it is , obviously it wouldn't be that floppy 'cause this would be hard plastic . These would be like the rubber , the rubber grips . So that's so that would hopefully help with grip , or like the ergonomics of it . Um but all the controlling would be done with this scroll wheel . You have to use your imagination a little bit . And this here represents the screen , where you , where you'd go through .
Project Manager: Very nice .
User Interface: And the the simplest functions would be um almost identical to an iPod , where that one way ch through channels , that way th other way through channels . Volume up and down . And then to access the more complicated functions you'd you sorta go , you press that and go through the menus . It's that that simple . That just represents the infrared uh beam . That's a simple on and off switch . Um I don't know , we could use the voice . T that blue bits should be yellow , that that'd be where the batteries would be I suppose . And um {vocalsound} that's about it . It's as simple as you , we could make it really .
Industrial Designer: Right .
User Interface: Is there anything you want to add ?
Industrial Designer: That's what we have there . That's plastic . Plastic covered with rubber . We might uh add some more underneath here . Maybe give it , give it a form . I mean you're supposed to hold it like that , but um just if you grab it , take it from somewhere ,
User Interface: Yeah .
Project Manager: Mm-hmm .
Industrial Designer: so {disfmarker} yeah ,
User Interface: Doesn't make much make much difference .
Industrial Designer: you have some rub yeah .
User Interface: You could work left-handed or right-handed I suppose .
Industrial Designer: Exactly , {gap} use both . Might as well think about {disfmarker}
User Interface: T the actual thing might be smaller .
Industrial Designer: Th think about the button as well . Like either put either one {gap} one on either side or
User Interface: {vocalsound} Yeah .
Project Manager: What but what's that button ?
Industrial Designer: not do it at all . It's a quick on-off button .
User Interface: Just the on and off .
Project Manager: Uh , 'kay .
Industrial Designer: That's um
Marketing: {vocalsound}
Industrial Designer: yeah I think it's pretty important . So you don't have to fiddle with that .
Project Manager: 'Kay .
Industrial Designer: Right ? Um that's not um {disfmarker}
Project Manager: {vocalsound}
Industrial Designer: I'd say a bit smaller would probably be nice . You wanna play with that over there .
User Interface: Yeah .
Industrial Designer: There you go .
User Interface: It's you know it's flimsy 'cause it's made out of heavy Play-Doh ,
Marketing: {vocalsound}
Project Manager: Would you like to uh {disfmarker}
Industrial Designer: Right .
User Interface: but {disfmarker}
Marketing: Pretty impressive .
Project Manager: Well done .
User Interface: {vocalsound}
Marketing: {vocalsound} Kind of a banana .
User Interface: And whether or not it would fall into the cost {gap} everything I suppose . With the scroll and the L_C_D_ .
Project Manager: Well luckily we are going to find out . Or not luckily . Um do you have a marketing presentation for us .
Industrial Designer: {vocalsound}
Marketing: {vocalsound} I do . Okay . You guys are gonna help me do an evaluation of the criteria . Um . Okay . So first I'll just discuss some of the criteria that I found . Just based on the past trend reports that I was looking at earlier . And then we'll do a group evaluation of the prototype . And then we will calculate the average score to see how we did . Um so the criteria we're gonna be looking at are the complaints um that we heard from the users who were interviewed earlier . So we're gonna be doing it based on a seven point scale . And one is going to mean true , that we did actually achieve that . With seven being false , we did not achieve that . {gap} . Okay . So for the first one , we need to decide , did we solved the problem of the users who complained about an ugly remote ? {vocalsound}
Industrial Designer: {vocalsound} {vocalsound} .
User Interface: {vocalsound}
Project Manager: I think it's definitely different than anything else out there .
User Interface: {vocalsound}
Marketing: Mm .
User Interface: Yeah .
Project Manager: So if they think that what is out there is ugly , then yes I would say , I would say most definitely .
Marketing: {vocalsound}
User Interface: I would {gap} .
Project Manager: It's bright .
User Interface: It's bright . It's {disfmarker}
Project Manager: It still has your traditional black .
User Interface: It's curved . It's not {disfmarker} there's no sharp
Industrial Designer: {vocalsound}
User Interface: angles to it .
Project Manager: Yep , not angular .
Marketing: Mm .
Industrial Designer: I'd say , when it comes to the ergonomics , the form and stuff , yes that's definitely more beautiful than your average .
Marketing: {vocalsound}
Industrial Designer: However the colour , we don't have a say in that .
Marketing: Yeah I think the colours detract a little bit . {vocalsound}
User Interface: Some people might say it . Yeah .
Industrial Designer: That has been , that has been dictated pretty much by the company .
Project Manager: Mm .
Industrial Designer: So uh to answer that honestly I would rather say like uh , we have not solved the problem completely with the ugly remote because the colour is ugly , definitely .
Project Manager: {vocalsound} Yep .
Marketing: That's true . Yeah .
Project Manager: {vocalsound}
User Interface: Yeah .
Industrial Designer: 'S nothing you can say about that . I mean I much prefer something like brushed chrome with that form .
User Interface: Yeah .
Industrial Designer: But {disfmarker}
Project Manager: Yeah something more modern to go {disfmarker} a a modern colour to go with the modern form .
Industrial Designer: Right . Right . It's different . You don't want your uh three feet huge L_C_D_ dis display in your living room that's hanging from the wall to be controlled with something like that .
Marketing: Um okay so , do you think , since we {disfmarker} This was a a sign criteria , do you think maybe we should put it somewhere in the middle then ?
Industrial Designer: Yeah .
Marketing: Does that sound good ?
Project Manager: Yeah .
User Interface: Yeah .
Industrial Designer: {vocalsound}
Marketing: What do you think ? Three ? Four ?
Project Manager: I would say
Marketing: Five ?
Project Manager: four . {vocalsound}
Industrial Designer: Yeah . {vocalsound}
Marketing: {vocalsound} Four is fair . Okay .
Project Manager: Very non-committal , four .
Marketing: Okay , the second one . Did we make it simple for new users ?
Industrial Designer: It's very intuitive , I think yeah .
User Interface: Yeah . I think that was the main aim , one of the main aims that we had .
Industrial Designer: {vocalsound} S give it a one .
Marketing: One ,
Project Manager: Yeah .
Marketing: 'kay . Okay . Um , do the controls now match the operating behaviour of the users ?
User Interface: Uh yeah . 'Cause we've we've brought it down to basically four controls {gap} most common , which are channel and volume .
Marketing: I'd say that {disfmarker}
Project Manager: Mm-hmm .
Industrial Designer: Right .
User Interface: And then the other ones are just a matter of just going , just scrolling further .
Project Manager: S scrolling through and selecting a few .
Industrial Designer: Right . So that's a one .
Marketing: So one ?
Project Manager: I think that's a one .
Marketing: Yeah ? {vocalsound} Okay . Okay um the fourth one . How about the problem of a remote being easily lost ? One of the number one complaints .
Industrial Designer: Something that big and that yellow you just don't lose anymore .
Project Manager: {vocalsound}
User Interface: {vocalsound} Yeah .
Marketing: {vocalsound} Whether you want to or not , you're not gonna lose it . {vocalsound}
User Interface: It's bright yellow .
Industrial Designer: {vocalsound}
User Interface: Bright yellow's hard to lose . But um if we were to , if we were , that , the speech recognition . That , we could maybe just use that solely for the the finding thing . That was what we'd we'd mentioned .
Project Manager: So if we incorporate speech recognition into it then it could {disfmarker}
User Interface: Just just to use , to find it when it was lost . But like I said , like I don't think you'd lose something so yellow so easily .
Industrial Designer: Oops . Hmm .
User Interface: And it's not gonna fall , like a rectangle would slip down behind things . That's gonna be a difficult shape to {disfmarker}
Industrial Designer: Well what {disfmarker}
Project Manager: And it is quite bright and {disfmarker}
User Interface: Yeah .
Marketing: {vocalsound}
User Interface: Maybe in the middle again , three or four or something ?
Project Manager: Uh {disfmarker}
Industrial Designer: S
Marketing: Okay .
User Interface: I mean you know {gap} loo losing things is one of those things that people can lose , I mean a million ways .
Project Manager: Yeah .
User Interface: You can pick it up and walk away with it and then you've lost it .
Industrial Designer: Mm .
Marketing: That's true .
Project Manager: But if we do go with the , with the speech recognition , then it , then our scale goes up quite a bit I think .
Marketing: Mm .
Industrial Designer: Oh yeah . You probably {disfmarker}
User Interface: Yeah .
Project Manager: Probably two . You know . If we eliminate the fact that you know it's impossible to guarantee that it's not gonna be lost then
User Interface: Yeah .
Industrial Designer: Mm .
Project Manager: I'd say two .
Industrial Designer: {vocalsound}
Marketing: {vocalsound}
Project Manager: With the speech recognition , which of course may be changed depending on budget .
User Interface: Yeah .
Industrial Designer: Y you could add an extra feature actually . Which makes this thing raise hell when you remove it too far from the television .
User Interface: Yeah .
Industrial Designer: We could add that but that's nothing we have thought of so far .
Project Manager: Which , which may be cheaper than speech recognition if it were just a {disfmarker}
Industrial Designer: Yes .
User Interface: Yeah true . But I mean d just those whistling , clapping key rings you have . They're cheap .
Marketing: Annoying alarm or something ?
Project Manager: {vocalsound}
Industrial Designer: It's it's {disfmarker}
Marketing: Yeah .
User Interface: So it can't be that
Industrial Designer: Um the {disfmarker} it's based on this anti anti-theft technology for suitcases and stuff ,
User Interface: expensive .
Project Manager: Some sort of proximity {vocalsound} {disfmarker}
User Interface: Yeah .
Industrial Designer: where you have one piece that's attached to your luggage , another piece that starts beeping . That can't cost much .
User Interface: Yeah .
Industrial Designer: So that can also easily be integrated because these things are small enough to to hide , so you have one piece , you have to glue somewhere behind your {disfmarker} stick it behind your T_V_ and the other {disfmarker}
User Interface: {gap} stick it on the T_V_ {gap} .
Project Manager: {vocalsound} Pray that you don't accidentally lose that piece . {vocalsound}
Industrial Designer: Right .
User Interface: {vocalsound}
Marketing: {vocalsound}
Industrial Designer: That'd be tough then . {vocalsound} Well also your remote would uh alarm you if somebody stole you t your television , yeah . Ran off with it without taking the beautiful remote control .
Project Manager: {vocalsound}
User Interface: Yeah . {vocalsound}
Marketing: So . Are we adding one of these two features ?
Industrial Designer: Let's add one of those features and say yes . {vocalsound}
Marketing: {gap} gonna say {disfmarker} okay .
Project Manager: Okay .
Marketing: So we're {vocalsound} back to a one ?
User Interface: Two .
Marketing: Or a two ?
Project Manager: Two .
Industrial Designer: Two .
Marketing: Two , 'kay . Okay . Are we technologically innovative ?
Industrial Designer: Uh {disfmarker}
User Interface: {vocalsound} I'd say so .
Industrial Designer: {vocalsound}
User Interface: Uh don't get many mo remote controls with
Industrial Designer: It's all just {disfmarker}
User Interface: screens on .
Industrial Designer: It's all just stolen technology when it comes down to {disfmarker} {vocalsound} {vocalsound}
User Interface: Yeah it's stolen technology .
Marketing: From iPod yeah . {vocalsound}
Project Manager: It's {disfmarker} {vocalsound}
User Interface: But we have {gap} .
Project Manager: But there's not a lot of yellow , there's not a lotta yellow .
Industrial Designer: right
Marketing: But for remotes {disfmarker} yeah .
Project Manager: Course that wasn't really {disfmarker}
Industrial Designer: right
User Interface: Fa
Industrial Designer: right right .
Project Manager: we were kinda forced to take that colour .
Marketing: Two ? Three ?
User Interface: {gap} 'cause it's stolen .
Project Manager: I don't know that we are that innovative , to tell you the truth . {vocalsound}
User Interface: No maybe not .
Industrial Designer: Yeah not really .
Marketing: But how many remotes do you see like this ?
User Interface: {vocalsound}
Project Manager: If we added the screaming factor {vocalsound} then we go up .
Industrial Designer: {vocalsound}
Marketing: Not so many .
Industrial Designer: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: {vocalsound} Um I would say we're probably at four .
Industrial Designer: Right .
Marketing: Really ? Okay . {vocalsound} That's gonna hurt us .
User Interface: {vocalsound}
Marketing: Okay . Um spongy material ?
Industrial Designer: Yeah well you have that , kind of , sort of .
Project Manager: We have some spongy , yeah .
User Interface: Yeah as much as as needed , I think .
Marketing: 'Kay .
Industrial Designer: It's not a one though .
Project Manager: No .
Industrial Designer: One would be the whole thing
Project Manager: Yeah . Because it's only got what , these parts are the grips and perhaps the back side {disfmarker} the bottom {disfmarker} the underneath on the back .
Industrial Designer: to fold and stuff . Yeah .
User Interface: Yeah .
Industrial Designer: So that's a four at most .
Project Manager: Probably a four at most . Possibly even a five .
Marketing: And lastly , did we put the fashion in electronics ?
Project Manager: {vocalsound}
Industrial Designer: Y yes .
User Interface: Yeah .
Marketing: I'd say we did .
Project Manager: If your fashion is b is Carmen Miranda , you betcha . {vocalsound}
Industrial Designer: More {disfmarker} {vocalsound}
User Interface: Yeah . {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound} Well the recent fashion is rather displayed in the in the L_C_D_ and the way you operate it than the form and the colour ,
User Interface: On the {disfmarker}
Project Manager: It's true .
User Interface: Yeah .
Industrial Designer: but it definitely is {disfmarker}
User Interface: Be what we were told , and they'd say yeah , definitely .
Industrial Designer: {gap} .
Marketing: 'Kay . Alright . Now we just gotta calculate . Six eight twelve sixteen . Seventeen divided by s
User Interface: {gap} .
Project Manager: Seven is {disfmarker}
Marketing: Eight .
Project Manager: Two point {disfmarker}
Marketing: {vocalsound}
Project Manager: {gap} two point four ?
User Interface: Is that some long division ? No .
Project Manager: Something .
Marketing: Well I haven't done math in years .
Industrial Designer: {vocalsound}
Marketing: What two {disfmarker}
User Interface: {vocalsound}
Marketing: {vocalsound} I dunno .
User Interface: Just , I'm sure there's a {gap} .
Marketing: Okay we'll say two point four two . Right ? How does that look ?
Industrial Designer: I'm impressed . I can't do that without a calculator . {vocalsound}
User Interface: No I can't do long {gap}
Marketing: {vocalsound} It's been a while .
User Interface: very impressive .
Project Manager: And what what is the acceptable criteria ? Is there like a scale that we have to hit ?
Marketing: Oh no . They just told me to
Industrial Designer: {vocalsound}
Marketing: {vocalsound} pick my own criteria and have you guys evaluate it {vocalsound} basically .
Project Manager: {vocalsound} Alright then .
Marketing: So that's that .
Project Manager: Okay . Well , let's see .
Marketing: {vocalsound}
Project Manager: Now we get to do the budget numbers . You didn't know that you were gonna have a budget . But we do . Okay .
User Interface: Yeah . Yeah so . You'd been going a long time dividing that . It's two point four two eight five se it just keeps going on .
Marketing: Oh my god .
User Interface: Two point four two basically .
Marketing: Okay . Yeah we'll go with that .
Project Manager: So I have here an {disfmarker}
Industrial Designer: {vocalsound} Fifty percent , you're kidding .
Marketing: Not too shabby .
Project Manager: Yeah .
Industrial Designer: {vocalsound} P
Project Manager: We want a fifty percent profit on this . Oh you can't really see that very well .
User Interface: {vocalsound} Charge about three hundred quid for it .
Project Manager: {vocalsound} Twelve and a half Euros is what supposed to cost us . Okay , so {disfmarker}
Industrial Designer: It's too much .
Project Manager: Well let's see .
Industrial Designer: Um {disfmarker}
Project Manager: The f the {disfmarker} Wonder if I can make this {disfmarker}
Industrial Designer: Uh {disfmarker}
Project Manager: What the {disfmarker} {vocalsound} Oh it won't let me do that . Okay . Alright so at top , I don't know if you guys can read that or not . I can't 'cause I don't have my glasses on ,
Industrial Designer: {vocalsound}
Project Manager: but so we've got the energy source . There's uh four , five , six categories .
Industrial Designer: Battery .
Project Manager: We have energy source , electronics , case . Then we have case material supplements , interface type , and then button supplements . Okay so {disfmarker} Uh first of all energy source , we picked battery . Um and how many batteries do we think this will probably take ?
User Interface: {vocalsound}
Project Manager: Probably some e either two or four .
Industrial Designer: Two .
Project Manager: Two ? {vocalsound} Like it . {vocalsound}
Industrial Designer: At four it's gonna be too heavy , so that that's not our problem . People can change it every month .
Project Manager: {vocalsound} Excellent .
Industrial Designer: {vocalsound} They won't know until after they bought it .
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: This is consumerism .
Industrial Designer: {vocalsound}
Project Manager: Alright so for the electronics our choices are simpl simple chip-on-print , regular chip-on-print , advanced chip-on-print , sample sensor , sample speaker .
Industrial Designer: {gap} .
User Interface: We're advanced chip are we ?
Industrial Designer: That's the advanced chip-on-print , yeah .
Project Manager: 'Kay , {gap} we have one of those . 'Kay then the case is a {disfmarker} Probably it's double curved .
Industrial Designer: Double curved , yes .
Project Manager: Case materials are
Industrial Designer: Plastic .
Project Manager: plastic . Um I guess it's two , since one for the top , one for the bottom .
Industrial Designer: N no .
Project Manager: Is that right or is it just one ?
Industrial Designer: No that's just one .
Project Manager: Maybe it's one because of the {disfmarker}
Industrial Designer: It's just one mo single mould , we can do that .
Project Manager: 'Kay .
User Interface: Yeah {gap} yeah .
Marketing: Right . {vocalsound}
Project Manager: I guess it doesn't matter 'cause the price on that one is zero , which is nice .
Industrial Designer: Exactly , right .
Marketing: Oh .
Project Manager: Special colour ?
Industrial Designer: That's not a special colour . It's a specially ugly colour , but it's not special .
Marketing: Bright yellow .
Project Manager: {vocalsound} Interface type . We have pushbutton , scroll-wheel interface , integrated scroll-wheel pushbutton , and an L_C_D_ display .
User Interface: {vocalsound}
Marketing: {vocalsound}
User Interface: S
Industrial Designer: S {vocalsound}
User Interface: That's {disfmarker} Yeah .
Project Manager: So we actually have the L_C_D_ display
Marketing: {vocalsound}
User Interface: And then {disfmarker}
Project Manager: and then is it the integrated or is it {disfmarker}
User Interface: I'd say the integrated .
Project Manager: Yeah .
Industrial Designer: Yes unfortunately .
Project Manager: 'Kay . Button supplement ? Special colour ?
User Interface: Mm .
Project Manager: Um special form ? Special material .
Industrial Designer: We could of course make the buttons wood .
Project Manager: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound} Say mahogany or so
Marketing: {vocalsound} It'd look really lovely .
Project Manager: Or titanium .
Industrial Designer: Mm-hmm or titanium .
Project Manager: They cost us all the same .
Marketing: {vocalsound} Yeah .
User Interface: {gap} remote control {gap} .
Project Manager: Well we only have one button so really we shouldn't be charged ,
Industrial Designer: Uh just {disfmarker}
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
Project Manager: {vocalsound} we shouldn't be charged anything for the the button supplements .
User Interface: No that's getting a bit tiny .
Project Manager: Um {disfmarker}
User Interface: Yeah .
Marketing: {vocalsound}
User Interface: I'd ignore that .
Marketing: Leave it blank .
Project Manager: Okay . We're gonna leave that one blank because we run on a L_C_D_ and scroll . So our total is fifteen point five . Which I believe is
Industrial Designer: Yeah that's too much .
Project Manager: by three Euros over .
Industrial Designer: It's hard to believe . So we'll go for the hand dynamo huh ? {vocalsound}
Project Manager: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: So the only thing better than um a banana-shaped remote is one that you shake .
User Interface: If it w What if we completely took out the the one single button we've got on .
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
User Interface: And just had a scroll wheel interface . And the L_C_D_ display . I suppose the L_C_D_ C_ display's the one that's pushing it up a bit though .
Project Manager: Yeah 'cause the {disfmarker}
Marketing: {vocalsound}
Project Manager: Well 'cause we have to have both right ?
User Interface: Yeah .
Industrial Designer: I mean let's let's face it , it also depends on the software on the on the television .
User Interface: Yeah .
Industrial Designer: You can have the the information that this thing transmits be being displayed on the on the screen .
Project Manager: Mm-hmm .
Industrial Designer: So s yeah let's take away the {disfmarker}
User Interface: Yeah you could maybe take out the L_C_D_ dis display even ,
Industrial Designer: Yeah . Yeah .
User Interface: if it if it comes up on the computer itsel on the T_V_ itself .
Industrial Designer: Right .
Project Manager: So we may not need the L_C_D_ display ?
User Interface: Uh that is possible yeah .
Industrial Designer: Right . We may not need it . There you go .
Project Manager: Well there we go .
Industrial Designer: Perfect .
Project Manager: Twelve point five .
User Interface: There we go .
Marketing: {vocalsound} Perfect .
Project Manager: Okay . So we just remove our {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
User Interface: Screen .
Marketing: {vocalsound}
Project Manager: screen here .
User Interface: Make it a bigger dial .
Industrial Designer: {vocalsound}
User Interface: Easier to use . Even easier to use then .
Project Manager: {vocalsound}
Industrial Designer: {vocalsound} Okay , the {disfmarker}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: Besides look at what the L_C_D_ does to our lovely remote .
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: Back to the design room boys .
Industrial Designer: So we can just take away a heck of a lot of the {disfmarker} {vocalsound}
Marketing: {vocalsound}
User Interface: {vocalsound}
Marketing: {gap} .
Industrial Designer: there you go . {gap} central ?
Marketing: What's the blue part ?
User Interface: That was just {disfmarker}
Industrial Designer: Oh that's just {disfmarker}
User Interface: we ran out of yellow . {vocalsound}
Marketing: Oh that's the batteries .
Industrial Designer: yeah .
Marketing: Okay . {vocalsound}
Industrial Designer: There you go
User Interface: There you go .
Industrial Designer: . Oops .
User Interface: Even simpler .
Marketing: {vocalsound} Looks more like a banana .
User Interface: Yeah .
Industrial Designer: There you go .
User Interface: For all those fruit lovers out there .
Industrial Designer: One more criteria .
Project Manager: {vocalsound}
Marketing: {vocalsound}
Project Manager: {vocalsound} Okay so the costs under twelve point five Euro . Was no . We redesigned it . Now it's yes .
User Interface: Yeah .
Marketing: {vocalsound}
Project Manager: Next slide . Project evaluation . Uh project process , satisfaction with , for example , room for creativity , leadership , teamwork , means , new ideas found . Um {disfmarker} So {disfmarker} I guess that {disfmarker} Let's see here . I think that perhaps the project evaluation's just supposed to be completed by me . But I'd like to hear your thoughts .
Marketing: {vocalsound}
Project Manager: {vocalsound}
Industrial Designer: {vocalsound} Fair enough . {vocalsound}
User Interface: {vocalsound}
Marketing: Trying to fill in some time there . {vocalsound}
Project Manager: Uh h what did you think of our project process ? {vocalsound}
Industrial Designer: Great . {vocalsound}
User Interface: I think we did {disfmarker} yeah I think we did quite well . Um {disfmarker}
Industrial Designer: Yeah .
Project Manager: Good .
Marketing: Good teamwork {gap} . {vocalsound}
Industrial Designer: Just half a day , you have a remote . There you go .
User Interface: Yeah . Right from the start of the day .
Project Manager: Yeah I think {disfmarker}
User Interface: We sort of knew where we were going straight away I thought .
Project Manager: {gap} we st we started off a little little weak . Our leadership was quite weak in the beginning .
Marketing: {vocalsound}
Project Manager: Um {vocalsound} um {disfmarker}
Marketing: {vocalsound}
Project Manager: But as the day went along we had more idea of what we were doing . Um room for creativity ? There was that . Um I think we tried a lotta different things and um I think it was um interesting as you guys brought up more um information and studies that we were right on with a lot of those things . Um you guys worked together well as a team . And um the means ? Which was the whiteboard and the pens .
User Interface: Yeah . We've used the whiteboard .
Industrial Designer: Super super .
Project Manager: I had some problem with the pen I think , but {vocalsound} minus your p
Marketing: Minus your PowerPoint fiasco .
Industrial Designer: {vocalsound} Well that's not my fault . That's obviously the people I work for uh that work for me ,
Marketing: No I know . I'm {disfmarker}
Project Manager: Well {disfmarker}
Marketing: yeah . Incom {vocalsound}
Industrial Designer: uh they've just you know {disfmarker}
User Interface: {vocalsound}
Project Manager: {vocalsound} Have a {disfmarker}
Industrial Designer: Heads are gonna roll , believe me .
Project Manager: we have a list of employees that you would like fired .
User Interface: {vocalsound}
Marketing: {vocalsound}
User Interface: {vocalsound}
Industrial Designer: Yes yes .
Project Manager: Okay . N new ideas found ? Um {disfmarker}
Industrial Designer: {vocalsound}
Marketing: Mm . Kinda .
Project Manager: Yes for the remote . Maybe no not f for
User Interface: Technology used .
Project Manager: technology . Alright . Closing . Costs are within the budget . Project is evaluated . Um complete the final questionnaire and meeting summary . That's it .
User Interface: Excellent .
Project Manager: And I still have to do my minutes for the last meeting . {vocalsound}
Marketing: {vocalsound}
Project Manager: Actually . Um so there will probably be another questionnaire coming up . And then we'll have to check with the main boss whether we can , what goes on after that .
Marketing: We might have a while though .
Industrial Designer: {gap} .
Project Manager: But that's the end of our meeting .
2022-06-15 06:44:09 | INFO | __main__ | output #0: Project Manager introduced that the prototype incorporated fashion trends that people prefer fancy looking products like fruit and vegetable. After That, User Interface presented the product which looked like a banana and was bright yellow except for the blue button. The style was as simple as possible in order to fit the customers' need for simplicity. Also, the product could be curved and used both-handed with advanced chips hidden inside, which seemed quite creative and identical to iPod features. In the end, Industrial Designer commented that the remote control could be smaller in size.
2022-06-15 06:44:09 | INFO | __main__ | input #1: How did Marketing design the product evaluation?

Project Manager: Yep . Soon as I get this . Okay . This is our last meeting . Um I'll go ahead and go through the minutes from the previous meeting . Uh and then we'll have a , the prototype presentation . {vocalsound} Um then we will um do an evaluation . Uh or we'll see what , what we need to have under the criteria for the evaluation . Then we'll go through the finance and see if we fall within the budget . Um then we'll do the evaluation , and then we can finish up after that with um any changes that we'll need to make , or hopefully everything will fall right in line . Um let's see , minutes from the last meeting . Um we looked at uh the the trends . We had uh the fashion trends that people want a fancy look-and-feel . It was twice as important as anything else . Um they liked fruit and vegetables in the new styles . Um and a spongy feel . So we were talking about trying to incorporate those into our prototype . Um they wanted limited buttons and simplicity . Um then we looked at the uh the method for coming up with our own remote . Um looking at other other devices . Um the iPod , we really liked the look of that . Um we also had uh the kid's remote for a simple idea . Um a two part remote , which was what were were originally looking at . Uh and then um there was talk of spee uh speech recognition um becoming more uh predominant and easier to use . But I think we've still decided not to go with that . {vocalsound} Then we looked at the components um the materials for the case , the different energy sources , the different types of chips , um and made a decision on what we were going to use to make our remote . Um and basically how , what were making for the prototype . So I'm going to leave it at that and let you guys take over .
User Interface: The prototype discussion .
Project Manager: The prototype yeah . Do you need a {disfmarker} this ?
User Interface: No . {vocalsound}
Project Manager: Okay .
Industrial Designer: {vocalsound} Can try to plug that in there
User Interface: There is our remo {gap} the banana .
Marketing: {vocalsound}
Industrial Designer: but {disfmarker}
User Interface: Um {vocalsound} yeah basically we we st went with the colour yellow . Um working on the principle of a fruit which was mentioned , it's basically designed around a banana .
Project Manager: {vocalsound}
User Interface: Um but it would be held in such a fashion ,
Marketing: {vocalsound}
User Interface: where it is , obviously it wouldn't be that floppy 'cause this would be hard plastic . These would be like the rubber , the rubber grips . So that's so that would hopefully help with grip , or like the ergonomics of it . Um but all the controlling would be done with this scroll wheel . You have to use your imagination a little bit . And this here represents the screen , where you , where you'd go through .
Project Manager: Very nice .
User Interface: And the the simplest functions would be um almost identical to an iPod , where that one way ch through channels , that way th other way through channels . Volume up and down . And then to access the more complicated functions you'd you sorta go , you press that and go through the menus . It's that that simple . That just represents the infrared uh beam . That's a simple on and off switch . Um I don't know , we could use the voice . T that blue bits should be yellow , that that'd be where the batteries would be I suppose . And um {vocalsound} that's about it . It's as simple as you , we could make it really .
Industrial Designer: Right .
User Interface: Is there anything you want to add ?
Industrial Designer: That's what we have there . That's plastic . Plastic covered with rubber . We might uh add some more underneath here . Maybe give it , give it a form . I mean you're supposed to hold it like that , but um just if you grab it , take it from somewhere ,
User Interface: Yeah .
Project Manager: Mm-hmm .
Industrial Designer: so {disfmarker} yeah ,
User Interface: Doesn't make much make much difference .
Industrial Designer: you have some rub yeah .
User Interface: You could work left-handed or right-handed I suppose .
Industrial Designer: Exactly , {gap} use both . Might as well think about {disfmarker}
User Interface: T the actual thing might be smaller .
Industrial Designer: Th think about the button as well . Like either put either one {gap} one on either side or
User Interface: {vocalsound} Yeah .
Project Manager: What but what's that button ?
Industrial Designer: not do it at all . It's a quick on-off button .
User Interface: Just the on and off .
Project Manager: Uh , 'kay .
Industrial Designer: That's um
Marketing: {vocalsound}
Industrial Designer: yeah I think it's pretty important . So you don't have to fiddle with that .
Project Manager: 'Kay .
Industrial Designer: Right ? Um that's not um {disfmarker}
Project Manager: {vocalsound}
Industrial Designer: I'd say a bit smaller would probably be nice . You wanna play with that over there .
User Interface: Yeah .
Industrial Designer: There you go .
User Interface: It's you know it's flimsy 'cause it's made out of heavy Play-Doh ,
Marketing: {vocalsound}
Project Manager: Would you like to uh {disfmarker}
Industrial Designer: Right .
User Interface: but {disfmarker}
Marketing: Pretty impressive .
Project Manager: Well done .
User Interface: {vocalsound}
Marketing: {vocalsound} Kind of a banana .
User Interface: And whether or not it would fall into the cost {gap} everything I suppose . With the scroll and the L_C_D_ .
Project Manager: Well luckily we are going to find out . Or not luckily . Um do you have a marketing presentation for us .
Industrial Designer: {vocalsound}
Marketing: {vocalsound} I do . Okay . You guys are gonna help me do an evaluation of the criteria . Um . Okay . So first I'll just discuss some of the criteria that I found . Just based on the past trend reports that I was looking at earlier . And then we'll do a group evaluation of the prototype . And then we will calculate the average score to see how we did . Um so the criteria we're gonna be looking at are the complaints um that we heard from the users who were interviewed earlier . So we're gonna be doing it based on a seven point scale . And one is going to mean true , that we did actually achieve that . With seven being false , we did not achieve that . {gap} . Okay . So for the first one , we need to decide , did we solved the problem of the users who complained about an ugly remote ? {vocalsound}
Industrial Designer: {vocalsound} {vocalsound} .
User Interface: {vocalsound}
Project Manager: I think it's definitely different than anything else out there .
User Interface: {vocalsound}
Marketing: Mm .
User Interface: Yeah .
Project Manager: So if they think that what is out there is ugly , then yes I would say , I would say most definitely .
Marketing: {vocalsound}
User Interface: I would {gap} .
Project Manager: It's bright .
User Interface: It's bright . It's {disfmarker}
Project Manager: It still has your traditional black .
User Interface: It's curved . It's not {disfmarker} there's no sharp
Industrial Designer: {vocalsound}
User Interface: angles to it .
Project Manager: Yep , not angular .
Marketing: Mm .
Industrial Designer: I'd say , when it comes to the ergonomics , the form and stuff , yes that's definitely more beautiful than your average .
Marketing: {vocalsound}
Industrial Designer: However the colour , we don't have a say in that .
Marketing: Yeah I think the colours detract a little bit . {vocalsound}
User Interface: Some people might say it . Yeah .
Industrial Designer: That has been , that has been dictated pretty much by the company .
Project Manager: Mm .
Industrial Designer: So uh to answer that honestly I would rather say like uh , we have not solved the problem completely with the ugly remote because the colour is ugly , definitely .
Project Manager: {vocalsound} Yep .
Marketing: That's true . Yeah .
Project Manager: {vocalsound}
User Interface: Yeah .
Industrial Designer: 'S nothing you can say about that . I mean I much prefer something like brushed chrome with that form .
User Interface: Yeah .
Industrial Designer: But {disfmarker}
Project Manager: Yeah something more modern to go {disfmarker} a a modern colour to go with the modern form .
Industrial Designer: Right . Right . It's different . You don't want your uh three feet huge L_C_D_ dis display in your living room that's hanging from the wall to be controlled with something like that .
Marketing: Um okay so , do you think , since we {disfmarker} This was a a sign criteria , do you think maybe we should put it somewhere in the middle then ?
Industrial Designer: Yeah .
Marketing: Does that sound good ?
Project Manager: Yeah .
User Interface: Yeah .
Industrial Designer: {vocalsound}
Marketing: What do you think ? Three ? Four ?
Project Manager: I would say
Marketing: Five ?
Project Manager: four . {vocalsound}
Industrial Designer: Yeah . {vocalsound}
Marketing: {vocalsound} Four is fair . Okay .
Project Manager: Very non-committal , four .
Marketing: Okay , the second one . Did we make it simple for new users ?
Industrial Designer: It's very intuitive , I think yeah .
User Interface: Yeah . I think that was the main aim , one of the main aims that we had .
Industrial Designer: {vocalsound} S give it a one .
Marketing: One ,
Project Manager: Yeah .
Marketing: 'kay . Okay . Um , do the controls now match the operating behaviour of the users ?
User Interface: Uh yeah . 'Cause we've we've brought it down to basically four controls {gap} most common , which are channel and volume .
Marketing: I'd say that {disfmarker}
Project Manager: Mm-hmm .
Industrial Designer: Right .
User Interface: And then the other ones are just a matter of just going , just scrolling further .
Project Manager: S scrolling through and selecting a few .
Industrial Designer: Right . So that's a one .
Marketing: So one ?
Project Manager: I think that's a one .
Marketing: Yeah ? {vocalsound} Okay . Okay um the fourth one . How about the problem of a remote being easily lost ? One of the number one complaints .
Industrial Designer: Something that big and that yellow you just don't lose anymore .
Project Manager: {vocalsound}
User Interface: {vocalsound} Yeah .
Marketing: {vocalsound} Whether you want to or not , you're not gonna lose it . {vocalsound}
User Interface: It's bright yellow .
Industrial Designer: {vocalsound}
User Interface: Bright yellow's hard to lose . But um if we were to , if we were , that , the speech recognition . That , we could maybe just use that solely for the the finding thing . That was what we'd we'd mentioned .
Project Manager: So if we incorporate speech recognition into it then it could {disfmarker}
User Interface: Just just to use , to find it when it was lost . But like I said , like I don't think you'd lose something so yellow so easily .
Industrial Designer: Oops . Hmm .
User Interface: And it's not gonna fall , like a rectangle would slip down behind things . That's gonna be a difficult shape to {disfmarker}
Industrial Designer: Well what {disfmarker}
Project Manager: And it is quite bright and {disfmarker}
User Interface: Yeah .
Marketing: {vocalsound}
User Interface: Maybe in the middle again , three or four or something ?
Project Manager: Uh {disfmarker}
Industrial Designer: S
Marketing: Okay .
User Interface: I mean you know {gap} loo losing things is one of those things that people can lose , I mean a million ways .
Project Manager: Yeah .
User Interface: You can pick it up and walk away with it and then you've lost it .
Industrial Designer: Mm .
Marketing: That's true .
Project Manager: But if we do go with the , with the speech recognition , then it , then our scale goes up quite a bit I think .
Marketing: Mm .
Industrial Designer: Oh yeah . You probably {disfmarker}
User Interface: Yeah .
Project Manager: Probably two . You know . If we eliminate the fact that you know it's impossible to guarantee that it's not gonna be lost then
User Interface: Yeah .
Industrial Designer: Mm .
Project Manager: I'd say two .
Industrial Designer: {vocalsound}
Marketing: {vocalsound}
Project Manager: With the speech recognition , which of course may be changed depending on budget .
User Interface: Yeah .
Industrial Designer: Y you could add an extra feature actually . Which makes this thing raise hell when you remove it too far from the television .
User Interface: Yeah .
Industrial Designer: We could add that but that's nothing we have thought of so far .
Project Manager: Which , which may be cheaper than speech recognition if it were just a {disfmarker}
Industrial Designer: Yes .
User Interface: Yeah true . But I mean d just those whistling , clapping key rings you have . They're cheap .
Marketing: Annoying alarm or something ?
Project Manager: {vocalsound}
Industrial Designer: It's it's {disfmarker}
Marketing: Yeah .
User Interface: So it can't be that
Industrial Designer: Um the {disfmarker} it's based on this anti anti-theft technology for suitcases and stuff ,
User Interface: expensive .
Project Manager: Some sort of proximity {vocalsound} {disfmarker}
User Interface: Yeah .
Industrial Designer: where you have one piece that's attached to your luggage , another piece that starts beeping . That can't cost much .
User Interface: Yeah .
Industrial Designer: So that can also easily be integrated because these things are small enough to to hide , so you have one piece , you have to glue somewhere behind your {disfmarker} stick it behind your T_V_ and the other {disfmarker}
User Interface: {gap} stick it on the T_V_ {gap} .
Project Manager: {vocalsound} Pray that you don't accidentally lose that piece . {vocalsound}
Industrial Designer: Right .
User Interface: {vocalsound}
Marketing: {vocalsound}
Industrial Designer: That'd be tough then . {vocalsound} Well also your remote would uh alarm you if somebody stole you t your television , yeah . Ran off with it without taking the beautiful remote control .
Project Manager: {vocalsound}
User Interface: Yeah . {vocalsound}
Marketing: So . Are we adding one of these two features ?
Industrial Designer: Let's add one of those features and say yes . {vocalsound}
Marketing: {gap} gonna say {disfmarker} okay .
Project Manager: Okay .
Marketing: So we're {vocalsound} back to a one ?
User Interface: Two .
Marketing: Or a two ?
Project Manager: Two .
Industrial Designer: Two .
Marketing: Two , 'kay . Okay . Are we technologically innovative ?
Industrial Designer: Uh {disfmarker}
User Interface: {vocalsound} I'd say so .
Industrial Designer: {vocalsound}
User Interface: Uh don't get many mo remote controls with
Industrial Designer: It's all just {disfmarker}
User Interface: screens on .
Industrial Designer: It's all just stolen technology when it comes down to {disfmarker} {vocalsound} {vocalsound}
User Interface: Yeah it's stolen technology .
Marketing: From iPod yeah . {vocalsound}
Project Manager: It's {disfmarker} {vocalsound}
User Interface: But we have {gap} .
Project Manager: But there's not a lot of yellow , there's not a lotta yellow .
Industrial Designer: right
Marketing: But for remotes {disfmarker} yeah .
Project Manager: Course that wasn't really {disfmarker}
Industrial Designer: right
User Interface: Fa
Industrial Designer: right right .
Project Manager: we were kinda forced to take that colour .
Marketing: Two ? Three ?
User Interface: {gap} 'cause it's stolen .
Project Manager: I don't know that we are that innovative , to tell you the truth . {vocalsound}
User Interface: No maybe not .
Industrial Designer: Yeah not really .
Marketing: But how many remotes do you see like this ?
User Interface: {vocalsound}
Project Manager: If we added the screaming factor {vocalsound} then we go up .
Industrial Designer: {vocalsound}
Marketing: Not so many .
Industrial Designer: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: {vocalsound} Um I would say we're probably at four .
Industrial Designer: Right .
Marketing: Really ? Okay . {vocalsound} That's gonna hurt us .
User Interface: {vocalsound}
Marketing: Okay . Um spongy material ?
Industrial Designer: Yeah well you have that , kind of , sort of .
Project Manager: We have some spongy , yeah .
User Interface: Yeah as much as as needed , I think .
Marketing: 'Kay .
Industrial Designer: It's not a one though .
Project Manager: No .
Industrial Designer: One would be the whole thing
Project Manager: Yeah . Because it's only got what , these parts are the grips and perhaps the back side {disfmarker} the bottom {disfmarker} the underneath on the back .
Industrial Designer: to fold and stuff . Yeah .
User Interface: Yeah .
Industrial Designer: So that's a four at most .
Project Manager: Probably a four at most . Possibly even a five .
Marketing: And lastly , did we put the fashion in electronics ?
Project Manager: {vocalsound}
Industrial Designer: Y yes .
User Interface: Yeah .
Marketing: I'd say we did .
Project Manager: If your fashion is b is Carmen Miranda , you betcha . {vocalsound}
Industrial Designer: More {disfmarker} {vocalsound}
User Interface: Yeah . {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound} Well the recent fashion is rather displayed in the in the L_C_D_ and the way you operate it than the form and the colour ,
User Interface: On the {disfmarker}
Project Manager: It's true .
User Interface: Yeah .
Industrial Designer: but it definitely is {disfmarker}
User Interface: Be what we were told , and they'd say yeah , definitely .
Industrial Designer: {gap} .
Marketing: 'Kay . Alright . Now we just gotta calculate . Six eight twelve sixteen . Seventeen divided by s
User Interface: {gap} .
Project Manager: Seven is {disfmarker}
Marketing: Eight .
Project Manager: Two point {disfmarker}
Marketing: {vocalsound}
Project Manager: {gap} two point four ?
User Interface: Is that some long division ? No .
Project Manager: Something .
Marketing: Well I haven't done math in years .
Industrial Designer: {vocalsound}
Marketing: What two {disfmarker}
User Interface: {vocalsound}
Marketing: {vocalsound} I dunno .
User Interface: Just , I'm sure there's a {gap} .
Marketing: Okay we'll say two point four two . Right ? How does that look ?
Industrial Designer: I'm impressed . I can't do that without a calculator . {vocalsound}
User Interface: No I can't do long {gap}
Marketing: {vocalsound} It's been a while .
User Interface: very impressive .
Project Manager: And what what is the acceptable criteria ? Is there like a scale that we have to hit ?
Marketing: Oh no . They just told me to
Industrial Designer: {vocalsound}
Marketing: {vocalsound} pick my own criteria and have you guys evaluate it {vocalsound} basically .
Project Manager: {vocalsound} Alright then .
Marketing: So that's that .
Project Manager: Okay . Well , let's see .
Marketing: {vocalsound}
Project Manager: Now we get to do the budget numbers . You didn't know that you were gonna have a budget . But we do . Okay .
User Interface: Yeah . Yeah so . You'd been going a long time dividing that . It's two point four two eight five se it just keeps going on .
Marketing: Oh my god .
User Interface: Two point four two basically .
Marketing: Okay . Yeah we'll go with that .
Project Manager: So I have here an {disfmarker}
Industrial Designer: {vocalsound} Fifty percent , you're kidding .
Marketing: Not too shabby .
Project Manager: Yeah .
Industrial Designer: {vocalsound} P
Project Manager: We want a fifty percent profit on this . Oh you can't really see that very well .
User Interface: {vocalsound} Charge about three hundred quid for it .
Project Manager: {vocalsound} Twelve and a half Euros is what supposed to cost us . Okay , so {disfmarker}
Industrial Designer: It's too much .
Project Manager: Well let's see .
Industrial Designer: Um {disfmarker}
Project Manager: The f the {disfmarker} Wonder if I can make this {disfmarker}
Industrial Designer: Uh {disfmarker}
Project Manager: What the {disfmarker} {vocalsound} Oh it won't let me do that . Okay . Alright so at top , I don't know if you guys can read that or not . I can't 'cause I don't have my glasses on ,
Industrial Designer: {vocalsound}
Project Manager: but so we've got the energy source . There's uh four , five , six categories .
Industrial Designer: Battery .
Project Manager: We have energy source , electronics , case . Then we have case material supplements , interface type , and then button supplements . Okay so {disfmarker} Uh first of all energy source , we picked battery . Um and how many batteries do we think this will probably take ?
User Interface: {vocalsound}
Project Manager: Probably some e either two or four .
Industrial Designer: Two .
Project Manager: Two ? {vocalsound} Like it . {vocalsound}
Industrial Designer: At four it's gonna be too heavy , so that that's not our problem . People can change it every month .
Project Manager: {vocalsound} Excellent .
Industrial Designer: {vocalsound} They won't know until after they bought it .
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: This is consumerism .
Industrial Designer: {vocalsound}
Project Manager: Alright so for the electronics our choices are simpl simple chip-on-print , regular chip-on-print , advanced chip-on-print , sample sensor , sample speaker .
Industrial Designer: {gap} .
User Interface: We're advanced chip are we ?
Industrial Designer: That's the advanced chip-on-print , yeah .
Project Manager: 'Kay , {gap} we have one of those . 'Kay then the case is a {disfmarker} Probably it's double curved .
Industrial Designer: Double curved , yes .
Project Manager: Case materials are
Industrial Designer: Plastic .
Project Manager: plastic . Um I guess it's two , since one for the top , one for the bottom .
Industrial Designer: N no .
Project Manager: Is that right or is it just one ?
Industrial Designer: No that's just one .
Project Manager: Maybe it's one because of the {disfmarker}
Industrial Designer: It's just one mo single mould , we can do that .
Project Manager: 'Kay .
User Interface: Yeah {gap} yeah .
Marketing: Right . {vocalsound}
Project Manager: I guess it doesn't matter 'cause the price on that one is zero , which is nice .
Industrial Designer: Exactly , right .
Marketing: Oh .
Project Manager: Special colour ?
Industrial Designer: That's not a special colour . It's a specially ugly colour , but it's not special .
Marketing: Bright yellow .
Project Manager: {vocalsound} Interface type . We have pushbutton , scroll-wheel interface , integrated scroll-wheel pushbutton , and an L_C_D_ display .
User Interface: {vocalsound}
Marketing: {vocalsound}
User Interface: S
Industrial Designer: S {vocalsound}
User Interface: That's {disfmarker} Yeah .
Project Manager: So we actually have the L_C_D_ display
Marketing: {vocalsound}
User Interface: And then {disfmarker}
Project Manager: and then is it the integrated or is it {disfmarker}
User Interface: I'd say the integrated .
Project Manager: Yeah .
Industrial Designer: Yes unfortunately .
Project Manager: 'Kay . Button supplement ? Special colour ?
User Interface: Mm .
Project Manager: Um special form ? Special material .
Industrial Designer: We could of course make the buttons wood .
Project Manager: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound} Say mahogany or so
Marketing: {vocalsound} It'd look really lovely .
Project Manager: Or titanium .
Industrial Designer: Mm-hmm or titanium .
Project Manager: They cost us all the same .
Marketing: {vocalsound} Yeah .
User Interface: {gap} remote control {gap} .
Project Manager: Well we only have one button so really we shouldn't be charged ,
Industrial Designer: Uh just {disfmarker}
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
Project Manager: {vocalsound} we shouldn't be charged anything for the the button supplements .
User Interface: No that's getting a bit tiny .
Project Manager: Um {disfmarker}
User Interface: Yeah .
Marketing: {vocalsound}
User Interface: I'd ignore that .
Marketing: Leave it blank .
Project Manager: Okay . We're gonna leave that one blank because we run on a L_C_D_ and scroll . So our total is fifteen point five . Which I believe is
Industrial Designer: Yeah that's too much .
Project Manager: by three Euros over .
Industrial Designer: It's hard to believe . So we'll go for the hand dynamo huh ? {vocalsound}
Project Manager: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: So the only thing better than um a banana-shaped remote is one that you shake .
User Interface: If it w What if we completely took out the the one single button we've got on .
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
User Interface: And just had a scroll wheel interface . And the L_C_D_ display . I suppose the L_C_D_ C_ display's the one that's pushing it up a bit though .
Project Manager: Yeah 'cause the {disfmarker}
Marketing: {vocalsound}
Project Manager: Well 'cause we have to have both right ?
User Interface: Yeah .
Industrial Designer: I mean let's let's face it , it also depends on the software on the on the television .
User Interface: Yeah .
Industrial Designer: You can have the the information that this thing transmits be being displayed on the on the screen .
Project Manager: Mm-hmm .
Industrial Designer: So s yeah let's take away the {disfmarker}
User Interface: Yeah you could maybe take out the L_C_D_ dis display even ,
Industrial Designer: Yeah . Yeah .
User Interface: if it if it comes up on the computer itsel on the T_V_ itself .
Industrial Designer: Right .
Project Manager: So we may not need the L_C_D_ display ?
User Interface: Uh that is possible yeah .
Industrial Designer: Right . We may not need it . There you go .
Project Manager: Well there we go .
Industrial Designer: Perfect .
Project Manager: Twelve point five .
User Interface: There we go .
Marketing: {vocalsound} Perfect .
Project Manager: Okay . So we just remove our {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
User Interface: Screen .
Marketing: {vocalsound}
Project Manager: screen here .
User Interface: Make it a bigger dial .
Industrial Designer: {vocalsound}
User Interface: Easier to use . Even easier to use then .
Project Manager: {vocalsound}
Industrial Designer: {vocalsound} Okay , the {disfmarker}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: Besides look at what the L_C_D_ does to our lovely remote .
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: Back to the design room boys .
Industrial Designer: So we can just take away a heck of a lot of the {disfmarker} {vocalsound}
Marketing: {vocalsound}
User Interface: {vocalsound}
Marketing: {gap} .
Industrial Designer: there you go . {gap} central ?
Marketing: What's the blue part ?
User Interface: That was just {disfmarker}
Industrial Designer: Oh that's just {disfmarker}
User Interface: we ran out of yellow . {vocalsound}
Marketing: Oh that's the batteries .
Industrial Designer: yeah .
Marketing: Okay . {vocalsound}
Industrial Designer: There you go
User Interface: There you go .
Industrial Designer: . Oops .
User Interface: Even simpler .
Marketing: {vocalsound} Looks more like a banana .
User Interface: Yeah .
Industrial Designer: There you go .
User Interface: For all those fruit lovers out there .
Industrial Designer: One more criteria .
Project Manager: {vocalsound}
Marketing: {vocalsound}
Project Manager: {vocalsound} Okay so the costs under twelve point five Euro . Was no . We redesigned it . Now it's yes .
User Interface: Yeah .
Marketing: {vocalsound}
Project Manager: Next slide . Project evaluation . Uh project process , satisfaction with , for example , room for creativity , leadership , teamwork , means , new ideas found . Um {disfmarker} So {disfmarker} I guess that {disfmarker} Let's see here . I think that perhaps the project evaluation's just supposed to be completed by me . But I'd like to hear your thoughts .
Marketing: {vocalsound}
Project Manager: {vocalsound}
Industrial Designer: {vocalsound} Fair enough . {vocalsound}
User Interface: {vocalsound}
Marketing: Trying to fill in some time there . {vocalsound}
Project Manager: Uh h what did you think of our project process ? {vocalsound}
Industrial Designer: Great . {vocalsound}
User Interface: I think we did {disfmarker} yeah I think we did quite well . Um {disfmarker}
Industrial Designer: Yeah .
Project Manager: Good .
Marketing: Good teamwork {gap} . {vocalsound}
Industrial Designer: Just half a day , you have a remote . There you go .
User Interface: Yeah . Right from the start of the day .
Project Manager: Yeah I think {disfmarker}
User Interface: We sort of knew where we were going straight away I thought .
Project Manager: {gap} we st we started off a little little weak . Our leadership was quite weak in the beginning .
Marketing: {vocalsound}
Project Manager: Um {vocalsound} um {disfmarker}
Marketing: {vocalsound}
Project Manager: But as the day went along we had more idea of what we were doing . Um room for creativity ? There was that . Um I think we tried a lotta different things and um I think it was um interesting as you guys brought up more um information and studies that we were right on with a lot of those things . Um you guys worked together well as a team . And um the means ? Which was the whiteboard and the pens .
User Interface: Yeah . We've used the whiteboard .
Industrial Designer: Super super .
Project Manager: I had some problem with the pen I think , but {vocalsound} minus your p
Marketing: Minus your PowerPoint fiasco .
Industrial Designer: {vocalsound} Well that's not my fault . That's obviously the people I work for uh that work for me ,
Marketing: No I know . I'm {disfmarker}
Project Manager: Well {disfmarker}
Marketing: yeah . Incom {vocalsound}
Industrial Designer: uh they've just you know {disfmarker}
User Interface: {vocalsound}
Project Manager: {vocalsound} Have a {disfmarker}
Industrial Designer: Heads are gonna roll , believe me .
Project Manager: we have a list of employees that you would like fired .
User Interface: {vocalsound}
Marketing: {vocalsound}
User Interface: {vocalsound}
Industrial Designer: Yes yes .
Project Manager: Okay . N new ideas found ? Um {disfmarker}
Industrial Designer: {vocalsound}
Marketing: Mm . Kinda .
Project Manager: Yes for the remote . Maybe no not f for
User Interface: Technology used .
Project Manager: technology . Alright . Closing . Costs are within the budget . Project is evaluated . Um complete the final questionnaire and meeting summary . That's it .
User Interface: Excellent .
Project Manager: And I still have to do my minutes for the last meeting . {vocalsound}
Marketing: {vocalsound}
Project Manager: Actually . Um so there will probably be another questionnaire coming up . And then we'll have to check with the main boss whether we can , what goes on after that .
Marketing: We might have a while though .
Industrial Designer: {gap} .
Project Manager: But that's the end of our meeting .
2022-06-15 06:44:09 | INFO | __main__ | output #1: Marketing had some evaluation criteria in mind, based on previous marketing strategy, on the latest trends, and on user preferences. The team should figure out whether their product could solve the complaints of the ugly remote control. There was a seven-point scale rating for each criterion. The team would give comments to each feature listed and agree on the final rating.
2022-06-15 06:44:09 | INFO | __main__ | 
Running tokenizer on train dataset:   0%|          | 0/2 [00:00<?, ?ba/s]Running tokenizer on train dataset:  50%|█████     | 1/2 [00:17<00:17, 17.77s/ba]Running tokenizer on train dataset: 100%|██████████| 2/2 [00:23<00:00, 14.18s/ba]Running tokenizer on train dataset: 100%|██████████| 2/2 [00:23<00:00, 11.80s/ba]
2022-06-15 06:44:33 | INFO | datasets.arrow_writer | Done writing 1257 examples in 17861718 bytes .
2022-06-15 06:44:33 | INFO | __main__ | 
2022-06-15 06:44:33 | INFO | __main__ | Validation examples before tokenization:
2022-06-15 06:44:33 | INFO | __main__ | input #0: What was agreed upon on sample transcripts?

Professor E: So . OK . Doesn't look like it crashed . That 's great .
Grad G: So I think maybe what 's causing it to crash is I keep starting it and then stopping it to see if it 's working . And so I think starting it and then stopping it and starting it again causes it to crash . So , I won't do that anymore .
Postdoc B: And it looks like you 've found a way of uh mapping the location to the {disfmarker} without having people have to give their names each time ?
PhD A: Sounds like an initialization thing .
Postdoc B: I mean it 's like you have the {disfmarker} So you know that {disfmarker}
Grad G: No .
Postdoc B: I mean , are you going to write down {pause} that I sat here ?
Grad G: I 'm gonna collect the digit forms and write it down .
Postdoc B: OK .
PhD C: Oh , OK .
Grad G: So {disfmarker} So they should be right with what 's on the digit forms . OK , so I 'll go ahead and start with digits . u And I should say that uh , you just pau you just read each line an and then pause briefly .
Professor E: And start by giving the transcript number .
PhD A: Tran
PhD D: Transcript {disfmarker} Uh . OK , OK .
PhD A: Oh sorry , go ahead .
Professor E: So uh , you see , Don , the unbridled excitement of the work that we have on this project .
Grad H: OK .
Professor E: It 's just uh {disfmarker}
Grad H: Umh .
Professor E: Uh , you know , it doesn't seem like a bad idea to have {comment} that information .
Grad G: And I 'm surprised I sort of {disfmarker} I 'm surprised I forgot that ,
Professor E: Yeah , I {disfmarker} I 'd {disfmarker} I think it 's some
Grad G: but uh I think that would be a good thing to add . After I just printed out a zillion of them .
Professor E: Yeah , well , that 's {disfmarker} Um , so I {disfmarker} I do have a {disfmarker} a an agenda suggestion . Uh , we {disfmarker} I think the things that we talk about in this meeting uh tend to be a mixture of uh procedural uh mundane things and uh research points and um I was thinking I think it was a meeting a couple of weeks ago that we {disfmarker} we spent much of the time talking about the mundane stuff cuz that 's easier to get out of the way and then we sort of drifted into the research and maybe five minutes into that Andreas had to leave . So {vocalsound} uh I 'm suggesting we turn it around and {disfmarker} and uh sort of we have {disfmarker} anybody has some mundane points that we could send an email later , uh hold them for a bit , and let 's talk about the {disfmarker} the research - y kind of things . Um , so um the one th one thing I know that we have on that is uh we had talked a {disfmarker} a couple weeks before um uh about the uh {disfmarker} the stuff you were doing with {disfmarker} with uh um uh l l attempting to locate events , we had a little go around trying to figure out what you meant by " events " but I think , you know , what we had meant by " events " I guess was uh points of overlap between speakers . But I th I gather from our discussion a little earlier today that you also mean uh interruptions with something else
PhD D: Yeah .
Professor E: like some other noise .
PhD D: Uh - huh . Yeah .
Professor E: Yes ? You mean that as an event also .
PhD D: To
Professor E: So at any rate you were {disfmarker} you 've {disfmarker} you 've done some work on that
PhD D: right .
Professor E: and um then the other thing would be it might be nice to have a preliminary discussion of some of the other uh research uh areas that uh we 're thinking about doing . Um , I think especially since you {disfmarker} you haven't been in {disfmarker} in these meetings for a little bit , maybe you have some discussion of some of the p the plausible things to look at now that we 're starting to get data , uh and one of the things I know that also came up uh is some discussions that {disfmarker} that uh {disfmarker} that uh Jane had with Lokendra uh about some {disfmarker} some {disfmarker} some um uh work about I {disfmarker} I {disfmarker} I d I {disfmarker} I don't want to try to say cuz I {disfmarker} I 'll say it wrong , but anyway some {disfmarker} some potential collaboration there about {disfmarker} about the {disfmarker} about the {disfmarker} working with these data .
PhD C: Oh . Sure .
Professor E: So . So , uh .
Grad G: You wanna just go around ?
Professor E: Uh . {pause} Well , I don't know if we {disfmarker} if this is sort of like everybody has something to contribute sort of thing , I think there 's just just a couple {disfmarker} a couple people primarily um but um Uh , wh why don't {disfmarker} Actually I think that {disfmarker} that last one I just said we could do fairly quickly so why don't you {disfmarker} you start with that .
Postdoc B: OK . Shall I {disfmarker} shall I just start ? OK .
Professor E: Yeah , just explain what it was .
Postdoc B: Um , so , uh , he was interested in the question of {disfmarker} you know , relating to his {disfmarker} to the research he presented recently , um of inference structures , and uh , the need to build in , um , this {disfmarker} this sort of uh mechanism for understanding of language . And he gave the example in his talk about how {pause} um , e a I 'm remembering it just off the top of my head right now , but it 's something about how um , i " Joe slipped " you know , " John had washed the floor " or something like that . And I don't have it quite right , but that kind of thing , where you have to draw the inference that , OK , there 's this time sequence , but also the {disfmarker} the {disfmarker} the causal aspects of the uh floor and {disfmarker} and how it might have been the cause of the fall and that um it was the other person who fell than the one who cleaned it and it {disfmarker} {comment} These sorts of things . So , I looked through the transcript that we have so far , {comment} and um , fou identified a couple different types of things of that type and um , one of them was something like uh , during the course of the transcript , um um , w we had gone through the part where everyone said which channel they were on and which device they were on , and um , the question was raised " Well , should we restart the recording at this point ? " And {disfmarker} and Dan Ellis said , " Well , we 're just so far ahead of the game right now {pause} we really don't need to " . Now , how would you interpret that without a lot of inference ? So , the inferences that are involved are things like , OK , so , how do you interpret " ahead of the game " ? You know . So it 's the {disfmarker} it 's {pause} i What you {disfmarker} what you int what you draw {disfmarker} you know , the conclusions that you need to draw are that space is involved in recording ,
Grad G: Hmm , metaphorically .
Postdoc B: that um , i that {pause} i we have enough space , and he continues , like " we 're so ahead of the game cuz now we have built - in downsampling " . So you have to sort of get the idea that um , " ahead of the game " is sp speaking with respect to space limitations , that um that in fact downsampling is gaining us enough space , and that therefore we can keep the recording we 've done so far . But there are a lot of different things like that .
Grad G: So , do you think his interest is in using this as {pause} a data source , or {pause} training material , or what ?
Professor E: Well , I {disfmarker} I should maybe interject to say this started off with a discussion that I had with him , so um we were trying to think of ways that his interests could interact with ours
Grad G: Mm - hmm .
Professor E: and um uh I thought that if we were going to project into the future when we had a lot of data , uh and um such things might be useful for that in or before we invested too much uh effort into that he should uh , with Jane 's help , look into some of the data that we 're {disfmarker} already have and see , is there anything to this at all ?
Grad G: Mm - hmm .
Professor E: Is there any point which you think that , you know , you could gain some advantage and some potential use for it . Cuz it could be that you 'd look through it and you say " well , this is just the wrong {pause} task for {disfmarker} for him to pursue his {disfmarker} "
Grad G: Wrong , yeah .
Professor E: And {disfmarker} and uh I got the impression from your mail that in fact there was enough things like this just in the little sample that {disfmarker} that you looked at that {disfmarker} that it 's plausible at least .
Postdoc B: It 's possible . Uh , he was {disfmarker} he {disfmarker} he {disfmarker} you know {disfmarker} We met and he was gonna go and uh you know , y look through them more systematically
Professor E: Yeah .
Postdoc B: and then uh meet again .
Professor E: Yeah .
Postdoc B: So it 's , you know , not a matter of a {disfmarker}
Professor E: Yeah .
Postdoc B: But , yeah , I think {disfmarker} I think it was optimistic .
Professor E: So anyway , that 's {disfmarker} that 's e a quite different thing from anything we 've talked about that , you know , might {disfmarker} might {disfmarker} might come out from some of this .
PhD C: But he can use text , basically . I mean , he 's talking about just using text
Postdoc B: That 's his major {disfmarker} I mentioned several that w had to do with implications drawn from intonational contours
PhD C: pretty much , or {disfmarker} ?
Postdoc B: and {pause} that wasn't as directly relevant to what he 's doing . He 's interested in these {disfmarker} these knowledge structures ,
PhD C: OK .
PhD D: Yeah , interesting .
Postdoc B: inferences that you draw {pause} i from {disfmarker}
Professor E: I mean , he certainly could use text , but we were in fact looking to see if there {disfmarker} is there {disfmarker} is there something in common between our interest in meetings and his interest in {disfmarker} in {disfmarker} in this stuff . So .
Grad G: And I imagine that transcripts of speech {disfmarker} I mean text that is speech {disfmarker} probably has more of those than sort of prepared writing . I {disfmarker} I don't know whether it would or not , but it seems like it would .
Professor E: I don't know , probably de probably depends on what the prepared writing was . But .
Postdoc B: Yeah , I don't think I would make that leap , because i in narratives , you know {disfmarker} I mean , if you spell out everything in a narrative , it can be really tedious ,
Grad G: Mm - hmm .
Postdoc B: so .
Grad G: Yeah , I 'm just thinking , you know , when you 're {disfmarker} when you 're face to face , you have a lot of backchannel and {disfmarker} And {disfmarker}
Postdoc B: Oh . That aspect .
Grad G: Yeah . And so I think it 's just easier to do that sort of broad inference jumping if it 's face to face . I mean , so , if I just read that Dan was saying " we 're ahead of the game " {comment} in that {disfmarker} in that context ,
Postdoc B: Well {disfmarker} Yeah .
Grad G: I might not realize that he was talking about disk space as opposed to anything else .
Postdoc B: I {disfmarker} you know , I {disfmarker} I had several that had to do with backchannels and this wasn't one of them .
Grad G: Uh - huh .
Postdoc B: This {disfmarker} this one really does um m make you leap from {disfmarker} So he said , you know , " we 're ahead of the game , w we have built - in downsampling " .
Grad G: Mm - hmm .
Postdoc B: And the inference , i if you had it written down , would be {disfmarker}
Grad G: I guess it would be the same .
Postdoc B: Uh - huh . But there are others that have backchannelling , it 's just he was less interested in those .
PhD F: Can I {disfmarker} Sorry to interrupt . Um , I f f f I 've {disfmarker} @ @ {comment} d A minute {disfmarker} uh , several minutes ago , I , like , briefly was {disfmarker} was not listening and {disfmarker} So who is " he " in this context ?
PhD C: Yeah , there 's a lot of pronoun {disfmarker}
PhD F: OK . So I was just realizing we 've {disfmarker} You guys have been talking about " he " um for at least uh , I don't know , three {disfmarker} three four minutes without ever mentioning the person 's name again .
PhD C: I believe it . Yeah . Actually to make it worse , {comment} uh , Morgan uses " you " and " you "
PhD F: So this is {disfmarker} this is {disfmarker} this is {disfmarker} gonna be a big , big problem if you want to later do uh , you know , indexing , or speech understanding of any sort .
Grad G: It 's in my notes .
PhD C: with gaze and no identification , or {disfmarker} I just wrote this down . Yeah , actually . Cuz Morgan will say well , " you had some ideas "
PhD D: Yeah .
PhD F: You just wrote this ?
PhD C: and he never said Li - He looked {disfmarker}
Grad G: Well , I think he 's doing that intentionally ,
PhD C: Right , so it 's great .
Grad G: aren't you ?
PhD C: So this is really great
PhD F: Right .
PhD C: because the thing is , because he 's looking at the per even for addressees in the conversation ,
PhD D: Yeah .
PhD F: Mm - hmm .
PhD C: I bet you could pick that up in the acoustics . Just because your gaze is also correlated with the directionality of your voice .
Professor E: Uh - huh . Could be .
Postdoc B: Can we
Professor E: Yeah . That would be tou
Grad G: Oh , that would be interesting .
PhD C: Yeah , so that , I mean , to even know um when {disfmarker}
PhD D: Yeah .
PhD C: Yeah , if you have the P Z Ms you should be able to pick up what a person is looking at from their voice .
Grad G: Well , especially with Morgan , with the way we have the microphones arranged . I 'm sort of right on axis and it would be very hard to tell .
PhD C: Right .
Grad G: Uh .
Postdoc B: Oh , but you 'd have the {disfmarker}
PhD C: Put Morgan always like this
Postdoc B: You 'd have fainter {disfmarker}
PhD C: and {disfmarker}
Postdoc B: Wouldn't you get fainter reception out here ?
Professor E: Well , these {disfmarker}
Grad G: Sure , but I think if I 'm talking like this ? Right now I 'm looking at Jane and talking , now I 'm looking at Chuck and talking , I don't think the microphones would pick up that difference .
PhD C: But you don't have this {disfmarker} this problem .
Postdoc B: I see .
PhD C: Morgan is the one who does this most .
Grad G: So if I 'm talking at you , or I 'm talking at you .
Professor E: I probably been affect No , I th I think I 've been affected by too many conversations where we were talking about lawyers and talking about {disfmarker} and concerns about " oh gee is somebody going to say something bad ? " and so on .
Grad G: Lawyers .
Professor E: And so I {disfmarker} so I 'm {disfmarker} I 'm tending to stay away from people 's names even though uh {disfmarker}
Postdoc B: I am too .
PhD C: Even though you could pick up later on , just from the acoustics who you were t who you were looking at .
Postdoc B: I am too .
Grad G: And we did mention who " he " was .
PhD C: Yeah .
Professor E: Yeah .
PhD F: Right , but I missed it .
Grad G: Early in the conversation .
PhD F: But {disfmarker} it was uh {disfmarker}
PhD C: Yeah , yeah .
Professor E: Yeah .
Grad G: Do {disfmarker} Sh - Can I say
Professor E: Yeah . No no , there 's {disfmarker}
PhD F: Yeah .
Grad G: or {disfmarker} or is that just too sensitive ?
Professor E: No no , it isn't sensitive at all .
Postdoc B: Well {disfmarker}
Professor E: I was just {disfmarker} I was just {disfmarker} I was overreacting just because we 've been talking about it .
Postdoc B: And in fact , it is {disfmarker} it is {disfmarker} it is sensitive .
PhD C: No , but that {disfmarker} it 's interesting .
Professor E: It 's OK to {disfmarker}
Postdoc B: I {disfmarker} I came up with something from the Human Subjects people that I wanted to mention . I mean , it fits into the m area of the mundane , but they did say {disfmarker} You know , I asked her very specifically about this clause of how , um , you know , it says " no individuals will be identified uh , " in any publication using the data . " OK , well , individuals being identified , let 's say you have a {disfmarker} a snippet that says , " Joe s uh thinks such - and - such about {disfmarker} about this field , but I think he 's wrongheaded . " Now I mean , we 're {disfmarker} we 're gonna be careful not to have the " wrongheaded " part in there , but {disfmarker} but you know , let 's say we say , you know , " Joe used to think so - and - so about this area , in his publication he says that but I think he 's changed his mind . " or whatever . Then the issue of {disfmarker} of being able to trace Joe , because we know he 's well - known in this field , and all this and {disfmarker} and tie it to the speaker , whose name was just mentioned a moment ago , can be sensitive .
Professor E: b But I {disfmarker}
Postdoc B: So I think it 's really {disfmarker} really kind of adaptive and wise to not mention names any more than we have to because if there 's a slanderous aspect to it , then how much to we wanna be able to have to remove ?
Professor E: Yeah , well , there 's that . But I {disfmarker} I mean I think also to some extent it 's just educating the Human Subjects people , in a way , because there 's {disfmarker} If uh {disfmarker} You know , there 's court transcripts , there 's {disfmarker} there 's transcripts of radio shows {disfmarker} I mean people say people 's names all the time . So I think it {disfmarker} it can't be bad to say people 's names . It 's just that {disfmarker} i I mean you 're right that there 's more poten If we never say anybody 's name , then there 's no chance of {disfmarker} of {disfmarker} of slandering anybody ,
PhD C: But , then it won't {disfmarker} I mean , if we {disfmarker} if we {disfmarker}
Professor E: but {disfmarker}
Grad G: It 's not a meeting .
PhD C: Yeah . I mean we should do whatever 's natural in a meeting if {disfmarker} if we weren't being recorded .
Professor E: Yeah . Right , so I {disfmarker} So my behavior is probably not natural .
PhD C: " If Person X {disfmarker} "
Professor E: So .
Postdoc B: Well , my feeling on it was that it wasn't really important who said it , you know .
Professor E: Yeah .
PhD F: Well , if you ha since you have to um go over the transcripts later anyway , you could make it one of the jobs of the {pause} people who do that to mark
Grad G: Well , we t we t we talked about this during the anon anonymization .
PhD F: Right .
Grad G: If we wanna go through and extract from the audio and the written every time someone says a name . And I thought that our conclusion was that we didn't want to do that .
Professor E: Yeah , we really can't . But a actually , I 'm sorry . I really would like to push {disfmarker} finish this off .
Postdoc B: I understand . No I just {disfmarker} I just was suggesting that it 's not a bad policy p potentially .
Professor E: So it 's {disfmarker}
Postdoc B: So , we need to talk about this later .
Professor E: Yeah , I di I didn't intend it an a policy though .
Postdoc B: Uh - huh .
Professor E: It was {disfmarker} it was just it was just unconscious {disfmarker} well , semi - conscious behavior . I sorta knew I was doing it but it was {disfmarker}
PhD F: Well , I still don't know who " he " is .
Professor E: I {disfmarker} I do I don't remember who " he " is .
PhD C: No , you have to say , you still don't know who " he " is , with that prosody .
Professor E: Ah . Uh , we were talking about Dan at one point {comment} and we were talking about Lokendra at another point .
Postdoc B: Yeah , depends on which one you mean .
Professor E: And I don't {disfmarker} I don't remember which {disfmarker} which part .
PhD F: Oh .
PhD C: It 's ambiguous , so it 's OK .
Professor E: Uh , I think {disfmarker}
Grad G: Well , the inference structures was Lokendra .
PhD F: But no . The inference stuff was {disfmarker} was {disfmarker} was Lokendra .
Professor E: Yeah . Yeah . Yeah .
PhD F: OK . That makes sense , yeah .
PhD C: And the downsampling must have been Dan .
Professor E: Um {disfmarker}
Grad G: Yeah .
Professor E: Good {disfmarker} Yeah .
PhD C: It 's an inference .
Professor E: Yeah , you could do all these inferences , yeah .
Grad G: Yeah .
Professor E: Yeah . Um , I {disfmarker} I would like to move it into {disfmarker} into uh what Jose uh has been doing
Postdoc B: Yeah .
Professor E: because he 's actually been doing something .
PhD D: Uh - huh . OK .
Professor E: So . {vocalsound} Right .
PhD F: As opposed to the rest of us .
PhD D: Well - {comment} {vocalsound} OK . I {disfmarker} I remind that me {disfmarker} my first objective eh , in the project is to {disfmarker} to study difference parameters to {disfmarker} to find a {disfmarker} a good solution to detect eh , the overlapping zone in eh speech recorded . But eh , {vocalsound} tsk , {comment} {vocalsound} ehhh {comment} In that way {comment} I {disfmarker} {vocalsound} I {disfmarker} {vocalsound} I begin to {disfmarker} to study and to analyze the ehn {disfmarker} the recorded speech eh the different session to {disfmarker} to find and to locate and to mark eh the {disfmarker} the different overlapping zone . And eh so eh I was eh {disfmarker} I am transcribing the {disfmarker} the first session and I {disfmarker} I have found eh , eh one thousand acoustic events , eh besides the overlapping zones , eh I {disfmarker} I {disfmarker} I mean the eh breaths eh aspiration eh , eh , talk eh , eh , clap , eh {disfmarker} {comment} I don't know what is the different names eh you use to {disfmarker} to name the {disfmarker} the {pause} n speech
PhD A: Nonspeech sounds ?
PhD D: Yeah .
Grad G: Oh , I don't think we 've been doing it at that level of detail . So .
PhD D: Yeah . Eh , {vocalsound} I {disfmarker} I {disfmarker} I do I don't need to {disfmarker} to {disfmarker} to mmm {vocalsound} {disfmarker} to m to label the {disfmarker} the different acoustic , but I prefer because eh I would like to {disfmarker} to study if eh , I {disfmarker} I will find eh , eh , a good eh parameters eh to detect overlapping I would like to {disfmarker} to {disfmarker} to test these parameters eh with the {disfmarker} another eh , eh acoustic events , to nnn {disfmarker} {vocalsound} to eh {disfmarker} to find what is the ehm {disfmarker} the false {disfmarker} eh , the false eh hypothesis eh , nnn , which eh are produced when we use the {disfmarker} the ehm {disfmarker} this eh parameter {disfmarker} eh I mean pitch eh , eh , difference eh , feature {disfmarker}
Grad G: Mm - hmm .
PhD A: You know {disfmarker} I think some of these um that are the nonspeech overlapping events may be difficult even for humans to tell that there 's two there .
Grad G: So it was {disfmarker}
PhD D: Yeah .
PhD A: I mean , if it 's a tapping sound , you wouldn't necessarily {disfmarker} or , you know , something like that , it 'd be {disfmarker} it might be hard to know that it was two separate events .
PhD D: Yeah . Yeah . Yeah . Yeah .
Grad G: Well {disfmarker} You weren't talking about just overlaps
PhD D: Ye
Grad G: were you ? You were just talking about acoustic events .
PhD D: I {disfmarker} I {disfmarker} I {disfmarker} I t I t I talk eh about eh acoustic events in general ,
Grad G: Someone starts , someone stops {disfmarker} Yeah .
PhD A: Oh .
PhD D: but eh my {disfmarker} my objective eh will be eh to study eh overlapping zone .
Grad G: Mm - hmm .
PhD D: Eh ? {comment} n Eh in twelve minutes I found eh , eh one thousand acoustic events .
Professor E: How many overlaps were there uh in it ? No no , how many of them were the overlaps of speech , though ?
PhD D: How many ? Eh almost eh three hundred eh in one session
Grad G: Oh , God !
PhD D: in five {disfmarker} eh in forty - five minutes .
PhD A: Three hundred overlapping speech {disfmarker}
PhD D: Alm - Three hundred overlapping zone .
Grad G: Ugh .
PhD C: Overlapping speech .
PhD D: With the overlapping zone , overlapping speech {disfmarker} speech what eh different duration .
PhD A: Mm - hmm .
Professor E: Sure .
Postdoc B: Does this {disfmarker} ? So if you had an overlap involving three people , how many times was that counted ?
PhD D: Yeah , three people , two people . Eh , um I would like to consider eh one people with difference noise eh in the background , be
Professor E: No no , but I think what she 's asking is {pause} if at some particular for some particular stretch you had three people talking , instead of two , did you call that one event ?
PhD D: Oh . Oh . Yeah . I consider one event eh for th for that eh for all the zone . This {disfmarker} th I {disfmarker} I {disfmarker} I con I consider {disfmarker} I consider eh an acoustic event , the overlapping zone , the period where three speaker or eh {disfmarker} are talking together .
Grad G: Well {disfmarker} So let 's {disfmarker}
Postdoc B: For
Grad G: So let 's say me and Jane are talking at the same time , and then Liz starts talking also over all of us . How many events would that be ?
PhD D: So - I don't understand .
Grad G: So , two people are talking , {comment} and then a third person starts talking .
PhD D: Yeah ?
Grad G: Is there an event right here ?
PhD D: Eh no . No no . For me is the overlapping zone , because {disfmarker} because you {disfmarker} you have s you have more one {disfmarker} eh , more one voice eh , eh produced in a {disfmarker} in {disfmarker} in a moment .
Professor E: I see .
Grad G: So i if two or more people are talking .
Professor E: OK . Yeah . So I think {disfmarker} Yeah . We just wanted to understand how you 're defining it .
PhD D: Yeah . If
Professor E: So then , in the region between {disfmarker} since there {disfmarker} there is some continuous region , in between regions where there is only one person speaking .
PhD D: Uh - huh .
Professor E: And one contiguous region like that you 're calling an event .
PhD D: Uh - huh .
Professor E: Is it {disfmarker} Are you calling the beginning or the end of it the event ,
PhD D: Yeah .
Professor E: or are you calling the entire length of it the event ?
PhD D: I consider the {disfmarker} the , nnn {disfmarker} the nnn , nnn {disfmarker} eh , the entirety eh , eh , all {disfmarker} all the time there were {disfmarker} the voice has overlapped .
Professor E: OK .
PhD D: This is the idea . But eh I {disfmarker} I don't distinguish between the {disfmarker} the numbers of eh speaker . Uh , I 'm not considering {vocalsound} eh the {disfmarker} the {disfmarker} ehm {vocalsound} eh , the fact of eh , eh , for example , what did you say ? Eh at first eh , eh two talkers are uh , eh speaking , and eh , eh a third person eh join to {disfmarker} to that . For me , it 's eh {disfmarker} it 's eh , all overlap zone , with eh several numbers of speakers is eh , eh the same acoustic event . Wi - but {disfmarker} uh , without any mark between the zone {disfmarker} of the overlapping zone with two speakers eh speaking together , and the zone with the three speakers .
Postdoc B: That would j just be one .
PhD D: It {disfmarker} One . One .
Postdoc B: OK .
PhD D: Eh , with eh , a beginning mark and the ending mark . Because eh {vocalsound} for me , is the {disfmarker} is the zone with eh some kind of eh distortion the spectral .
Professor E: Got it .
PhD D: I don't mind {disfmarker} By the moment , by the moment .
Grad G: Well , but {disfmarker} But you could imagine that three people talking has a different spectral characteristic than two .
PhD D: I {disfmarker} I don't {disfmarker} Yeah , but eh {disfmarker} but eh I have to study . {comment} What will happen in a general way ,
Professor E: Could .
Grad G: So . You had to start somewhere .
Professor E: Yeah . We just w
PhD C: So there 's a lot of overlap .
PhD D: I {disfmarker} {vocalsound} I don't know what eh will {disfmarker} will happen with the {disfmarker}
Grad G: Yep .
PhD C: So .
Grad G: That 's a lot of overlap ,
PhD D: Yeah ?
Professor E: So again , that 's {disfmarker} that 's three {disfmarker} three hundred in forty - five minutes that are {disfmarker} that are speakers , just speakers .
Grad G: yeah , for forty - five minutes .
PhD D: Yeah . Yeah .
Professor E: Uh - huh . OK . Yeah .
Postdoc B: But a {disfmarker} a {disfmarker} a th
Professor E: So that 's about eight per minute .
Postdoc B: But a thousand events in twelve minutes , that 's {disfmarker}
PhD D: Yeah , {pause} but {disfmarker} Yeah .
PhD C: But that can include taps .
PhD D: But {disfmarker}
Professor E: Uh . Yeah .
Postdoc B: Well , but a thousand taps in eight minutes is a l in twelve minutes is a lot .
PhD D: General .
PhD C: Actually {disfmarker}
PhD D: I {disfmarker} I con I consider {disfmarker} I consider acoustic events eh , the silent too .
Postdoc B: Silent .
Grad G: Silence starting or silence ending {disfmarker}
PhD D: Yeah , silent , ground to {disfmarker} bec to detect {disfmarker} eh because I consider acoustic event all the things are not eh speech .
PhD C: Oh , OK .
Professor E: Mm - hmm .
PhD A: Oh .
PhD D: In ge in {disfmarker} in {disfmarker} in a general point of view .
PhD C: Oh .
Professor E: OK , so how many of those thousand were silence ?
PhD C: Alright .
PhD D: in the per
PhD F: Not speech {disfmarker} not speech or too much speech .
PhD D: Too much speech .
Professor E: Right . So how many of those thousand were silence , silent sections ?
PhD D: Yeah . Uh silent , I {disfmarker} I {disfmarker} I {disfmarker} I don't {disfmarker} I {disfmarker} I haven't the {disfmarker} eh I {disfmarker} I would like to {disfmarker} to do a stylistic study
Professor E: Yeah .
PhD D: and give you eh with the report eh from eh the {disfmarker} the study from the {disfmarker} the {disfmarker} the session {disfmarker} one session .
Professor E: Yeah . Yeah .
PhD D: And I {disfmarker} I found that eh another thing . When eh {vocalsound} eh I w I {disfmarker} {vocalsound} I was eh look at eh nnn , the difference speech file , um , for example , eh if eh we use the ehm {disfmarker} the mixed file , to {disfmarker} to transcribe , the {disfmarker} the events and the words , I {disfmarker} I saw that eh the eh speech signal , collected by the eh this kind of mike {disfmarker} eh of this kind of mike , eh are different from the eh mixed signal eh , we eh {disfmarker} collected by headphone .
Grad G: Yep .
PhD D: And {disfmarker} It 's right .
Professor E: Yeah .
Grad G: Right .
PhD D: But the problem is {vocalsound} the following . The {disfmarker} the {disfmarker} the {disfmarker} I {disfmarker} I {disfmarker} I knew that eh the signal eh , eh would be different , but eh the {disfmarker} the problem is eh , eh we eh detected eh difference events in the speech file eh collected by {disfmarker} by that mike uh qui compared with the mixed file . And so if {disfmarker} when you transcribe eh only eh using the nnn {disfmarker} the mixed file , it 's possible {disfmarker} eh if you use the transcription to evaluate a different system , it 's possible you eh {disfmarker} in the eh i and you use the eh speech file collected by the eh fet mike , to eh {disfmarker} to nnn {disfmarker} to do the experiments {pause} with the {disfmarker} the system ,
Professor E: Mm - hmm .
Grad G: Right .
PhD D: its possible to evaluate eh , eh {disfmarker} or to consider eh acoustic events that {disfmarker} which you marked eh in the mixed file , but eh they don't appear in the eh speech signal eh collected by the {disfmarker} by the mike .
Grad G: Right . The {disfmarker} the reason that I generated the mixed file was for IBM to do word level transcription , not speech event transcription .
PhD D: Yeah . Yeah . Oh , it 's a good idea . It 's a good idea I think .
Grad G: So I agree that if someone wants to do speech event transcription , that the mixed signals here {disfmarker}
PhD D: Yeah .
Grad G: I mean , if I 'm tapping on the table , you it 's not gonna show up on any of the mikes , but it 's gonna show up rather loudly in the PZM .
PhD D: Yeah . Yeah . Yeah . So and I {disfmarker} I {disfmarker} {vocalsound} I say eh that eh , eh , or this eh only because eh I c I {disfmarker} I {disfmarker} {vocalsound} in my opinion , it 's necessary to eh {disfmarker} to eh {disfmarker} to put the transcription on the speech file , collected by the objective signal .
Grad G: So .
PhD D: I mean the {disfmarker} the {disfmarker} the signal collected by the {disfmarker} eh , the real mike in the future , in the prototype to {disfmarker} to eh correct the initial eh segmentation eh with the eh real speech
Professor E: Mm - hmm . The {disfmarker} the {disfmarker} the far - field , yeah .
PhD D: you have to {disfmarker} to analyze {disfmarker} you have to {disfmarker} to process . Because I {disfmarker} I found a difference .
Professor E: Yeah , well , just {disfmarker} I mean , just in that {disfmarker} that one s ten second , or whatever it was , example that Adam had that {disfmarker} that we {disfmarker} we passed on to others a few months ago , there was that business where I g I guess it was Adam and Jane were talking at the same time and {disfmarker} and uh , in the close - talking mikes you couldn't hear the overlap , and in the distant mike you could . So yeah , it 's clear that if you wanna study {disfmarker} if you wanna find all the places where there were overlap , it 's probably better to use a distant mike .
PhD F: That 's good .
Professor E: On the other hand , there 's other phenomena that are going on at the same time for which it might be useful to look at the close - talking mikes ,
PhD D: Yeah .
PhD C: But why can't you use the combination of the close - talking mikes , time aligned ?
Professor E: so it 's {disfmarker}
Grad G: If you use the combination of the close - talking mikes , you would hear Jane interrupting me , but you wouldn't hear the paper rustling . And so if you 're interested in {disfmarker}
PhD C: I {disfmarker} I mean if you 're interested in speakers overlapping other speakers and not the other kinds of nonspeech , that 's not a problem ,
Professor E: Some {comment} of it 's masking {disfmarker} masked .
PhD D: Yeah .
PhD A: Were you interrupting him or was he interrupting you ?
Professor E: Right .
PhD C: right ?
Grad G: Right .
PhD D: Yeah .
Grad G: Although the other issue is that the {pause} mixed close - talking mikes {disfmarker} I mean , I 'm doing weird normalizations and things like that .
PhD C: But it 's known .
PhD D: Yeah .
PhD C: I mean , the normalization you do is over the whole conversation
Grad G: Yep .
PhD C: isn't it , over the whole meeting .
Grad G: Right . Yep .
PhD C: So if you wanted to study people overlapping people , that 's not a problem .
PhD D: I {disfmarker} I {disfmarker} I think eh I saw the nnn {disfmarker} the {disfmarker} eh but eh I eh {disfmarker} I have eh any results . I {disfmarker} I {disfmarker} I saw the {disfmarker} the speech file collected by eh the fet mike , and eh eh signal eh to eh {disfmarker} to noise eh relation is eh low . It 's low .
Professor E: Mm - hmm .
PhD D: It 's very low . You would comp if we compare it with eh the headphone .
Grad G: Yep .
PhD D: And I {disfmarker} I found that nnn {disfmarker} that eh , {vocalsound} ehm , pr probably ,
Grad G: Did {disfmarker} Did you
PhD D: I 'm not sure eh by the moment , but it 's {disfmarker} it 's probably that eh a lot of eh , {vocalsound} eh for example , in the overlapping zone , on eh {disfmarker} in {disfmarker} in several eh parts of the files where you {disfmarker} you can find eh , eh {vocalsound} eh , smooth eh eh speech eh from eh one eh eh talker in the {disfmarker} in the meeting ,
Professor E: Mm - hmm . Mm - hmm .
PhD D: it 's probably in {disfmarker} in that eh {disfmarker} in {disfmarker} in those files you {disfmarker} you can not find {disfmarker} you can not process because eh it 's confused with {disfmarker} with noise .
Professor E: Mm - hmm .
PhD D: And there are {vocalsound} a lot of I think . But I have to study with more detail . But eh my idea is to {disfmarker} to process only {pause} nnn , this eh {disfmarker} nnn , this kind of s of eh speech . Because I think it 's more realistic . I 'm not sure it 's a good idea , but eh {disfmarker}
Professor E: No {disfmarker} i
Grad G: Well , it 's more realistic but it 'll {disfmarker} it 'll be a lot harder .
PhD D: Yeah .
Professor E: Well , it 'd be hard , but on the other hand as you point out , if your {disfmarker} if i if {disfmarker} if your concern is to get uh the overlapping people {disfmarker} people 's speech , you will {disfmarker} you will get that somewhat better .
PhD D: Mm - hmm . Yeah .
Professor E: Um , Are you making any use {disfmarker} uh you were {disfmarker} you were working with th the data that had already been transcribed .
PhD D: With {disfmarker} By Jane .
Professor E: Does it uh {disfmarker} Yes .
PhD D: Yeah .
Professor E: Now um did you make any use of that ? See I was wondering cuz we st we have these ten hours of other stuff that is not yet transcribed .
PhD D: Yeah . Yeah .
Professor E: Do you {disfmarker}
PhD D: The {disfmarker} the transcription by Jane , t eh i eh , I {disfmarker} I {disfmarker} I want to use to {disfmarker} to nnn , {vocalsound} eh to put {disfmarker} i i it 's a reference for me . But eh the transcription {disfmarker} eh for example , I {disfmarker} I don't {disfmarker} I {disfmarker} I 'm not interested in the {disfmarker} in the {disfmarker} in the words , transcription words , eh transcribed eh eh in {disfmarker} eh follow in the {disfmarker} {vocalsound} in the {disfmarker} in the speech file , but eh eh Jane eh for example eh put a mark eh at the beginning eh of each eh talker , in the {disfmarker} in the meeting , um eh she {disfmarker} she nnn includes information about the zone where eh there are eh {disfmarker} there is an overlapping zone . But eh there isn't any {disfmarker} any mark , time {disfmarker} temporal mark , to {disfmarker} to c eh {disfmarker} to mmm {vocalsound} {disfmarker} e - heh , to label {comment} the beginning and the end of the {disfmarker} of the
Professor E: Mm - hmm . OK . Right , so she is {disfmarker}
PhD D: ta I 'm {disfmarker} I {disfmarker} I {disfmarker} I think eh we need this information to
Professor E: Right . So the twelve {disfmarker} you {disfmarker} you {disfmarker} it took you twelve hours {disfmarker} of course this included maybe some {disfmarker} some time where you were learning about what {disfmarker} what you wanted to do , but {disfmarker} but uh , it took you something like twelve hours to mark the forty - five minutes , your
Grad G: Twelve minutes .
PhD D: Twelve minutes .
Professor E: s Twelve minutes !
PhD D: Twelve minutes . Twelve .
Professor E: I thought you did forty - five minutes of {disfmarker}
PhD D: No , forty - five minutes is the {disfmarker} is the session , all the session .
Postdoc B: Oh .
Professor E: Oh , you haven't done the whole session .
PhD D: Yeah , all is the {vocalsound} the session .
Professor E: This is just twelve minutes .
PhD D: Tw - twelve hours of work to {disfmarker} {vocalsound} to segment eh and label eh twelve minutes from a session of part {disfmarker} of f
Professor E: Oh . So {comment} let me back up again . So the {disfmarker} when you said there were three hundred speaker overlaps ,
PhD D: Yeah .
Professor E: that 's in twelve minutes ?
PhD D: No no no . I {disfmarker} I consider all the {disfmarker} all the session because eh I {disfmarker} I count the nnn {disfmarker} the nnn {disfmarker} the overlappings marked by {disfmarker} by Jane ,
Professor E: Oh , OK .
Postdoc B: Oh , I see .
PhD D: in {disfmarker} in {disfmarker} in {disfmarker} in the {pause} fin in {disfmarker} in the {pause} forty - five minutes .
Professor E: OK . So it 's three hundred in forty - five minutes , but you have {disfmarker} you have time uh , uh marked {disfmarker} twelve minute {disfmarker} the {disfmarker} the {disfmarker} the um overlaps in twelve minutes of it .
PhD D: Yeah .
Professor E: Got it .
PhD F: So , can I ask {disfmarker} {vocalsound} can I ask whether you found {disfmarker} uh , you know , how accurate uh Jane 's uh uh labels were as far as {disfmarker}
Grad G: Well , not just the overlaps , everything .
PhD F: you know , did she miss some overlaps ? or did she n ?
PhD D: But , by {disfmarker} by the moment , I {disfmarker} I don't compare , my {disfmarker} my temporal mark with eh Jane , but eh I {disfmarker} I want to do it . Because eh eh i per perhaps I have eh errors in the {disfmarker} in the marks , I {disfmarker} and if I {disfmarker} I compare with eh Jane , it 's probably I {disfmarker} I {disfmarker} I can correct and {disfmarker} and {disfmarker} and {disfmarker} to get eh eh a more accurately eh eh transcription in the file .
Professor E: Yeah .
Grad G: Well , also Jane {disfmarker} Jane was doing word level .
PhD D: Yeah .
Professor E: Yeah .
Grad G: So we weren't concerned with {comment} exactly when an overlap started and stopped .
PhD F: Right . Right .
PhD C: Well , not only a word level , but actually
PhD D: Well {disfmarker}
PhD F: I 'm expect I 'm not expecting {disfmarker}
PhD D: No , it 's {disfmarker}
PhD C: I mean , you didn't need to show the exact point of interruption , you just were showing at the level of the phrase or the level of the speech spurt , or {disfmarker}
Grad G: Right .
Professor E: Mm - hmm .
Grad G: Yep .
Postdoc B: Well {disfmarker}
PhD D: Yeah . Yeah .
Postdoc B: Well , yeah , b yeah , I would say time bin . So my {disfmarker} my goal is to get words with reference to a time bin , {pause} beginning and end point .
PhD C: Yeah .
PhD D: Yeah .
PhD C: Right .
PhD D: Yeah .
Postdoc B: And {disfmarker} and sometimes , you know , it was like you could have an overlap where someone said something in the middle ,
PhD D: Yeah .
Postdoc B: but , yeah , w it just wasn't important for our purposes to have it that {disfmarker} i disrupt that unit in order to have , you know , a the words in the order in which they were spoken , it would have {disfmarker} it would have been hard with the interface that we have .
PhD D: Yeah .
Postdoc B: Now , my {disfmarker} a Adam 's working on a of course , on a revised overlapping interface ,
PhD D: Uh - huh .
Grad G: Right .
PhD D: I {disfmarker} I {disfmarker} I think {disfmarker} It 's {disfmarker} it 's a good eh work ,
Postdoc B: but {disfmarker}
PhD D: but eh I think we need eh eh more information .
PhD F: No , of course .
Postdoc B: Yeah .
PhD F: I expect you to find more overlaps than {disfmarker} than Jane
Grad G: Always need more for {disfmarker}
Postdoc B: Yeah .
PhD D: No , no . I {disfmarker} I have to go to {disfmarker}
PhD F: because you 're looking at it at a much more detailed level .
PhD D: I want eh {disfmarker} I wanted to eh compare the {disfmarker} the transcription .
Professor E: I have {disfmarker}
Grad G: But if it takes sixty to one {disfmarker}
Professor E: Well , I but I have a suggestion about that . Um , obviously this is very , very time - consuming , and you 're finding lots of things which I 'm sure are gonna be very interesting , but in the interests of making progress , uh might I s how {disfmarker} how would it affect your time if you only marked speaker overlaps ?
PhD D: Only .
Professor E: Yes .
PhD D: Yeah .
Professor E: Do not mark any other events ,
PhD D: Uh - huh .
Professor E: but only mark speaker {disfmarker} Do you think that would speed it up quite a bit ?
PhD D: OK . OK . I {disfmarker} I {disfmarker} I {disfmarker} I w I {disfmarker} I wanted to {disfmarker}
Professor E: Do y do you think that would speed it up ? Uh , speed up your {disfmarker} your {disfmarker} your marking ?
PhD D: nnn , I don't understand very .
Professor E: It took you a long time {pause} to mark twelve minutes .
PhD D: Yeah . Oh , yeah , yeah .
Professor E: Now , my suggestion was for the other thirty - three {disfmarker}
PhD D: On - only to mark {disfmarker} only to mark overlapping zone , but {disfmarker}
Professor E: Yeah , and my question is , if you did that , if you followed my suggestion , would it take much less time ?
PhD D: Oh , yeah . Sure .
Professor E: Yeah OK .
PhD D: Yeah sure .
Professor E: Then I think it 's a good idea .
PhD D: Sure sure .
Professor E: Then I think it 's a good idea , because it
PhD D: Sure , because I {disfmarker} I need a lot of time to {disfmarker} to put the label or to do that . Yeah .
Professor E: Yeah , I mean , we we know that there 's noise .
Grad G: And
PhD D: Uh - huh .
Professor E: There 's {disfmarker} there 's uh continual noise uh from fans and so forth , and there is uh more impulsive noise from uh taps and so forth
PhD D: Yeah .
Professor E: and {disfmarker} and something in between with paper rustling . We know that all that 's there and it 's a g worthwhile thing to study , but obviously it takes a lot of time to mark all of these things .
PhD D: Yeah .
Professor E: Whereas th i I would think that uh you {disfmarker} we can study more or less as a distinct phenomenon the overlapping of people talking .
PhD D: Uh - huh . OK . OK .
Professor E: So . Then you can get the {disfmarker} Cuz you need {disfmarker} If it 's three hundred uh {disfmarker} i i it sounds like you probably only have fifty or sixty or seventy events right now that are really {disfmarker}
PhD D: Yeah .
Professor E: And {disfmarker} and you need to have a lot more than that to have any kind of uh even visual sense of {disfmarker} of what 's going on , much less any kind of reasonable statistics .
Grad G: Right .
PhD C: Now , why do you need to mark speaker overlap by hand if you can infer it from the relative energy in the {disfmarker}
Grad G: Well , that 's {disfmarker} That 's what I was gonna bring up .
PhD C: I mean , you shouldn't need to do this p completely by hand ,
Professor E: Um , OK , yeah . So let 's back up because you weren't here for an earlier conversation .
PhD C: right ? I 'm sorry .
Professor E: So the idea was that what he was going to be doing was experimenting with different measures such as the increase in energy , such as the energy in the LPC residuals , such as {disfmarker} I mean there 's a bunch of things {disfmarker} I mean , increased energy is - is sort of an obvious one .
PhD C: Mm - hmm . In the far - field mike .
Professor E: Yeah .
PhD C: Oh , OK .
Professor E: Um , and uh , it 's not obvious , I mean , you could {disfmarker} you could do the dumbest thing and get {disfmarker} get it ninety percent of the time . But when you start going past that and trying to do better , it 's not obvious what combination of features is gonna give you the {disfmarker} you know , the right detector . So the idea is to have some ground truth first . And so the i the idea of the manual marking was to say " OK this , i you know , it 's {disfmarker} it 's really here " .
PhD A: But I think Liz is saying why not get it out of the transcripts ?
PhD C: What I mean is {pause} get it from the close - talking mikes .
Professor E: Uh , yeah .
PhD C: A or ge get a first pass from those ,
Professor E: We t we t w we t we talked about that .
PhD C: and then go through sort of {disfmarker} It 'd be a lot faster probably to {disfmarker}
PhD F: And you can {disfmarker}
Grad G: Yeah , that 's his , uh {disfmarker}
Professor E: We {disfmarker} we {disfmarker} we talked about that . s But so it 's a bootstrapping thing and the thing is ,
PhD C: Yeah , I just {disfmarker}
Professor E: the idea was , i we i i we thought it would be useful for him to look at the data anyway , and {disfmarker} and then whatever he could mark would be helpful ,
PhD C: Right .
Professor E: and we could {disfmarker} Uh it 's a question of what you bootstrap from . You know , do you bootstrap from a simple measurement which is right most of the time and then you g do better , or do you bootstrap from some human being looking at it and then {disfmarker} then do your simple measurements , uh from the close - talking mike . I mean , even with the close - talking mike you 're not gonna get it right all the time .
PhD C: Well , that 's what I wonder , because um {disfmarker} or how bad it is ,
Professor E: Well
PhD C: be um , because that would be interesting
Grad G: I 'm working on a program to do that , and {disfmarker}
PhD C: especially because the bottleneck is the transcription . Right ? I mean , we 've got a lot more data than we have transcriptions for . We have the audio data , we have the close - talking mike ,
Professor E: Yeah .
PhD C: so I mean it seems like one kind of project that 's not perfect , but {disfmarker} um , that you can get the training data for pretty quickly is , you know , if you infer form the close - talking mikes where the on - off points are of speech ,
Professor E: Right , we discussed that .
PhD C: you know , how can we detect that from a far - field ?
Grad G: And {disfmarker}
Postdoc B: Oh .
Grad G: I 've {disfmarker} I 've written a program to do that ,
PhD C: OK , I 'm sorry I missed the {disfmarker}
Grad G: and it , uh {disfmarker}
Professor E: It 's OK .
Grad G: and {disfmarker} so {disfmarker} but it 's {disfmarker} it 's doing something very , very simple . It just takes a threshold , based on {disfmarker} on the volume ,
PhD C: Uh - huh .
PhD F: Or you can set the threshold low and then weed out the false alarms by hand .
PhD C: Right , by hand . Yeah .
PhD F: Yeah .
Grad G: um , and then it does a median filter , and then it looks for runs . And , it seems to work , I 've {disfmarker} I 'm sort of fiddling with the parameters , to get it to actually generate something , and I haven't {disfmarker} I don't {disfmarker} what I 'm working on {disfmarker} was working on {disfmarker} was getting it to a form where we can import it into the user interface that we have , {pause} into Transcriber . And so {disfmarker} I told {disfmarker} I said it would take about a day . I 've worked on it for about half a day ,
Grad H: I have to go .
Grad G: so give me another half day and I we 'll have something we can play with .
PhD C: OK .
Professor E: See , this is where we really need the Meeting Recorder query stuff to be working , because we 've had these meetings and we 've had this discussion about this , and I 'm sort of remembering a little bit about what we decided ,
PhD C: Right . I 'm sorry . I just {disfmarker}
Professor E: but I couldn't remember all of it .
PhD C: It
Professor E: So , I think it was partly that , you know , give somebody a chance to actually look at the data and see what these are like , partly that we have e some ground truth to compare against , you know , when {disfmarker} when he {disfmarker} he gets his thing going ,
Grad G: But {disfmarker}
Professor E: uh , and {disfmarker}
PhD C: Well , it 's definitely good to have somebody look at it . I was just thinking as a way to speed up you know , the amount of {disfmarker}
Postdoc B: Mm - hmm .
Professor E: That was {disfmarker} that was exactly the notion that {disfmarker} that {disfmarker} that we discussed .
PhD C: OK .
Grad G: Thanks .
Postdoc B: Another thing we discussed was um that {disfmarker}
PhD C: It looks good .
Professor E: So .
PhD C: I 'll be in touch . Thanks .
Professor E: S See ya . Yeah .
Postdoc B: Was that um there m {pause} there was this already a script I believe uh that Dan had written , {comment} that uh handle bleedthrough , I mean cuz you have this {disfmarker} this close {disfmarker} you have contamination from other people who speak loudly .
Grad G: Yeah , and I haven't tried using that . It would probably help the program that I 'm doing to first feed it through that . It 's a cross - correlation filter . So I {disfmarker} I haven't tried that , but that {disfmarker} If {disfmarker} It {disfmarker} it might be something {disfmarker} it might be a good way of cleaning it up a little .
Postdoc B: So , some thought of maybe having {disfmarker} Yeah , having that be a preprocessor and then run it through yours .
Grad G: Exactly . Yep .
Professor E: But {disfmarker} but that 's a refinement
Postdoc B: That 's what we were discussing .
Professor E: and I think we wanna see {disfmarker} try the simple thing first , cuz you add this complex thing up uh afterwards that does something good y y yo you sort of wanna see what the simple thing does first .
Grad G: Yep .
Professor E: But uh , having {disfmarker} having somebody have some experience , again , with {disfmarker} with uh {disfmarker} with marking it from a human standpoint , we 're {disfmarker} I mean , I don't expect Jose to {disfmarker} to do it for uh f fifty hours of {disfmarker} {comment} of speech , but I mean we {disfmarker} {comment} if uh {disfmarker} if he could speed up what he was doing by just getting the speaker overlaps so that we had it , say , for forty - five minutes , then at least we 'd have three hundred examples of it .
PhD D: Yeah . Sure . Sure .
Professor E: And when {disfmarker} when uh Adam was doing his automatic thing he could then compare to that and see what it was different .
PhD C: Oh yeah , definitely .
PhD A: You know , I did {disfmarker} I did uh something almost identical to this at one of my previous jobs , and it works pretty well . I mean , i almost exactly what you described , an energy detector with a median filter , you look for runs . And uh , you know , you can {disfmarker}
Grad G: It seemed like the right thing to do .
PhD A: Yeah . I mean , you {disfmarker} you can get y I mean , you get them pretty close .
Grad G: That was with zero literature search .
PhD A: And so I think doing that to generate these possibilities and then going through and saying yes or no on them would be a quick way to {disfmarker} to do it .
Grad G: That 's good validation .
PhD A: Yeah .
Postdoc B: Is this proprietary ?
PhD A: Uh . {comment} No . No .
Grad G: Yeah , do you have a patent on it ?
PhD A: It was when I was working for the government .
Professor E: Oh , then everybody owns it . It 's the people .
Postdoc B: Well , I mean , is this something that we could just co - opt , or is it {disfmarker} ?
PhD A: Nah .
Postdoc B: No . OK .
Professor E: Well , i i i he 's pretty close , anyway . I think {disfmarker} I think it 's {disfmarker}
PhD A: Yeah , he 's {disfmarker} it {disfmarker} it doesn't take a long time .
Postdoc B: Right . I just thought if it was tried and true , then {disfmarker} {comment} and he 's gone through additional levels of {disfmarker} of development .
Grad G: Just output . Although if you {disfmarker} if you have some parameters like what 's a good window size for the median filter {disfmarker}
PhD A: Oh ! {comment} I have to remember . I 'll think about it , and try to remember .
PhD F: And it might be different for government people .
Grad G: That 's alright .
Professor E: Yeah , good enough for government work , as they say .
PhD C: They {disfmarker} they {disfmarker}
PhD A: Di - dif different {disfmarker} different bandwidth .
PhD F: They
Grad G: I was doing pretty short , you know , tenth of a second , {comment} sorts of numbers .
PhD F: OK .
Professor E: Uh , I don't know , it {disfmarker} if {disfmarker} if we want to uh {disfmarker} So , uh , maybe we should move on to other {disfmarker} other things in limited time .
Postdoc B: Can I ask one question about his statistics ? So {disfmarker} so in the tw twelve minutes , um , if we took three hundred and divided it by four , which is about the length of twelve minutes , i Um , I 'd expect like there should be seventy - five overlaps .
Professor E: Yeah .
Postdoc B: Did you find uh more than seventy - five overlaps in that period , or {disfmarker} ?
PhD D: More than ?
Postdoc B: More than {disfmarker} How many overlaps in your twelve minutes ?
PhD D: How many ? Eh , not @ @ I Onl - only I {disfmarker} I transcribe eh only twelve minutes from the
Professor E: Yeah .
PhD D: but eh I {disfmarker} I don't co eh {disfmarker} I don't count eh the {disfmarker} the overlap .
Postdoc B: The overlaps . OK .
PhD D: I consider I {disfmarker} I {disfmarker} The {disfmarker} the nnn {disfmarker} The {disfmarker} the three hundred is eh considered only you {disfmarker} your transcription . I have to {disfmarker} {vocalsound} to finish transcribing . So .
Grad G: I b I bet they 're more , because the beginning of the meeting had a lot more overlaps than {disfmarker} than sort of the middle .
PhD D: Yeah .
Grad G: Middle or end .
Postdoc B: I 'm not sure .
PhD D: Yeah .
Grad G: Because i we 're {disfmarker} we 're dealing with the {disfmarker} Uh , in the early meetings , we 're recording while we 're saying who 's talking on what microphone , {comment} and things like that ,
PhD D: Yeah .
Grad G: and that seems to be a lot of overlap .
Postdoc B: I think it 's an empirical question .
PhD D: Yeah .
Postdoc B: I think we could find that out .
PhD D: Yeah .
Grad G: Yep .
Postdoc B: I 'm {disfmarker} I 'm not sure that the beginning had more .
Professor E: So {disfmarker} so I was gonna ask , I guess about any {disfmarker} any other things that {disfmarker} that {disfmarker} that either of you wanted to talk about , especially since Andreas is leaving in five minutes , that {disfmarker} that you wanna go with .
PhD C: Can I just ask about the data , like very straightforward question is where we are on the amount of data and the amount of transcribed data , just cuz I 'm {disfmarker} I wanted to get a feel for that to sort of be able to know what {disfmarker} what can be done first and like how many meetings are we recording
Professor E: Right so there 's this {disfmarker} this {disfmarker} There 's this forty - five minute piece that Jane transcribed .
PhD C: and {disfmarker}
Professor E: That piece was then uh sent to IBM so they could transcribe so we have some comparison point . Then there 's s a larger piece that 's been recorded and uh put on CD - ROM and sent uh to IBM . Right ? And then we don't know .
PhD C: How many meetings is that ? Like {disfmarker} how many {disfmarker}
Grad G: What 's that ?
Professor E: That was about ten hours , and there was about {disfmarker}
PhD C: t ten {disfmarker} It 's like ten meetings or something ? Uh - huh .
Grad G: Yeah , something like that . And then {disfmarker} then we
PhD A: Ten meetings that have been sent to IBM ?
PhD C: And {disfmarker}
Professor E: Yeah .
Grad G: Well , I haven't sent them yet because I was having this problem with the {pause} missing files .
Professor E: Oh . Oh , that 's right , that had {disfmarker} those have not been sent .
PhD A: H how many total have we recorded now , altogether ?
Professor E: We 're saying about {pause} twelve hours .
Grad G: About twelve {pause} by now . Twelve or thirteen .
PhD C: Uh - huh . And we 're recording only this meeting , like continuously we 're only recording this one now ? or {disfmarker} ?
Professor E: No . No , so the {disfmarker} the {disfmarker} that 's the {disfmarker} that 's the biggest one {disfmarker} uh , chunk so far ,
Grad G: Nope .
PhD A: It was the morning one .
PhD C: OK .
Professor E: but there 's at least one meeting recorded of uh the uh uh natural language guys .
Grad G: Jerry .
PhD C: Do they meet every week ,
Professor E: And then there {disfmarker}
PhD C: or every {disfmarker}
Professor E: Uh , they do . w w And we talked to them about recording some more and we 're going to , uh , we 've started having a morning meeting , today uh i starting a w a week or two ago , on the uh front - end issues , and we 're recording those , uh there 's a network services and applications group here who 's agreed to have their meetings recorded ,
PhD C: Great .
Professor E: and we 're gonna start recording them . They 're {disfmarker} They meet on Tuesdays . We 're gonna start recording them next week . So actually , we 're gonna h start having a {disfmarker} a pretty significant chunk and so , you know , {vocalsound} Adam 's sort of struggling with trying to get things to be less buggy , and come up quicker when they do crash and stuff {disfmarker} things like that , now that uh {disfmarker} {vocalsound} the things are starting to happen . So right now , yeah , I th I 'd say the data is predominantly meeting meetings , but there are scattered other meetings in it and that {disfmarker} that amount is gonna grow uh so that the meeting meetings will probably ultimately {disfmarker} i if we 're {disfmarker} if we collect fifty or sixty hours , the meeting meetings it will probably be , you know , twenty or thirty percent of it , not {disfmarker} not {disfmarker} not eighty or ninety . But .
PhD C: So there 's probably {disfmarker} there 's three to four a week ,
Grad G: That 's what we 're aiming for .
PhD C: that we 're aiming for .
Professor E: Yeah .
PhD C: And they 're each about an hour or something .
Professor E: Yeah , yeah .
Grad G: Although {disfmarker} Yeah . We 'll find out tomorrow whether we can really do this or not .
PhD C: So {disfmarker} OK .
Professor E: Yeah and th the {disfmarker} the other thing is I 'm not pos I 'm sort of thinking as we 've been through this a few times , that I really don't know {disfmarker} maybe you wanna do it once for the novelty , but I don't know if in general we wanna have meetings that we record from outside this group do the digits .
Grad G: Right .
Professor E: Because it 's just an added bunch of weird stuff .
PhD C: Yeah .
Professor E: And , you know , we {disfmarker} we h we 're highly motivated . Uh in fact , the morning group is really motivated cuz they 're working on connected digits , so it 's {disfmarker}
Grad G: Actually that 's something I wanted to ask , is I have a bunch of scripts to help with the transcription of the digits .
Professor E: Yeah .
Grad G: We don't have to hand - transcribe the digits because we 're reading them and I have those .
PhD C: Right .
Professor E: Yeah .
Grad G: And so I have some scripts that let you very quickly extract the sections of each utterance . But I haven't been ru I haven't been doing that . Um , if I did that , is someone gonna be working on it ?
Professor E: Uh , yeah , I {disfmarker} I think definitely s so Absolutely .
Grad G: I mean , is it something of interest ?
Professor E: Yeah , whoever we have working on the acoustics for the Meeting Recorder are gonna start with that .
Grad G: OK . I mean , I I 'm {disfmarker} I 'm interested in it , I just don't have time to do it now .
PhD F: I was {disfmarker} these meetings {disfmarker} I 'm sure someone thought of this , but these {disfmarker} this uh reading of the numbers would be extremely helpful to do um adaptation .
Grad G: So
PhD F: Um .
Grad G: Yep . Yep .
PhD C: Actually I have o
Grad G: I {disfmarker} I would really like someone to do adaptation .
PhD F: Mm - hmm .
Grad G: So if we got someone interested in that , I think it would be great for Meeting Recorder .
Professor E: Well {disfmarker} I mean , one of the things I wanted to do , uh , that I I talked to {disfmarker} to Don about , is one of the possible things he could do or m also , we could have someone else do it , is to do block echo cancellation ,
Grad G: Since it 's the same people over and over .
PhD F: Mm - hmm .
Professor E: to try to get rid of some of the effects of the {disfmarker} the {disfmarker} the far - field effects . Um , I mean we have {disfmarker} the party line has been that echo cancellation is not the right way to handle the situation
PhD F: Mm - hmm .
Professor E: because people move around , and uh , if {disfmarker} if it 's {disfmarker} if it 's uh not a simple echo , like a cross - talk kind of echo , but it 's actually room acoustics , it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} you can't really do inversion ,
PhD F: Mm - hmm .
Professor E: and even echo cancellation is going to uh be something {disfmarker} It may {disfmarker} you {disfmarker} Someone may be moving enough that you are not able to adapt quickly and so the tack that we 've taken is more " lets come up with feature approaches and multi - stream approaches and so forth , that will be robust to it for the recognizer and not try to create a clean signal " .
PhD F: Mm - hmm .
Professor E: Uh , that 's the party line . But it occurred to me a few months ago that uh party lines are always , you know , sort of dangerous . It 's good {disfmarker} {vocalsound} good to sort of test them , actually . And so we haven't had anybody try to do a good serious job on echo cancellation and we should know how well that can do . So that 's something I 'd like somebody to do at some point , just take these digits , take the far - field mike signal , and the close uh mike signal , and apply really good echo cancellation . Um , there was a {disfmarker} have been some nice talks recently by {disfmarker} by Lucent on {disfmarker} on their b
PhD F: Hmm .
Professor E: the block echo cancellation particularly appealed to me , uh you know , trying and change it sample by sample , but you have some reasonable sized blocks . {comment} And um , you know , th
PhD A: W what is the um {disfmarker} the artifact you try to {disfmarker} you 're trying to get rid of when you do that ?
PhD F: Ciao .
Professor E: Uh so it 's {disfmarker} it {disfmarker} you have a {disfmarker} a direct uh {disfmarker} Uh , what 's the difference in {disfmarker} If you were trying to construct a linear filter , that would um {disfmarker}
PhD F: I 'm signing off .
Professor E: Yeah . that would subtract off {comment} the um uh parts of the signal that were the aspects of the signal that were different between the close - talk and the distant . You know , so {disfmarker} so uh um I guess in most echo cancellation {disfmarker} Yeah , so you {disfmarker} Given that um {disfmarker} Yeah , so you 're trying to {disfmarker} So you 'd {disfmarker} There 's a {disfmarker} a distance between the close and the distant mikes so there 's a time delay there , and after the time delay , there 's these various reflections . And if you figure out well what 's the {disfmarker} there 's a {disfmarker} a least squares algorithm that adjusts itself {disfmarker} adjusts the weight so that you try to subtract {disfmarker} essentially to subtract off uh different uh {disfmarker} different reflections . Right ? So let 's take the simple case where you just had {disfmarker} you had some uh some delay in a satellite connection or something and then there 's a {disfmarker} there 's an echo . It comes back . And you want to adjust this filter so that it will maximally reduce the effect of this echo .
PhD A: So that would mean like if you were listening to the data that was recorded on one of those . Uh , just the raw data , you would {disfmarker} you might hear kind of an echo ? And {disfmarker} and then this {disfmarker} noise cancellation would get
Professor E: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm saying {disfmarker} That 's a simplified version of what 's really happening . {comment} What 's really happening is {disfmarker} Well , when I 'm talking to you right now , you 're getting the direct sound from my speech , but you 're also getting , uh , the indirect sound that 's bounced around the room a number of times . OK ? So now , if you um try to r you {disfmarker} To completely remove the effect of that is sort of impractical for a number of technical reasons , but I {disfmarker} but {disfmarker} not to try to completely remove it , that is , invert the {disfmarker} the room response , but just to try to uh uh eliminate some of the {disfmarker} the effect of some of the echos . Um , a number of people have done this so that , say , if you 're talking to a speakerphone , uh it makes it more like it would be , if you were talking right up to it . So this is sort of the st the straight - forward approach . You say I {disfmarker} I {disfmarker} I want to use this uh {disfmarker} this item but I want to subtract off various kinds of echos . So you construct a filter , and you have this {disfmarker} this filtered version uh of the speech um gets uh uh {disfmarker} gets subtracted off from the original speech . Then you try to {disfmarker} you try to minimize the energy in some sense . And so um {disfmarker} uh with some constraints .
PhD A: Kind of a clean up thing , that {disfmarker}
Professor E: It 's a clean up thing . Right .
PhD A: OK .
Professor E: So , echo cancelling is {disfmarker} is , you know , commonly done in telephony , and {disfmarker} and {disfmarker} and it 's sort of the obvious thing to do in this situation if you {disfmarker} if , you know , you 're gonna be talking some distance from a mike .
PhD A: When uh , I would have meetings with the folks in Cambridge when I was at BBN over the phone , they had a um {disfmarker} some kind of a special speaker phone and when they would first connect me , it would come on and we 'd hear all this noise . And then it was uh {disfmarker} And then it would come on and it was very clear ,
Professor E: Yeah .
PhD A: you know .
Professor E: Right . So it 's taking samples , it 's doing adaptation , it 's adjusting weights , and then it 's getting the sum . So um , uh anyway that 's {disfmarker} that 's kind of a reasonable thing that I 'd like to have somebody try {disfmarker} somebody look {disfmarker} And {disfmarker} and the digits would be a reasonable thing to do that with . I think that 'd be enough data {disfmarker} plenty of data to do that with , and i for that sort of task you wouldn't care whether it was uh large vocabulary speech or anything . Uh . {vocalsound} Um
Postdoc B: Is Brian Kingsbury 's work related to that , or is it a different type of reverberation ?
Professor E: Brian 's {comment} Kingsbury 's work is an example of what we did f f from the opposite dogma . Right ? Which is what I was calling the " party line " , which is that uh doing that sort of thing is not really what we want . We want something more flexible , uh i i where people might change their position , and there might be , you know {disfmarker} There 's also um oh yeah , noise . So the echo cancellation does not really allow for noise . It 's if you have a clean situation but you just have some delays , Then we 'll figure out the right {disfmarker} the right set of weights for your taps for your filter in order to produce the effect of those {disfmarker} those echos . But um if there 's noise , then the very signal that it 's looking at is corrupted so that it 's decision about what the right {disfmarker} you know , right {disfmarker} right uh {disfmarker} delays are {disfmarker} is , uh {disfmarker} is {disfmarker} right delayed signal is {disfmarker} is {disfmarker} is {disfmarker} uh is incorrect . And so , in a noisy situation , um , also in a {disfmarker} in a situation that 's very reverberant {disfmarker} {comment} with long reverberation times {comment} and really long delays , it 's {disfmarker} it 's sort of typically impractical . So for those kind of reasons , and also a {disfmarker} a c a complete inversion , if you actually {disfmarker} I mentioned that it 's kind of hard to really do the inversion of the room acoustics . Um , that 's difficult because um often times the {disfmarker} the um {disfmarker} {vocalsound} the system transfer function is such that when it 's inverted you get something that 's unstable , and so , if you {disfmarker} you do your estimate of what the system is , and then you try to invert it , you get a filter that actually uh , you know , rings , and {disfmarker} and uh goes to infinity . So it 's {disfmarker} so there 's {disfmarker} there 's {disfmarker} there 's that sort of technical reason , and the fact that things move , and there 's air currents {disfmarker} I mean there 's all sorts of {disfmarker} all sorts of reasons why it 's not really practical . So for all those kinds of reasons , uh we {disfmarker} we {disfmarker} we sort of um , concluded we didn't want to in do inversion , and we 're even pretty skeptical of echo cancellation , which isn't really inversion , and um we decided to do this approach of taking {disfmarker} uh , just picking uh features , which were {disfmarker} uh will give you more {disfmarker} something that was more stable , in the presence of , or absence of , room reverberation , and that 's what Brian was trying to do . So , um , let me just say a couple things that I was {disfmarker} I was gonna bring up . Uh . Let 's see . I guess you {disfmarker} you actually already said this thing about the uh {disfmarker} about the consent forms , which was that we now don't have to {disfmarker} So this was the human subjects folks who said this , {comment} or that {disfmarker} that {disfmarker} ?
Postdoc B: The a apparently {disfmarker} I mean , we 're gonna do a revised form , of course . Um but once a person has signed it once , then that 's valid for a certain number of meetings . She wanted me to actually estimate how many meetings and put that on the consent form . I told her that would be a little bit difficult to say . So I think from a s practical standpoint , maybe we could have them do it once every ten meetings , or something . It won't be that many people who do it {pause} that often , but um just , you know , so long as they don't forget that they 've done it , I guess .
Professor E: OK . Um , back on the data thing , so there 's this sort of one hour , ten hour , a hundred hour sort of thing that {disfmarker} that we have . We have {disfmarker} we have an hour uh that {disfmarker} that is transcribed , we have {disfmarker} we have twelve hours that 's recorded but not transcribed , and at the rate we 're going , uh by the end of the semester we 'll have , I don't know , forty or fifty or something , if we {disfmarker} if this really uh {disfmarker} Well , do we have that much ?
PhD C: Not really . It 's three to four per week .
Professor E: Let 's see , we have {disfmarker}
PhD C: So that 's what {disfmarker} You know , that {disfmarker}
Professor E: uh eight weeks , uh is {disfmarker}
PhD C: So that 's not a lot of hours .
Professor E: Eight weeks times three hours is twenty - four , so that 's {disfmarker} Yeah , so like thirty {disfmarker} thirty hours ?
PhD A: Three {disfmarker} Three hours .
PhD C: Yeah . I mean , is there {disfmarker} I know this sounds {pause} tough but we 've got the room set up . Um I was starting to think of some projects where you would use well , similar to what we talked about with uh energy detection on the close - talking mikes . There are a number of interesting questions that you can ask about how interactions happen in a meeting , that don't require any transcription . So what are the patterns , the energy patterns over the meeting ? And I 'm really interested in this {vocalsound} but we don't have a whole lot of data . So I was thinking , you know , we 've got the room set up and you can always think of , also for political reasons , if ICSI collected you know , two hundred hours , that looks different than forty hours , even if we don't transcribe it ourselves ,
Professor E: But I don't think we 're gonna stop at the end of this semester .
PhD C: so {disfmarker}
Professor E: Right ? So , I th I think that if we are able to keep that up for a few months , we are gonna have more like a hundred hours .
PhD C: I mean , is there {disfmarker} Are there any other meetings here that we can record , especially meetings that have some kind of conflict in them {comment} or some kind of deci I mean , that are less well {disfmarker} I don't {disfmarker} uh , that have some more emotional aspects to them , or strong {disfmarker}
Grad G: We had some good ones earlier .
PhD C: There 's laughter , um I 'm talking more about strong differences of opinion meetings , maybe with manager types , or {disfmarker}
Grad G: I think it 's hard to record those .
PhD C: To be allowed to record them ?
Postdoc B: It 's also likely that people will cancel out afterwards .
PhD C: OK .
Professor E: Yeah , people will get {disfmarker}
Postdoc B: But I {disfmarker} but I wanted to raise the KPFA idea .
PhD C: OK . Well , if there is , anyway .
Professor E: Yeah , I was gonna mention that .
Grad G: Oh , that 's a good idea . That 's {disfmarker} That would be a good match .
Professor E: Yeah . So {disfmarker} Yeah . So I {disfmarker} I {disfmarker} uh , I {disfmarker} I 'd mentioned to Adam , and {disfmarker} that was another thing I was gonna talk {disfmarker} uh , mention to them before {disfmarker} {comment} that uh there 's uh {disfmarker} It {disfmarker} it oc it occurred to me that we might be able to get some additional data by talking to uh acquaintances in local broadcast media . Because , you know , we had talked before about the problem about using found data , {comment} that {disfmarker} that uh it 's just set up however they have it set up and we don't have any say about it and it 's typically one microphone , in a , uh , uh {disfmarker} or {disfmarker} and {disfmarker} and so it doesn't really give us the {disfmarker} the {disfmarker} the uh characteristics we want . Um and so I do think we 're gonna continue recording here and record what we can . But um , it did occur to me that we could go to friends in broadcast media and say " hey you have this panel show , {pause} or this {disfmarker} you know , this discussion show , and um can you record multi - channel ? " And uh they may be willing to record it uh with {disfmarker}
PhD C: With lapel mikes or something ?
Professor E: Well , they probably already use lapel , but they might be able to have it {disfmarker} it wouldn't be that weird for them to have another mike that was somewhat distant .
PhD C: Right .
Professor E: It wouldn't be exactly this setup , but it would be that sort of thing , and what we were gonna get from UW , you know , assuming they {disfmarker} they {disfmarker} they start recording , isn't {disfmarker} als also is not going to be this exact setup .
PhD C: Right . No , I think that 'd be great , if we can get more data .
Professor E: So , {comment} I {disfmarker} I {disfmarker} I {disfmarker} I was thinking of looking into that . the other thing that occurred to me after we had that discussion , in fact , is that it 's even possible , since of course , many radio shows are not live , {comment} uh that we could invite them to have like some of their {disfmarker} {comment} record some of their shows here .
Postdoc B: Wow !
PhD C: Well {disfmarker} Or {disfmarker} The thing is , they 're not as averse to wearing one of these head - mount I mean , they 're on the radio ,
Grad G: Right , as we are .
PhD C: right ? So . {comment} Um , I think that 'd be fantastic
Professor E: Right .
PhD C: cuz those kinds of panels and {disfmarker} Those have interesting
Professor E: Yeah .
PhD C: Th - that 's an {disfmarker} a side of style {disfmarker} a style that we 're not collecting here , so it 'd be great .
Professor E: And {disfmarker} and the {disfmarker} I mean , the other side to it was the {disfmarker} what {disfmarker} which is where we were coming from {disfmarker} I 'll {disfmarker} I 'll talk to you more about it later {comment} is that {disfmarker} is that there 's {disfmarker} there 's uh the radio stations and television stations already have stuff worked out presumably , uh related to , you know , legal issues and {disfmarker} and permissions and all that . I mean , they already do what they do {disfmarker} do whatever they do . So it 's {disfmarker} uh , it 's {disfmarker} So it 's {disfmarker} so it 's another source . So I think it 's something we should look into , you know , we 'll collect what we collect here hopefully they will collect more at UW also and um {disfmarker} and maybe we have this other source . But yeah I think that it 's not unreasonable to aim at getting , you know , significantly in excess of a hundred hours . I mean , that was sort of our goal . The thing was , I was hoping that we could {disfmarker} @ @ in the {disfmarker} under this controlled situation we could at least collect , you know , thirty to fifty hours . And at the rate we 're going we 'll get pretty close to that I think this semester . And if we continue to collect some next semester , I think we should , uh {disfmarker}
PhD C: Right . Yeah I was mostly trying to think , " OK , if you start a project , within say a month , you know , how much data do you have to work with . And you {disfmarker} you wanna s you wanna sort of fr freeze your {disfmarker} your data for awhile so um right now {disfmarker} and we don't have the transcripts back yet from IBM right ? Do {disfmarker} Oh , do we now ?
Professor E: Well , we don't even have it for this f you know , forty - five minutes , that was {disfmarker}
PhD C: So um , not complaining , I was just trying to think , you know , what kinds of projects can you do now versus uh six months from now
Professor E: Yeah .
PhD C: and they 're pretty different , because
Professor E: Yeah . So I was thinking right now it 's sort of this exploratory stuff where you {disfmarker} you look at the data , you use some primitive measures and get a feeling for what the scatter plots look like ,
Grad G: Right .
PhD C: um {disfmarker} Right . Right , right .
Professor E: and {disfmarker} and {disfmarker} and uh {disfmarker} and meanwhile we collect , and it 's more like yeah , three months from now , or six months from now you can {disfmarker} you can do a lot of other things .
PhD C: Cuz I 'm not actually sure , just logistically that I can spend {disfmarker} you know , I don't wanna charge the time that I have on the project too early , before there 's enough data to make good use of the time . And that 's {disfmarker} and especially with the student
Grad G: Right .
PhD C: uh for instance this guy who seems {disfmarker}
Professor E: Yeah .
PhD C: Uh anyway , I shouldn't say too much , but um if someone came that was great and wanted to do some real work and they have to end by the end of this school year in the spring , how much data will I have to work with , with that person . And so it 's {disfmarker}
Professor E: i Yeah , so I would think , exploratory things now . Uh , three months from now {disfmarker} Um , I mean the transcriptions I think are a bit of an unknown cuz we haven't gotten those back yet as far as the timing , but I think as far as the collection , it doesn't seem to me l like , uh , unreasonable to say that uh in January , you know , ro roughly uh {disfmarker} which is roughly three months from now , we should have at least something like , you know , twenty - five , thirty hours .
PhD C: And we just don't know about the transcription part of that ,
Professor E: So that 's {disfmarker}
Postdoc B: Yeah , we need to {disfmarker} I think that there 's a possibility that the transcript will need to be adjusted afterwards ,
PhD C: so . I mean , it {disfmarker}
Postdoc B: and uh es especially since these people won't be uh used to dealing with multi - channel uh transcriptions .
PhD C: Right .
Professor E: Yeah .
Postdoc B: So I think that we 'll need to adjust some {disfmarker} And also if we wanna add things like um , well , more refined coding of overlaps , then definitely I think we should count on having an extra pass through . I wanted to ask another a a aspect of the data collection . There 'd be no reason why a person couldn't get together several uh , you know , friends , and come and argue about a topic if they wanted to , right ?
Professor E: If they really have something they wanna talk about as opposed to something @ @ {disfmarker} I mean , what we 're trying to stay away from was artificial constructions , but I think if it 's a real {disfmarker} Why not ? Yeah .
PhD C: I mean , I 'm thinking , politically {disfmarker}
Grad G: Stage some political debates .
Postdoc B: You could do this ,
PhD C: Well yeah ,
Postdoc B: you know . You could .
PhD C: or just if you 're {disfmarker} if you ha If there are meetings here that happen that we can record even if we don't {pause} um have them do the digits , {comment} or maybe have them do a shorter {pause} digit thing {comment} like if it was , you know , uh , one string of digits , or something , they 'd probably be willing to do .
Grad G: We don't have to do the digits at all if we don't want to .
PhD C: Then , having the data is very valuable , cuz I think it 's um politically better for us to say we have this many hours of audio data , especially with the ITR , if we put in a proposal on it . It 'll just look like ICSI 's collected a lot more audio data . Um , whether it 's transcribed or not um , is another issue , but there 's {disfmarker} there are research questions you can answer without the transcriptions , or at least that you can start to answer .
Postdoc B: It seems like you could hold some meetings .
Grad G: Yep .
Postdoc B: You know , you and maybe Adam ?
PhD C: So .
Postdoc B: You {disfmarker} you could {disfmarker} you could maybe hold some additional meetings , if you wanted .
PhD A: Would it help at all {disfmarker} I mean , we 're already talking about sort of two levels of detail in meetings . One is uh um without doing the digits {disfmarker} Or , I guess the full - blown one is where you do the digits , and everything , and then talk about doing it without digits , what if we had another level , just to collect data , which is without the headsets and we just did the table - mounted stuff .
PhD C: Need the close - talking mikes .
PhD A: You do , OK .
PhD C: I mean , absolutely ,
Professor E: Yeah . Yeah .
PhD C: yeah . I 'm really scared {disfmarker}
Grad G: It seems like it 's a big part of this corpus is to have the close - talking mikes .
PhD A: I see , OK .
PhD C: Um or at least , like , me personally ? I would {disfmarker} {comment} I {disfmarker} couldn't use that data .
Professor E: Yeah .
Postdoc B: I agree . And Mari also ,
PhD C: Um .
Postdoc B: we had {disfmarker} This came up when she she was here . That 's important .
PhD C: So it 's a great idea ,
Professor E: Yeah , I {disfmarker} I {disfmarker} b By the {disfmarker} by the way , I don't think the transcriptions are actually , in the long run , such a big bottleneck .
PhD C: and if it were true than I would just do that , but it 's not that bad {disfmarker} like the room is not the bottleneck , and we have enough time in the room , it 's getting the people to come in and put on the {disfmarker} and get the setup going .
Professor E: I think the issue is just that we 're {disfmarker} we 're blazing that path . Right ? And {disfmarker} and um {disfmarker} d Do you have any idea when {disfmarker} when uh the {disfmarker} you 'll be able to send uh the ten hours to them ?
Grad G: Well , I 've been burning two C Ds a day , which is about all I can do with the time I have .
Professor E: Yeah . Yeah .
Grad G: So it 'll be early next week .
Professor E: Yeah , OK . So early next week we send it to them , and then {disfmarker} then we check with them to see if they 've got it and we {disfmarker} we start , you know asking about the timing for it .
Grad G: Yep .
Professor E: So I think once they get it sorted out about how they 're gonna do it , which I think they 're pretty well along on , cuz they were able to read the files and so on .
Grad G: Yep .
Professor E: Right ?
Grad G: Yeah , but {disfmarker}
Professor E: Well {disfmarker}
Grad G: Yeah , who knows where they are .
PhD A: Have they ever responded to you ?
Grad G: Nope .
Professor E: Yeah , but {disfmarker} You know , so they {disfmarker} they {disfmarker} they have {disfmarker} you know , they 're volunteering their time and they have a lot of other things to do ,
PhD C: What if {disfmarker}
Grad G: Yeah , you {disfmarker} we can't complain .
Professor E: right ? But they {disfmarker} But at any rate , they 'll {disfmarker} I {disfmarker} I think once they get that sorted out , they 're {disfmarker} they 're making cassettes there , then they 're handing it to someone who they {disfmarker} who 's {disfmarker} who is doing it , and uh I think it 's not going to be {disfmarker} I don't think it 's going to be that much more of a deal for them to do thirty hours then to do one hour , I think . It 's not going to be thirty
Grad G: Yep . I think that 's probably true .
PhD C: Really ? So it 's the amount of {disfmarker}
Professor E: It 's {disfmarker} it 's just getting it going .
Grad G: It 's pipeline , pipeline issues .
PhD C: Right . What about these lunch meetings {disfmarker}
Grad G: Once the pipeline fills .
PhD C: I mean , I don't know , if there 's any way without too much more overhead , even if we don't ship it right away to IBM even if we just collect it here for awhile , {comment} to record you know , two or three more meeting a week , just to have the data , even if they 're um not doing the digits , but they do wear the headphones ?
Professor E: But the lunch meetings are pretty much one person getting up and {disfmarker}
PhD C: No , I meant , um , sorry , the meetings where people eat their lunch downstairs , maybe they don't wanna be recorded , but {disfmarker}
Grad G: Oh , and we 're just chatting ?
PhD C: Just the ch the chatting .
Grad G: Yeah , we have a lot of those .
PhD C: I actually {disfmarker} I actually think that 's {pause} useful {pause} data , um {pause} the chatting ,
Grad G: Yeah , the problem with that is I would {disfmarker} I think I would feel a little constrained to {disfmarker} You know ? Uh , some of the meetings {disfmarker}
PhD C: but {disfmarker} OK . You don't wanna do it , cuz {disfmarker} OK .
Grad G: You know , our " soccer ball " meeting ?
PhD C: Alright .
Grad G: I guess none of you were there for our soccer ball meeting .
PhD C: Alright , {comment} so I 'll just throw it out there , if anyone knows of one more m or two more wee meetings per week that happen at ICSI , um that we could record , I think it would be worth it .
Grad G: That was hilarious .
Professor E: Yeah . Well , we should also check with Mari again , because they {disfmarker} because they were really intending , you know , maybe just didn't happen , but they were really intending to be duplicating this in some level . So then that would double {pause} what we had . Uh . And there 's a lot of different meetings at UW uh {disfmarker} I mean really m a lot more {comment} than we have here right cuz we 're not right on campus ,
Grad G: Right .
Professor E: so .
PhD A: Is the uh , notion of recording any of Chuck 's meetings dead in the water , or is that still a possibility ?
Professor E: Uh , {vocalsound} they seem to have some problems with it . We can {disfmarker} we can talk about that later . Um , but , again , Jerry is {disfmarker} Jerry 's open {disfmarker} So I mean , we have two speech meetings , one uh network meeting , uh Jerry was open to it but I {disfmarker} I s One of the things that I think is a little {disfmarker} a little bit of a limitation , there is a think when the people are not involved uh in our work , we probably can't do it every week . You know ? I {disfmarker} I {disfmarker} I {disfmarker} I think that {disfmarker} that people are gonna feel uh {disfmarker} are gonna feel a little bit constrained . Now , it might get a little better if we don't have them do the digits all the time . And the {disfmarker} then {disfmarker} so then they can just really sort of try to {disfmarker} put the mikes on and then just charge in and {disfmarker}
Grad G: Yep .
PhD C: What if we give people {disfmarker} you know , we cater a lunch in exchange for them having their meeting here or something ?
Postdoc B: Well , you know , I {disfmarker} I do think eating while you 're doing a meeting is going to be increasing the noise .
PhD C: OK .
Postdoc B: But I had another question , which is um , you know , in principle , w um , I know that you don't want artificial topics ,
PhD C: Alright , alright , alright .
Postdoc B: but um it does seem to me that we might be able to get subjects from campus to come down and do something that wouldn't be too artificial . I mean , we could {disfmarker} political discussions , or {disfmarker} or something or other ,
PhD C: No , definitely .
Postdoc B: and i you know , people who are {disfmarker} Because , you know , there 's also this constraint . We d it 's like , you know , the {disfmarker} the {disfmarker} uh goldibears {disfmarker} goldi goldilocks , it 's like you don't want meetings that are too large , but you don't want meetings that are too small . And um {disfmarker} a and it just seems like maybe we could exploit the subj human subject p p pool , in the positive sense of the word .
PhD A: Well , even {disfmarker} I mean , coming down from campus is sort of a big thing , but what about
Postdoc B: We could pay subjects .
PhD A: or what about people in the {disfmarker} in the building ?
PhD C: Yeah , I was thinking , there 's all these other peo
PhD A: I mean , there 's the State of California downstairs , and {disfmarker}
PhD C: Yeah . I mean {disfmarker}
Grad G: I just really doubt that uh any of the State of California meetings would be recordable and then releasable to the general public .
Postdoc B: Yeah .
PhD A: Oh .
PhD C: Mm - hmm .
Grad G: So I {disfmarker} I mean I talked with some people at the Haas Business School who are i who are interested in speech recognition
PhD C: Alright , well .
Grad G: and , they sort of hummed and hawed and said " well maybe we could have meetings down here " , but then I got email from them that said " no , we decided we 're not really interested and we don't wanna come down and hold meetings . " So , I think it 's gonna be a problem to get people regularly .
PhD A: What about Joachim , maybe he can {disfmarker}
Professor E: But {disfmarker} but we c But I think , you know , we get some scattered things from this and that . And I {disfmarker} I d I do think that maybe we can get somewhere with the {disfmarker} with the radio .
PhD C: Mm - hmm .
Professor E: Uh i I have better contacts in radio than in television , but {disfmarker}
PhD A: You could get a lot of lively discussions from those radio ones .
PhD C: Well , and they 're already {disfmarker} they 're {disfmarker} these things are already recorded ,
Grad G: Yep .
Professor E: Yeah .
PhD C: we don't have to ask them to {disfmarker} even {disfmarker} and I 'm not sure wh how they record it , but they must record from individual {disfmarker}
Professor E: n Well {disfmarker} No , I 'm not talking about ones that are already recorded . I 'm talking about new ones
PhD C: Why {disfmarker} why not ?
Professor E: because {disfmarker} because {disfmarker} because we would be asking them to do something different .
PhD C: Well , we can find out . I know for instance Mark Liberman was interested uh in {disfmarker} in LDC getting {pause} data , uh , and {disfmarker}
Professor E: Right , that 's the found data idea .
PhD C: Yeah .
Professor E: But what I 'm saying is uh if I talk to people that I know who do these th who produce these things we could ask them if they could record an extra channel , let 's say , of a distant mike .
PhD C: Mm - hmm .
Professor E: And u I think routinely they would not do this . So , since I 'm interested in the distant mike stuff , I wanna make sure that there is at least that somewhere
PhD C: Right . Great . OK .  
Professor E: and uh {disfmarker} But if we ask them to do that they might be intrigued enough by the idea that they uh might be e e willing to {disfmarker} the {disfmarker} I might be able to talk them into it .
PhD C: Mm - hmm .
Grad G: Um . We 're getting towards the end of our disk space , so we should think about trying to wrap up here .
PhD C: That 's a good way to end a meeting .
Professor E: OK . Well I don't {disfmarker} why don't we {disfmarker} why d u why don't we uh uh turn them {disfmarker} turn
Grad G: OK , leave {disfmarker} leave them on for a moment until I turn this off , cuz that 's when it crashed last time .
Postdoc B: Oh . That 's good to know .
Professor E: Turning off the microphone made it crash . Well {disfmarker}
Postdoc B: That 's good to know .
Professor E: OK .
2022-06-15 06:44:33 | INFO | __main__ | output #0: To save time, speaker mn005 will only mark the sample of transcribed data for regions of overlapping speech, as opposed to marking all acoustic events. The digits extraction task will be delegated to whomever is working on acoustics for the Meeting Recorder project.
2022-06-15 06:44:33 | INFO | __main__ | input #1: What was said on speech overlap?

Professor E: So . OK . Doesn't look like it crashed . That 's great .
Grad G: So I think maybe what 's causing it to crash is I keep starting it and then stopping it to see if it 's working . And so I think starting it and then stopping it and starting it again causes it to crash . So , I won't do that anymore .
Postdoc B: And it looks like you 've found a way of uh mapping the location to the {disfmarker} without having people have to give their names each time ?
PhD A: Sounds like an initialization thing .
Postdoc B: I mean it 's like you have the {disfmarker} So you know that {disfmarker}
Grad G: No .
Postdoc B: I mean , are you going to write down {pause} that I sat here ?
Grad G: I 'm gonna collect the digit forms and write it down .
Postdoc B: OK .
PhD C: Oh , OK .
Grad G: So {disfmarker} So they should be right with what 's on the digit forms . OK , so I 'll go ahead and start with digits . u And I should say that uh , you just pau you just read each line an and then pause briefly .
Professor E: And start by giving the transcript number .
PhD A: Tran
PhD D: Transcript {disfmarker} Uh . OK , OK .
PhD A: Oh sorry , go ahead .
Professor E: So uh , you see , Don , the unbridled excitement of the work that we have on this project .
Grad H: OK .
Professor E: It 's just uh {disfmarker}
Grad H: Umh .
Professor E: Uh , you know , it doesn't seem like a bad idea to have {comment} that information .
Grad G: And I 'm surprised I sort of {disfmarker} I 'm surprised I forgot that ,
Professor E: Yeah , I {disfmarker} I 'd {disfmarker} I think it 's some
Grad G: but uh I think that would be a good thing to add . After I just printed out a zillion of them .
Professor E: Yeah , well , that 's {disfmarker} Um , so I {disfmarker} I do have a {disfmarker} a an agenda suggestion . Uh , we {disfmarker} I think the things that we talk about in this meeting uh tend to be a mixture of uh procedural uh mundane things and uh research points and um I was thinking I think it was a meeting a couple of weeks ago that we {disfmarker} we spent much of the time talking about the mundane stuff cuz that 's easier to get out of the way and then we sort of drifted into the research and maybe five minutes into that Andreas had to leave . So {vocalsound} uh I 'm suggesting we turn it around and {disfmarker} and uh sort of we have {disfmarker} anybody has some mundane points that we could send an email later , uh hold them for a bit , and let 's talk about the {disfmarker} the research - y kind of things . Um , so um the one th one thing I know that we have on that is uh we had talked a {disfmarker} a couple weeks before um uh about the uh {disfmarker} the stuff you were doing with {disfmarker} with uh um uh l l attempting to locate events , we had a little go around trying to figure out what you meant by " events " but I think , you know , what we had meant by " events " I guess was uh points of overlap between speakers . But I th I gather from our discussion a little earlier today that you also mean uh interruptions with something else
PhD D: Yeah .
Professor E: like some other noise .
PhD D: Uh - huh . Yeah .
Professor E: Yes ? You mean that as an event also .
PhD D: To
Professor E: So at any rate you were {disfmarker} you 've {disfmarker} you 've done some work on that
PhD D: right .
Professor E: and um then the other thing would be it might be nice to have a preliminary discussion of some of the other uh research uh areas that uh we 're thinking about doing . Um , I think especially since you {disfmarker} you haven't been in {disfmarker} in these meetings for a little bit , maybe you have some discussion of some of the p the plausible things to look at now that we 're starting to get data , uh and one of the things I know that also came up uh is some discussions that {disfmarker} that uh {disfmarker} that uh Jane had with Lokendra uh about some {disfmarker} some {disfmarker} some um uh work about I {disfmarker} I {disfmarker} I d I {disfmarker} I don't want to try to say cuz I {disfmarker} I 'll say it wrong , but anyway some {disfmarker} some potential collaboration there about {disfmarker} about the {disfmarker} about the {disfmarker} working with these data .
PhD C: Oh . Sure .
Professor E: So . So , uh .
Grad G: You wanna just go around ?
Professor E: Uh . {pause} Well , I don't know if we {disfmarker} if this is sort of like everybody has something to contribute sort of thing , I think there 's just just a couple {disfmarker} a couple people primarily um but um Uh , wh why don't {disfmarker} Actually I think that {disfmarker} that last one I just said we could do fairly quickly so why don't you {disfmarker} you start with that .
Postdoc B: OK . Shall I {disfmarker} shall I just start ? OK .
Professor E: Yeah , just explain what it was .
Postdoc B: Um , so , uh , he was interested in the question of {disfmarker} you know , relating to his {disfmarker} to the research he presented recently , um of inference structures , and uh , the need to build in , um , this {disfmarker} this sort of uh mechanism for understanding of language . And he gave the example in his talk about how {pause} um , e a I 'm remembering it just off the top of my head right now , but it 's something about how um , i " Joe slipped " you know , " John had washed the floor " or something like that . And I don't have it quite right , but that kind of thing , where you have to draw the inference that , OK , there 's this time sequence , but also the {disfmarker} the {disfmarker} the causal aspects of the uh floor and {disfmarker} and how it might have been the cause of the fall and that um it was the other person who fell than the one who cleaned it and it {disfmarker} {comment} These sorts of things . So , I looked through the transcript that we have so far , {comment} and um , fou identified a couple different types of things of that type and um , one of them was something like uh , during the course of the transcript , um um , w we had gone through the part where everyone said which channel they were on and which device they were on , and um , the question was raised " Well , should we restart the recording at this point ? " And {disfmarker} and Dan Ellis said , " Well , we 're just so far ahead of the game right now {pause} we really don't need to " . Now , how would you interpret that without a lot of inference ? So , the inferences that are involved are things like , OK , so , how do you interpret " ahead of the game " ? You know . So it 's the {disfmarker} it 's {pause} i What you {disfmarker} what you int what you draw {disfmarker} you know , the conclusions that you need to draw are that space is involved in recording ,
Grad G: Hmm , metaphorically .
Postdoc B: that um , i that {pause} i we have enough space , and he continues , like " we 're so ahead of the game cuz now we have built - in downsampling " . So you have to sort of get the idea that um , " ahead of the game " is sp speaking with respect to space limitations , that um that in fact downsampling is gaining us enough space , and that therefore we can keep the recording we 've done so far . But there are a lot of different things like that .
Grad G: So , do you think his interest is in using this as {pause} a data source , or {pause} training material , or what ?
Professor E: Well , I {disfmarker} I should maybe interject to say this started off with a discussion that I had with him , so um we were trying to think of ways that his interests could interact with ours
Grad G: Mm - hmm .
Professor E: and um uh I thought that if we were going to project into the future when we had a lot of data , uh and um such things might be useful for that in or before we invested too much uh effort into that he should uh , with Jane 's help , look into some of the data that we 're {disfmarker} already have and see , is there anything to this at all ?
Grad G: Mm - hmm .
Professor E: Is there any point which you think that , you know , you could gain some advantage and some potential use for it . Cuz it could be that you 'd look through it and you say " well , this is just the wrong {pause} task for {disfmarker} for him to pursue his {disfmarker} "
Grad G: Wrong , yeah .
Professor E: And {disfmarker} and uh I got the impression from your mail that in fact there was enough things like this just in the little sample that {disfmarker} that you looked at that {disfmarker} that it 's plausible at least .
Postdoc B: It 's possible . Uh , he was {disfmarker} he {disfmarker} he {disfmarker} you know {disfmarker} We met and he was gonna go and uh you know , y look through them more systematically
Professor E: Yeah .
Postdoc B: and then uh meet again .
Professor E: Yeah .
Postdoc B: So it 's , you know , not a matter of a {disfmarker}
Professor E: Yeah .
Postdoc B: But , yeah , I think {disfmarker} I think it was optimistic .
Professor E: So anyway , that 's {disfmarker} that 's e a quite different thing from anything we 've talked about that , you know , might {disfmarker} might {disfmarker} might come out from some of this .
PhD C: But he can use text , basically . I mean , he 's talking about just using text
Postdoc B: That 's his major {disfmarker} I mentioned several that w had to do with implications drawn from intonational contours
PhD C: pretty much , or {disfmarker} ?
Postdoc B: and {pause} that wasn't as directly relevant to what he 's doing . He 's interested in these {disfmarker} these knowledge structures ,
PhD C: OK .
PhD D: Yeah , interesting .
Postdoc B: inferences that you draw {pause} i from {disfmarker}
Professor E: I mean , he certainly could use text , but we were in fact looking to see if there {disfmarker} is there {disfmarker} is there something in common between our interest in meetings and his interest in {disfmarker} in {disfmarker} in this stuff . So .
Grad G: And I imagine that transcripts of speech {disfmarker} I mean text that is speech {disfmarker} probably has more of those than sort of prepared writing . I {disfmarker} I don't know whether it would or not , but it seems like it would .
Professor E: I don't know , probably de probably depends on what the prepared writing was . But .
Postdoc B: Yeah , I don't think I would make that leap , because i in narratives , you know {disfmarker} I mean , if you spell out everything in a narrative , it can be really tedious ,
Grad G: Mm - hmm .
Postdoc B: so .
Grad G: Yeah , I 'm just thinking , you know , when you 're {disfmarker} when you 're face to face , you have a lot of backchannel and {disfmarker} And {disfmarker}
Postdoc B: Oh . That aspect .
Grad G: Yeah . And so I think it 's just easier to do that sort of broad inference jumping if it 's face to face . I mean , so , if I just read that Dan was saying " we 're ahead of the game " {comment} in that {disfmarker} in that context ,
Postdoc B: Well {disfmarker} Yeah .
Grad G: I might not realize that he was talking about disk space as opposed to anything else .
Postdoc B: I {disfmarker} you know , I {disfmarker} I had several that had to do with backchannels and this wasn't one of them .
Grad G: Uh - huh .
Postdoc B: This {disfmarker} this one really does um m make you leap from {disfmarker} So he said , you know , " we 're ahead of the game , w we have built - in downsampling " .
Grad G: Mm - hmm .
Postdoc B: And the inference , i if you had it written down , would be {disfmarker}
Grad G: I guess it would be the same .
Postdoc B: Uh - huh . But there are others that have backchannelling , it 's just he was less interested in those .
PhD F: Can I {disfmarker} Sorry to interrupt . Um , I f f f I 've {disfmarker} @ @ {comment} d A minute {disfmarker} uh , several minutes ago , I , like , briefly was {disfmarker} was not listening and {disfmarker} So who is " he " in this context ?
PhD C: Yeah , there 's a lot of pronoun {disfmarker}
PhD F: OK . So I was just realizing we 've {disfmarker} You guys have been talking about " he " um for at least uh , I don't know , three {disfmarker} three four minutes without ever mentioning the person 's name again .
PhD C: I believe it . Yeah . Actually to make it worse , {comment} uh , Morgan uses " you " and " you "
PhD F: So this is {disfmarker} this is {disfmarker} this is {disfmarker} gonna be a big , big problem if you want to later do uh , you know , indexing , or speech understanding of any sort .
Grad G: It 's in my notes .
PhD C: with gaze and no identification , or {disfmarker} I just wrote this down . Yeah , actually . Cuz Morgan will say well , " you had some ideas "
PhD D: Yeah .
PhD F: You just wrote this ?
PhD C: and he never said Li - He looked {disfmarker}
Grad G: Well , I think he 's doing that intentionally ,
PhD C: Right , so it 's great .
Grad G: aren't you ?
PhD C: So this is really great
PhD F: Right .
PhD C: because the thing is , because he 's looking at the per even for addressees in the conversation ,
PhD D: Yeah .
PhD F: Mm - hmm .
PhD C: I bet you could pick that up in the acoustics . Just because your gaze is also correlated with the directionality of your voice .
Professor E: Uh - huh . Could be .
Postdoc B: Can we
Professor E: Yeah . That would be tou
Grad G: Oh , that would be interesting .
PhD C: Yeah , so that , I mean , to even know um when {disfmarker}
PhD D: Yeah .
PhD C: Yeah , if you have the P Z Ms you should be able to pick up what a person is looking at from their voice .
Grad G: Well , especially with Morgan , with the way we have the microphones arranged . I 'm sort of right on axis and it would be very hard to tell .
PhD C: Right .
Grad G: Uh .
Postdoc B: Oh , but you 'd have the {disfmarker}
PhD C: Put Morgan always like this
Postdoc B: You 'd have fainter {disfmarker}
PhD C: and {disfmarker}
Postdoc B: Wouldn't you get fainter reception out here ?
Professor E: Well , these {disfmarker}
Grad G: Sure , but I think if I 'm talking like this ? Right now I 'm looking at Jane and talking , now I 'm looking at Chuck and talking , I don't think the microphones would pick up that difference .
PhD C: But you don't have this {disfmarker} this problem .
Postdoc B: I see .
PhD C: Morgan is the one who does this most .
Grad G: So if I 'm talking at you , or I 'm talking at you .
Professor E: I probably been affect No , I th I think I 've been affected by too many conversations where we were talking about lawyers and talking about {disfmarker} and concerns about " oh gee is somebody going to say something bad ? " and so on .
Grad G: Lawyers .
Professor E: And so I {disfmarker} so I 'm {disfmarker} I 'm tending to stay away from people 's names even though uh {disfmarker}
Postdoc B: I am too .
PhD C: Even though you could pick up later on , just from the acoustics who you were t who you were looking at .
Postdoc B: I am too .
Grad G: And we did mention who " he " was .
PhD C: Yeah .
Professor E: Yeah .
PhD F: Right , but I missed it .
Grad G: Early in the conversation .
PhD F: But {disfmarker} it was uh {disfmarker}
PhD C: Yeah , yeah .
Professor E: Yeah .
Grad G: Do {disfmarker} Sh - Can I say
Professor E: Yeah . No no , there 's {disfmarker}
PhD F: Yeah .
Grad G: or {disfmarker} or is that just too sensitive ?
Professor E: No no , it isn't sensitive at all .
Postdoc B: Well {disfmarker}
Professor E: I was just {disfmarker} I was just {disfmarker} I was overreacting just because we 've been talking about it .
Postdoc B: And in fact , it is {disfmarker} it is {disfmarker} it is sensitive .
PhD C: No , but that {disfmarker} it 's interesting .
Professor E: It 's OK to {disfmarker}
Postdoc B: I {disfmarker} I came up with something from the Human Subjects people that I wanted to mention . I mean , it fits into the m area of the mundane , but they did say {disfmarker} You know , I asked her very specifically about this clause of how , um , you know , it says " no individuals will be identified uh , " in any publication using the data . " OK , well , individuals being identified , let 's say you have a {disfmarker} a snippet that says , " Joe s uh thinks such - and - such about {disfmarker} about this field , but I think he 's wrongheaded . " Now I mean , we 're {disfmarker} we 're gonna be careful not to have the " wrongheaded " part in there , but {disfmarker} but you know , let 's say we say , you know , " Joe used to think so - and - so about this area , in his publication he says that but I think he 's changed his mind . " or whatever . Then the issue of {disfmarker} of being able to trace Joe , because we know he 's well - known in this field , and all this and {disfmarker} and tie it to the speaker , whose name was just mentioned a moment ago , can be sensitive .
Professor E: b But I {disfmarker}
Postdoc B: So I think it 's really {disfmarker} really kind of adaptive and wise to not mention names any more than we have to because if there 's a slanderous aspect to it , then how much to we wanna be able to have to remove ?
Professor E: Yeah , well , there 's that . But I {disfmarker} I mean I think also to some extent it 's just educating the Human Subjects people , in a way , because there 's {disfmarker} If uh {disfmarker} You know , there 's court transcripts , there 's {disfmarker} there 's transcripts of radio shows {disfmarker} I mean people say people 's names all the time . So I think it {disfmarker} it can't be bad to say people 's names . It 's just that {disfmarker} i I mean you 're right that there 's more poten If we never say anybody 's name , then there 's no chance of {disfmarker} of {disfmarker} of slandering anybody ,
PhD C: But , then it won't {disfmarker} I mean , if we {disfmarker} if we {disfmarker}
Professor E: but {disfmarker}
Grad G: It 's not a meeting .
PhD C: Yeah . I mean we should do whatever 's natural in a meeting if {disfmarker} if we weren't being recorded .
Professor E: Yeah . Right , so I {disfmarker} So my behavior is probably not natural .
PhD C: " If Person X {disfmarker} "
Professor E: So .
Postdoc B: Well , my feeling on it was that it wasn't really important who said it , you know .
Professor E: Yeah .
PhD F: Well , if you ha since you have to um go over the transcripts later anyway , you could make it one of the jobs of the {pause} people who do that to mark
Grad G: Well , we t we t we talked about this during the anon anonymization .
PhD F: Right .
Grad G: If we wanna go through and extract from the audio and the written every time someone says a name . And I thought that our conclusion was that we didn't want to do that .
Professor E: Yeah , we really can't . But a actually , I 'm sorry . I really would like to push {disfmarker} finish this off .
Postdoc B: I understand . No I just {disfmarker} I just was suggesting that it 's not a bad policy p potentially .
Professor E: So it 's {disfmarker}
Postdoc B: So , we need to talk about this later .
Professor E: Yeah , I di I didn't intend it an a policy though .
Postdoc B: Uh - huh .
Professor E: It was {disfmarker} it was just it was just unconscious {disfmarker} well , semi - conscious behavior . I sorta knew I was doing it but it was {disfmarker}
PhD F: Well , I still don't know who " he " is .
Professor E: I {disfmarker} I do I don't remember who " he " is .
PhD C: No , you have to say , you still don't know who " he " is , with that prosody .
Professor E: Ah . Uh , we were talking about Dan at one point {comment} and we were talking about Lokendra at another point .
Postdoc B: Yeah , depends on which one you mean .
Professor E: And I don't {disfmarker} I don't remember which {disfmarker} which part .
PhD F: Oh .
PhD C: It 's ambiguous , so it 's OK .
Professor E: Uh , I think {disfmarker}
Grad G: Well , the inference structures was Lokendra .
PhD F: But no . The inference stuff was {disfmarker} was {disfmarker} was Lokendra .
Professor E: Yeah . Yeah . Yeah .
PhD F: OK . That makes sense , yeah .
PhD C: And the downsampling must have been Dan .
Professor E: Um {disfmarker}
Grad G: Yeah .
Professor E: Good {disfmarker} Yeah .
PhD C: It 's an inference .
Professor E: Yeah , you could do all these inferences , yeah .
Grad G: Yeah .
Professor E: Yeah . Um , I {disfmarker} I would like to move it into {disfmarker} into uh what Jose uh has been doing
Postdoc B: Yeah .
Professor E: because he 's actually been doing something .
PhD D: Uh - huh . OK .
Professor E: So . {vocalsound} Right .
PhD F: As opposed to the rest of us .
PhD D: Well - {comment} {vocalsound} OK . I {disfmarker} I remind that me {disfmarker} my first objective eh , in the project is to {disfmarker} to study difference parameters to {disfmarker} to find a {disfmarker} a good solution to detect eh , the overlapping zone in eh speech recorded . But eh , {vocalsound} tsk , {comment} {vocalsound} ehhh {comment} In that way {comment} I {disfmarker} {vocalsound} I {disfmarker} {vocalsound} I begin to {disfmarker} to study and to analyze the ehn {disfmarker} the recorded speech eh the different session to {disfmarker} to find and to locate and to mark eh the {disfmarker} the different overlapping zone . And eh so eh I was eh {disfmarker} I am transcribing the {disfmarker} the first session and I {disfmarker} I have found eh , eh one thousand acoustic events , eh besides the overlapping zones , eh I {disfmarker} I {disfmarker} I mean the eh breaths eh aspiration eh , eh , talk eh , eh , clap , eh {disfmarker} {comment} I don't know what is the different names eh you use to {disfmarker} to name the {disfmarker} the {pause} n speech
PhD A: Nonspeech sounds ?
PhD D: Yeah .
Grad G: Oh , I don't think we 've been doing it at that level of detail . So .
PhD D: Yeah . Eh , {vocalsound} I {disfmarker} I {disfmarker} I do I don't need to {disfmarker} to {disfmarker} to mmm {vocalsound} {disfmarker} to m to label the {disfmarker} the different acoustic , but I prefer because eh I would like to {disfmarker} to study if eh , I {disfmarker} I will find eh , eh , a good eh parameters eh to detect overlapping I would like to {disfmarker} to {disfmarker} to test these parameters eh with the {disfmarker} another eh , eh acoustic events , to nnn {disfmarker} {vocalsound} to eh {disfmarker} to find what is the ehm {disfmarker} the false {disfmarker} eh , the false eh hypothesis eh , nnn , which eh are produced when we use the {disfmarker} the ehm {disfmarker} this eh parameter {disfmarker} eh I mean pitch eh , eh , difference eh , feature {disfmarker}
Grad G: Mm - hmm .
PhD A: You know {disfmarker} I think some of these um that are the nonspeech overlapping events may be difficult even for humans to tell that there 's two there .
Grad G: So it was {disfmarker}
PhD D: Yeah .
PhD A: I mean , if it 's a tapping sound , you wouldn't necessarily {disfmarker} or , you know , something like that , it 'd be {disfmarker} it might be hard to know that it was two separate events .
PhD D: Yeah . Yeah . Yeah . Yeah .
Grad G: Well {disfmarker} You weren't talking about just overlaps
PhD D: Ye
Grad G: were you ? You were just talking about acoustic events .
PhD D: I {disfmarker} I {disfmarker} I {disfmarker} I t I t I talk eh about eh acoustic events in general ,
Grad G: Someone starts , someone stops {disfmarker} Yeah .
PhD A: Oh .
PhD D: but eh my {disfmarker} my objective eh will be eh to study eh overlapping zone .
Grad G: Mm - hmm .
PhD D: Eh ? {comment} n Eh in twelve minutes I found eh , eh one thousand acoustic events .
Professor E: How many overlaps were there uh in it ? No no , how many of them were the overlaps of speech , though ?
PhD D: How many ? Eh almost eh three hundred eh in one session
Grad G: Oh , God !
PhD D: in five {disfmarker} eh in forty - five minutes .
PhD A: Three hundred overlapping speech {disfmarker}
PhD D: Alm - Three hundred overlapping zone .
Grad G: Ugh .
PhD C: Overlapping speech .
PhD D: With the overlapping zone , overlapping speech {disfmarker} speech what eh different duration .
PhD A: Mm - hmm .
Professor E: Sure .
Postdoc B: Does this {disfmarker} ? So if you had an overlap involving three people , how many times was that counted ?
PhD D: Yeah , three people , two people . Eh , um I would like to consider eh one people with difference noise eh in the background , be
Professor E: No no , but I think what she 's asking is {pause} if at some particular for some particular stretch you had three people talking , instead of two , did you call that one event ?
PhD D: Oh . Oh . Yeah . I consider one event eh for th for that eh for all the zone . This {disfmarker} th I {disfmarker} I {disfmarker} I con I consider {disfmarker} I consider eh an acoustic event , the overlapping zone , the period where three speaker or eh {disfmarker} are talking together .
Grad G: Well {disfmarker} So let 's {disfmarker}
Postdoc B: For
Grad G: So let 's say me and Jane are talking at the same time , and then Liz starts talking also over all of us . How many events would that be ?
PhD D: So - I don't understand .
Grad G: So , two people are talking , {comment} and then a third person starts talking .
PhD D: Yeah ?
Grad G: Is there an event right here ?
PhD D: Eh no . No no . For me is the overlapping zone , because {disfmarker} because you {disfmarker} you have s you have more one {disfmarker} eh , more one voice eh , eh produced in a {disfmarker} in {disfmarker} in a moment .
Professor E: I see .
Grad G: So i if two or more people are talking .
Professor E: OK . Yeah . So I think {disfmarker} Yeah . We just wanted to understand how you 're defining it .
PhD D: Yeah . If
Professor E: So then , in the region between {disfmarker} since there {disfmarker} there is some continuous region , in between regions where there is only one person speaking .
PhD D: Uh - huh .
Professor E: And one contiguous region like that you 're calling an event .
PhD D: Uh - huh .
Professor E: Is it {disfmarker} Are you calling the beginning or the end of it the event ,
PhD D: Yeah .
Professor E: or are you calling the entire length of it the event ?
PhD D: I consider the {disfmarker} the , nnn {disfmarker} the nnn , nnn {disfmarker} eh , the entirety eh , eh , all {disfmarker} all the time there were {disfmarker} the voice has overlapped .
Professor E: OK .
PhD D: This is the idea . But eh I {disfmarker} I don't distinguish between the {disfmarker} the numbers of eh speaker . Uh , I 'm not considering {vocalsound} eh the {disfmarker} the {disfmarker} ehm {vocalsound} eh , the fact of eh , eh , for example , what did you say ? Eh at first eh , eh two talkers are uh , eh speaking , and eh , eh a third person eh join to {disfmarker} to that . For me , it 's eh {disfmarker} it 's eh , all overlap zone , with eh several numbers of speakers is eh , eh the same acoustic event . Wi - but {disfmarker} uh , without any mark between the zone {disfmarker} of the overlapping zone with two speakers eh speaking together , and the zone with the three speakers .
Postdoc B: That would j just be one .
PhD D: It {disfmarker} One . One .
Postdoc B: OK .
PhD D: Eh , with eh , a beginning mark and the ending mark . Because eh {vocalsound} for me , is the {disfmarker} is the zone with eh some kind of eh distortion the spectral .
Professor E: Got it .
PhD D: I don't mind {disfmarker} By the moment , by the moment .
Grad G: Well , but {disfmarker} But you could imagine that three people talking has a different spectral characteristic than two .
PhD D: I {disfmarker} I don't {disfmarker} Yeah , but eh {disfmarker} but eh I have to study . {comment} What will happen in a general way ,
Professor E: Could .
Grad G: So . You had to start somewhere .
Professor E: Yeah . We just w
PhD C: So there 's a lot of overlap .
PhD D: I {disfmarker} {vocalsound} I don't know what eh will {disfmarker} will happen with the {disfmarker}
Grad G: Yep .
PhD C: So .
Grad G: That 's a lot of overlap ,
PhD D: Yeah ?
Professor E: So again , that 's {disfmarker} that 's three {disfmarker} three hundred in forty - five minutes that are {disfmarker} that are speakers , just speakers .
Grad G: yeah , for forty - five minutes .
PhD D: Yeah . Yeah .
Professor E: Uh - huh . OK . Yeah .
Postdoc B: But a {disfmarker} a {disfmarker} a th
Professor E: So that 's about eight per minute .
Postdoc B: But a thousand events in twelve minutes , that 's {disfmarker}
PhD D: Yeah , {pause} but {disfmarker} Yeah .
PhD C: But that can include taps .
PhD D: But {disfmarker}
Professor E: Uh . Yeah .
Postdoc B: Well , but a thousand taps in eight minutes is a l in twelve minutes is a lot .
PhD D: General .
PhD C: Actually {disfmarker}
PhD D: I {disfmarker} I con I consider {disfmarker} I consider acoustic events eh , the silent too .
Postdoc B: Silent .
Grad G: Silence starting or silence ending {disfmarker}
PhD D: Yeah , silent , ground to {disfmarker} bec to detect {disfmarker} eh because I consider acoustic event all the things are not eh speech .
PhD C: Oh , OK .
Professor E: Mm - hmm .
PhD A: Oh .
PhD D: In ge in {disfmarker} in {disfmarker} in a general point of view .
PhD C: Oh .
Professor E: OK , so how many of those thousand were silence ?
PhD C: Alright .
PhD D: in the per
PhD F: Not speech {disfmarker} not speech or too much speech .
PhD D: Too much speech .
Professor E: Right . So how many of those thousand were silence , silent sections ?
PhD D: Yeah . Uh silent , I {disfmarker} I {disfmarker} I {disfmarker} I don't {disfmarker} I {disfmarker} I haven't the {disfmarker} eh I {disfmarker} I would like to {disfmarker} to do a stylistic study
Professor E: Yeah .
PhD D: and give you eh with the report eh from eh the {disfmarker} the study from the {disfmarker} the {disfmarker} the session {disfmarker} one session .
Professor E: Yeah . Yeah .
PhD D: And I {disfmarker} I found that eh another thing . When eh {vocalsound} eh I w I {disfmarker} {vocalsound} I was eh look at eh nnn , the difference speech file , um , for example , eh if eh we use the ehm {disfmarker} the mixed file , to {disfmarker} to transcribe , the {disfmarker} the events and the words , I {disfmarker} I saw that eh the eh speech signal , collected by the eh this kind of mike {disfmarker} eh of this kind of mike , eh are different from the eh mixed signal eh , we eh {disfmarker} collected by headphone .
Grad G: Yep .
PhD D: And {disfmarker} It 's right .
Professor E: Yeah .
Grad G: Right .
PhD D: But the problem is {vocalsound} the following . The {disfmarker} the {disfmarker} the {disfmarker} I {disfmarker} I {disfmarker} I knew that eh the signal eh , eh would be different , but eh the {disfmarker} the problem is eh , eh we eh detected eh difference events in the speech file eh collected by {disfmarker} by that mike uh qui compared with the mixed file . And so if {disfmarker} when you transcribe eh only eh using the nnn {disfmarker} the mixed file , it 's possible {disfmarker} eh if you use the transcription to evaluate a different system , it 's possible you eh {disfmarker} in the eh i and you use the eh speech file collected by the eh fet mike , to eh {disfmarker} to nnn {disfmarker} to do the experiments {pause} with the {disfmarker} the system ,
Professor E: Mm - hmm .
Grad G: Right .
PhD D: its possible to evaluate eh , eh {disfmarker} or to consider eh acoustic events that {disfmarker} which you marked eh in the mixed file , but eh they don't appear in the eh speech signal eh collected by the {disfmarker} by the mike .
Grad G: Right . The {disfmarker} the reason that I generated the mixed file was for IBM to do word level transcription , not speech event transcription .
PhD D: Yeah . Yeah . Oh , it 's a good idea . It 's a good idea I think .
Grad G: So I agree that if someone wants to do speech event transcription , that the mixed signals here {disfmarker}
PhD D: Yeah .
Grad G: I mean , if I 'm tapping on the table , you it 's not gonna show up on any of the mikes , but it 's gonna show up rather loudly in the PZM .
PhD D: Yeah . Yeah . Yeah . So and I {disfmarker} I {disfmarker} {vocalsound} I say eh that eh , eh , or this eh only because eh I c I {disfmarker} I {disfmarker} {vocalsound} in my opinion , it 's necessary to eh {disfmarker} to eh {disfmarker} to put the transcription on the speech file , collected by the objective signal .
Grad G: So .
PhD D: I mean the {disfmarker} the {disfmarker} the signal collected by the {disfmarker} eh , the real mike in the future , in the prototype to {disfmarker} to eh correct the initial eh segmentation eh with the eh real speech
Professor E: Mm - hmm . The {disfmarker} the {disfmarker} the far - field , yeah .
PhD D: you have to {disfmarker} to analyze {disfmarker} you have to {disfmarker} to process . Because I {disfmarker} I found a difference .
Professor E: Yeah , well , just {disfmarker} I mean , just in that {disfmarker} that one s ten second , or whatever it was , example that Adam had that {disfmarker} that we {disfmarker} we passed on to others a few months ago , there was that business where I g I guess it was Adam and Jane were talking at the same time and {disfmarker} and uh , in the close - talking mikes you couldn't hear the overlap , and in the distant mike you could . So yeah , it 's clear that if you wanna study {disfmarker} if you wanna find all the places where there were overlap , it 's probably better to use a distant mike .
PhD F: That 's good .
Professor E: On the other hand , there 's other phenomena that are going on at the same time for which it might be useful to look at the close - talking mikes ,
PhD D: Yeah .
PhD C: But why can't you use the combination of the close - talking mikes , time aligned ?
Professor E: so it 's {disfmarker}
Grad G: If you use the combination of the close - talking mikes , you would hear Jane interrupting me , but you wouldn't hear the paper rustling . And so if you 're interested in {disfmarker}
PhD C: I {disfmarker} I mean if you 're interested in speakers overlapping other speakers and not the other kinds of nonspeech , that 's not a problem ,
Professor E: Some {comment} of it 's masking {disfmarker} masked .
PhD D: Yeah .
PhD A: Were you interrupting him or was he interrupting you ?
Professor E: Right .
PhD C: right ?
Grad G: Right .
PhD D: Yeah .
Grad G: Although the other issue is that the {pause} mixed close - talking mikes {disfmarker} I mean , I 'm doing weird normalizations and things like that .
PhD C: But it 's known .
PhD D: Yeah .
PhD C: I mean , the normalization you do is over the whole conversation
Grad G: Yep .
PhD C: isn't it , over the whole meeting .
Grad G: Right . Yep .
PhD C: So if you wanted to study people overlapping people , that 's not a problem .
PhD D: I {disfmarker} I {disfmarker} I think eh I saw the nnn {disfmarker} the {disfmarker} eh but eh I eh {disfmarker} I have eh any results . I {disfmarker} I {disfmarker} I saw the {disfmarker} the speech file collected by eh the fet mike , and eh eh signal eh to eh {disfmarker} to noise eh relation is eh low . It 's low .
Professor E: Mm - hmm .
PhD D: It 's very low . You would comp if we compare it with eh the headphone .
Grad G: Yep .
PhD D: And I {disfmarker} I found that nnn {disfmarker} that eh , {vocalsound} ehm , pr probably ,
Grad G: Did {disfmarker} Did you
PhD D: I 'm not sure eh by the moment , but it 's {disfmarker} it 's probably that eh a lot of eh , {vocalsound} eh for example , in the overlapping zone , on eh {disfmarker} in {disfmarker} in several eh parts of the files where you {disfmarker} you can find eh , eh {vocalsound} eh , smooth eh eh speech eh from eh one eh eh talker in the {disfmarker} in the meeting ,
Professor E: Mm - hmm . Mm - hmm .
PhD D: it 's probably in {disfmarker} in that eh {disfmarker} in {disfmarker} in those files you {disfmarker} you can not find {disfmarker} you can not process because eh it 's confused with {disfmarker} with noise .
Professor E: Mm - hmm .
PhD D: And there are {vocalsound} a lot of I think . But I have to study with more detail . But eh my idea is to {disfmarker} to process only {pause} nnn , this eh {disfmarker} nnn , this kind of s of eh speech . Because I think it 's more realistic . I 'm not sure it 's a good idea , but eh {disfmarker}
Professor E: No {disfmarker} i
Grad G: Well , it 's more realistic but it 'll {disfmarker} it 'll be a lot harder .
PhD D: Yeah .
Professor E: Well , it 'd be hard , but on the other hand as you point out , if your {disfmarker} if i if {disfmarker} if your concern is to get uh the overlapping people {disfmarker} people 's speech , you will {disfmarker} you will get that somewhat better .
PhD D: Mm - hmm . Yeah .
Professor E: Um , Are you making any use {disfmarker} uh you were {disfmarker} you were working with th the data that had already been transcribed .
PhD D: With {disfmarker} By Jane .
Professor E: Does it uh {disfmarker} Yes .
PhD D: Yeah .
Professor E: Now um did you make any use of that ? See I was wondering cuz we st we have these ten hours of other stuff that is not yet transcribed .
PhD D: Yeah . Yeah .
Professor E: Do you {disfmarker}
PhD D: The {disfmarker} the transcription by Jane , t eh i eh , I {disfmarker} I {disfmarker} I want to use to {disfmarker} to nnn , {vocalsound} eh to put {disfmarker} i i it 's a reference for me . But eh the transcription {disfmarker} eh for example , I {disfmarker} I don't {disfmarker} I {disfmarker} I 'm not interested in the {disfmarker} in the {disfmarker} in the words , transcription words , eh transcribed eh eh in {disfmarker} eh follow in the {disfmarker} {vocalsound} in the {disfmarker} in the speech file , but eh eh Jane eh for example eh put a mark eh at the beginning eh of each eh talker , in the {disfmarker} in the meeting , um eh she {disfmarker} she nnn includes information about the zone where eh there are eh {disfmarker} there is an overlapping zone . But eh there isn't any {disfmarker} any mark , time {disfmarker} temporal mark , to {disfmarker} to c eh {disfmarker} to mmm {vocalsound} {disfmarker} e - heh , to label {comment} the beginning and the end of the {disfmarker} of the
Professor E: Mm - hmm . OK . Right , so she is {disfmarker}
PhD D: ta I 'm {disfmarker} I {disfmarker} I {disfmarker} I think eh we need this information to
Professor E: Right . So the twelve {disfmarker} you {disfmarker} you {disfmarker} it took you twelve hours {disfmarker} of course this included maybe some {disfmarker} some time where you were learning about what {disfmarker} what you wanted to do , but {disfmarker} but uh , it took you something like twelve hours to mark the forty - five minutes , your
Grad G: Twelve minutes .
PhD D: Twelve minutes .
Professor E: s Twelve minutes !
PhD D: Twelve minutes . Twelve .
Professor E: I thought you did forty - five minutes of {disfmarker}
PhD D: No , forty - five minutes is the {disfmarker} is the session , all the session .
Postdoc B: Oh .
Professor E: Oh , you haven't done the whole session .
PhD D: Yeah , all is the {vocalsound} the session .
Professor E: This is just twelve minutes .
PhD D: Tw - twelve hours of work to {disfmarker} {vocalsound} to segment eh and label eh twelve minutes from a session of part {disfmarker} of f
Professor E: Oh . So {comment} let me back up again . So the {disfmarker} when you said there were three hundred speaker overlaps ,
PhD D: Yeah .
Professor E: that 's in twelve minutes ?
PhD D: No no no . I {disfmarker} I consider all the {disfmarker} all the session because eh I {disfmarker} I count the nnn {disfmarker} the nnn {disfmarker} the overlappings marked by {disfmarker} by Jane ,
Professor E: Oh , OK .
Postdoc B: Oh , I see .
PhD D: in {disfmarker} in {disfmarker} in {disfmarker} in the {pause} fin in {disfmarker} in the {pause} forty - five minutes .
Professor E: OK . So it 's three hundred in forty - five minutes , but you have {disfmarker} you have time uh , uh marked {disfmarker} twelve minute {disfmarker} the {disfmarker} the {disfmarker} the um overlaps in twelve minutes of it .
PhD D: Yeah .
Professor E: Got it .
PhD F: So , can I ask {disfmarker} {vocalsound} can I ask whether you found {disfmarker} uh , you know , how accurate uh Jane 's uh uh labels were as far as {disfmarker}
Grad G: Well , not just the overlaps , everything .
PhD F: you know , did she miss some overlaps ? or did she n ?
PhD D: But , by {disfmarker} by the moment , I {disfmarker} I don't compare , my {disfmarker} my temporal mark with eh Jane , but eh I {disfmarker} I want to do it . Because eh eh i per perhaps I have eh errors in the {disfmarker} in the marks , I {disfmarker} and if I {disfmarker} I compare with eh Jane , it 's probably I {disfmarker} I {disfmarker} I can correct and {disfmarker} and {disfmarker} and {disfmarker} to get eh eh a more accurately eh eh transcription in the file .
Professor E: Yeah .
Grad G: Well , also Jane {disfmarker} Jane was doing word level .
PhD D: Yeah .
Professor E: Yeah .
Grad G: So we weren't concerned with {comment} exactly when an overlap started and stopped .
PhD F: Right . Right .
PhD C: Well , not only a word level , but actually
PhD D: Well {disfmarker}
PhD F: I 'm expect I 'm not expecting {disfmarker}
PhD D: No , it 's {disfmarker}
PhD C: I mean , you didn't need to show the exact point of interruption , you just were showing at the level of the phrase or the level of the speech spurt , or {disfmarker}
Grad G: Right .
Professor E: Mm - hmm .
Grad G: Yep .
Postdoc B: Well {disfmarker}
PhD D: Yeah . Yeah .
Postdoc B: Well , yeah , b yeah , I would say time bin . So my {disfmarker} my goal is to get words with reference to a time bin , {pause} beginning and end point .
PhD C: Yeah .
PhD D: Yeah .
PhD C: Right .
PhD D: Yeah .
Postdoc B: And {disfmarker} and sometimes , you know , it was like you could have an overlap where someone said something in the middle ,
PhD D: Yeah .
Postdoc B: but , yeah , w it just wasn't important for our purposes to have it that {disfmarker} i disrupt that unit in order to have , you know , a the words in the order in which they were spoken , it would have {disfmarker} it would have been hard with the interface that we have .
PhD D: Yeah .
Postdoc B: Now , my {disfmarker} a Adam 's working on a of course , on a revised overlapping interface ,
PhD D: Uh - huh .
Grad G: Right .
PhD D: I {disfmarker} I {disfmarker} I think {disfmarker} It 's {disfmarker} it 's a good eh work ,
Postdoc B: but {disfmarker}
PhD D: but eh I think we need eh eh more information .
PhD F: No , of course .
Postdoc B: Yeah .
PhD F: I expect you to find more overlaps than {disfmarker} than Jane
Grad G: Always need more for {disfmarker}
Postdoc B: Yeah .
PhD D: No , no . I {disfmarker} I have to go to {disfmarker}
PhD F: because you 're looking at it at a much more detailed level .
PhD D: I want eh {disfmarker} I wanted to eh compare the {disfmarker} the transcription .
Professor E: I have {disfmarker}
Grad G: But if it takes sixty to one {disfmarker}
Professor E: Well , I but I have a suggestion about that . Um , obviously this is very , very time - consuming , and you 're finding lots of things which I 'm sure are gonna be very interesting , but in the interests of making progress , uh might I s how {disfmarker} how would it affect your time if you only marked speaker overlaps ?
PhD D: Only .
Professor E: Yes .
PhD D: Yeah .
Professor E: Do not mark any other events ,
PhD D: Uh - huh .
Professor E: but only mark speaker {disfmarker} Do you think that would speed it up quite a bit ?
PhD D: OK . OK . I {disfmarker} I {disfmarker} I {disfmarker} I w I {disfmarker} I wanted to {disfmarker}
Professor E: Do y do you think that would speed it up ? Uh , speed up your {disfmarker} your {disfmarker} your marking ?
PhD D: nnn , I don't understand very .
Professor E: It took you a long time {pause} to mark twelve minutes .
PhD D: Yeah . Oh , yeah , yeah .
Professor E: Now , my suggestion was for the other thirty - three {disfmarker}
PhD D: On - only to mark {disfmarker} only to mark overlapping zone , but {disfmarker}
Professor E: Yeah , and my question is , if you did that , if you followed my suggestion , would it take much less time ?
PhD D: Oh , yeah . Sure .
Professor E: Yeah OK .
PhD D: Yeah sure .
Professor E: Then I think it 's a good idea .
PhD D: Sure sure .
Professor E: Then I think it 's a good idea , because it
PhD D: Sure , because I {disfmarker} I need a lot of time to {disfmarker} to put the label or to do that . Yeah .
Professor E: Yeah , I mean , we we know that there 's noise .
Grad G: And
PhD D: Uh - huh .
Professor E: There 's {disfmarker} there 's uh continual noise uh from fans and so forth , and there is uh more impulsive noise from uh taps and so forth
PhD D: Yeah .
Professor E: and {disfmarker} and something in between with paper rustling . We know that all that 's there and it 's a g worthwhile thing to study , but obviously it takes a lot of time to mark all of these things .
PhD D: Yeah .
Professor E: Whereas th i I would think that uh you {disfmarker} we can study more or less as a distinct phenomenon the overlapping of people talking .
PhD D: Uh - huh . OK . OK .
Professor E: So . Then you can get the {disfmarker} Cuz you need {disfmarker} If it 's three hundred uh {disfmarker} i i it sounds like you probably only have fifty or sixty or seventy events right now that are really {disfmarker}
PhD D: Yeah .
Professor E: And {disfmarker} and you need to have a lot more than that to have any kind of uh even visual sense of {disfmarker} of what 's going on , much less any kind of reasonable statistics .
Grad G: Right .
PhD C: Now , why do you need to mark speaker overlap by hand if you can infer it from the relative energy in the {disfmarker}
Grad G: Well , that 's {disfmarker} That 's what I was gonna bring up .
PhD C: I mean , you shouldn't need to do this p completely by hand ,
Professor E: Um , OK , yeah . So let 's back up because you weren't here for an earlier conversation .
PhD C: right ? I 'm sorry .
Professor E: So the idea was that what he was going to be doing was experimenting with different measures such as the increase in energy , such as the energy in the LPC residuals , such as {disfmarker} I mean there 's a bunch of things {disfmarker} I mean , increased energy is - is sort of an obvious one .
PhD C: Mm - hmm . In the far - field mike .
Professor E: Yeah .
PhD C: Oh , OK .
Professor E: Um , and uh , it 's not obvious , I mean , you could {disfmarker} you could do the dumbest thing and get {disfmarker} get it ninety percent of the time . But when you start going past that and trying to do better , it 's not obvious what combination of features is gonna give you the {disfmarker} you know , the right detector . So the idea is to have some ground truth first . And so the i the idea of the manual marking was to say " OK this , i you know , it 's {disfmarker} it 's really here " .
PhD A: But I think Liz is saying why not get it out of the transcripts ?
PhD C: What I mean is {pause} get it from the close - talking mikes .
Professor E: Uh , yeah .
PhD C: A or ge get a first pass from those ,
Professor E: We t we t w we t we talked about that .
PhD C: and then go through sort of {disfmarker} It 'd be a lot faster probably to {disfmarker}
PhD F: And you can {disfmarker}
Grad G: Yeah , that 's his , uh {disfmarker}
Professor E: We {disfmarker} we {disfmarker} we talked about that . s But so it 's a bootstrapping thing and the thing is ,
PhD C: Yeah , I just {disfmarker}
Professor E: the idea was , i we i i we thought it would be useful for him to look at the data anyway , and {disfmarker} and then whatever he could mark would be helpful ,
PhD C: Right .
Professor E: and we could {disfmarker} Uh it 's a question of what you bootstrap from . You know , do you bootstrap from a simple measurement which is right most of the time and then you g do better , or do you bootstrap from some human being looking at it and then {disfmarker} then do your simple measurements , uh from the close - talking mike . I mean , even with the close - talking mike you 're not gonna get it right all the time .
PhD C: Well , that 's what I wonder , because um {disfmarker} or how bad it is ,
Professor E: Well
PhD C: be um , because that would be interesting
Grad G: I 'm working on a program to do that , and {disfmarker}
PhD C: especially because the bottleneck is the transcription . Right ? I mean , we 've got a lot more data than we have transcriptions for . We have the audio data , we have the close - talking mike ,
Professor E: Yeah .
PhD C: so I mean it seems like one kind of project that 's not perfect , but {disfmarker} um , that you can get the training data for pretty quickly is , you know , if you infer form the close - talking mikes where the on - off points are of speech ,
Professor E: Right , we discussed that .
PhD C: you know , how can we detect that from a far - field ?
Grad G: And {disfmarker}
Postdoc B: Oh .
Grad G: I 've {disfmarker} I 've written a program to do that ,
PhD C: OK , I 'm sorry I missed the {disfmarker}
Grad G: and it , uh {disfmarker}
Professor E: It 's OK .
Grad G: and {disfmarker} so {disfmarker} but it 's {disfmarker} it 's doing something very , very simple . It just takes a threshold , based on {disfmarker} on the volume ,
PhD C: Uh - huh .
PhD F: Or you can set the threshold low and then weed out the false alarms by hand .
PhD C: Right , by hand . Yeah .
PhD F: Yeah .
Grad G: um , and then it does a median filter , and then it looks for runs . And , it seems to work , I 've {disfmarker} I 'm sort of fiddling with the parameters , to get it to actually generate something , and I haven't {disfmarker} I don't {disfmarker} what I 'm working on {disfmarker} was working on {disfmarker} was getting it to a form where we can import it into the user interface that we have , {pause} into Transcriber . And so {disfmarker} I told {disfmarker} I said it would take about a day . I 've worked on it for about half a day ,
Grad H: I have to go .
Grad G: so give me another half day and I we 'll have something we can play with .
PhD C: OK .
Professor E: See , this is where we really need the Meeting Recorder query stuff to be working , because we 've had these meetings and we 've had this discussion about this , and I 'm sort of remembering a little bit about what we decided ,
PhD C: Right . I 'm sorry . I just {disfmarker}
Professor E: but I couldn't remember all of it .
PhD C: It
Professor E: So , I think it was partly that , you know , give somebody a chance to actually look at the data and see what these are like , partly that we have e some ground truth to compare against , you know , when {disfmarker} when he {disfmarker} he gets his thing going ,
Grad G: But {disfmarker}
Professor E: uh , and {disfmarker}
PhD C: Well , it 's definitely good to have somebody look at it . I was just thinking as a way to speed up you know , the amount of {disfmarker}
Postdoc B: Mm - hmm .
Professor E: That was {disfmarker} that was exactly the notion that {disfmarker} that {disfmarker} that we discussed .
PhD C: OK .
Grad G: Thanks .
Postdoc B: Another thing we discussed was um that {disfmarker}
PhD C: It looks good .
Professor E: So .
PhD C: I 'll be in touch . Thanks .
Professor E: S See ya . Yeah .
Postdoc B: Was that um there m {pause} there was this already a script I believe uh that Dan had written , {comment} that uh handle bleedthrough , I mean cuz you have this {disfmarker} this close {disfmarker} you have contamination from other people who speak loudly .
Grad G: Yeah , and I haven't tried using that . It would probably help the program that I 'm doing to first feed it through that . It 's a cross - correlation filter . So I {disfmarker} I haven't tried that , but that {disfmarker} If {disfmarker} It {disfmarker} it might be something {disfmarker} it might be a good way of cleaning it up a little .
Postdoc B: So , some thought of maybe having {disfmarker} Yeah , having that be a preprocessor and then run it through yours .
Grad G: Exactly . Yep .
Professor E: But {disfmarker} but that 's a refinement
Postdoc B: That 's what we were discussing .
Professor E: and I think we wanna see {disfmarker} try the simple thing first , cuz you add this complex thing up uh afterwards that does something good y y yo you sort of wanna see what the simple thing does first .
Grad G: Yep .
Professor E: But uh , having {disfmarker} having somebody have some experience , again , with {disfmarker} with uh {disfmarker} with marking it from a human standpoint , we 're {disfmarker} I mean , I don't expect Jose to {disfmarker} to do it for uh f fifty hours of {disfmarker} {comment} of speech , but I mean we {disfmarker} {comment} if uh {disfmarker} if he could speed up what he was doing by just getting the speaker overlaps so that we had it , say , for forty - five minutes , then at least we 'd have three hundred examples of it .
PhD D: Yeah . Sure . Sure .
Professor E: And when {disfmarker} when uh Adam was doing his automatic thing he could then compare to that and see what it was different .
PhD C: Oh yeah , definitely .
PhD A: You know , I did {disfmarker} I did uh something almost identical to this at one of my previous jobs , and it works pretty well . I mean , i almost exactly what you described , an energy detector with a median filter , you look for runs . And uh , you know , you can {disfmarker}
Grad G: It seemed like the right thing to do .
PhD A: Yeah . I mean , you {disfmarker} you can get y I mean , you get them pretty close .
Grad G: That was with zero literature search .
PhD A: And so I think doing that to generate these possibilities and then going through and saying yes or no on them would be a quick way to {disfmarker} to do it .
Grad G: That 's good validation .
PhD A: Yeah .
Postdoc B: Is this proprietary ?
PhD A: Uh . {comment} No . No .
Grad G: Yeah , do you have a patent on it ?
PhD A: It was when I was working for the government .
Professor E: Oh , then everybody owns it . It 's the people .
Postdoc B: Well , I mean , is this something that we could just co - opt , or is it {disfmarker} ?
PhD A: Nah .
Postdoc B: No . OK .
Professor E: Well , i i i he 's pretty close , anyway . I think {disfmarker} I think it 's {disfmarker}
PhD A: Yeah , he 's {disfmarker} it {disfmarker} it doesn't take a long time .
Postdoc B: Right . I just thought if it was tried and true , then {disfmarker} {comment} and he 's gone through additional levels of {disfmarker} of development .
Grad G: Just output . Although if you {disfmarker} if you have some parameters like what 's a good window size for the median filter {disfmarker}
PhD A: Oh ! {comment} I have to remember . I 'll think about it , and try to remember .
PhD F: And it might be different for government people .
Grad G: That 's alright .
Professor E: Yeah , good enough for government work , as they say .
PhD C: They {disfmarker} they {disfmarker}
PhD A: Di - dif different {disfmarker} different bandwidth .
PhD F: They
Grad G: I was doing pretty short , you know , tenth of a second , {comment} sorts of numbers .
PhD F: OK .
Professor E: Uh , I don't know , it {disfmarker} if {disfmarker} if we want to uh {disfmarker} So , uh , maybe we should move on to other {disfmarker} other things in limited time .
Postdoc B: Can I ask one question about his statistics ? So {disfmarker} so in the tw twelve minutes , um , if we took three hundred and divided it by four , which is about the length of twelve minutes , i Um , I 'd expect like there should be seventy - five overlaps .
Professor E: Yeah .
Postdoc B: Did you find uh more than seventy - five overlaps in that period , or {disfmarker} ?
PhD D: More than ?
Postdoc B: More than {disfmarker} How many overlaps in your twelve minutes ?
PhD D: How many ? Eh , not @ @ I Onl - only I {disfmarker} I transcribe eh only twelve minutes from the
Professor E: Yeah .
PhD D: but eh I {disfmarker} I don't co eh {disfmarker} I don't count eh the {disfmarker} the overlap .
Postdoc B: The overlaps . OK .
PhD D: I consider I {disfmarker} I {disfmarker} The {disfmarker} the nnn {disfmarker} The {disfmarker} the three hundred is eh considered only you {disfmarker} your transcription . I have to {disfmarker} {vocalsound} to finish transcribing . So .
Grad G: I b I bet they 're more , because the beginning of the meeting had a lot more overlaps than {disfmarker} than sort of the middle .
PhD D: Yeah .
Grad G: Middle or end .
Postdoc B: I 'm not sure .
PhD D: Yeah .
Grad G: Because i we 're {disfmarker} we 're dealing with the {disfmarker} Uh , in the early meetings , we 're recording while we 're saying who 's talking on what microphone , {comment} and things like that ,
PhD D: Yeah .
Grad G: and that seems to be a lot of overlap .
Postdoc B: I think it 's an empirical question .
PhD D: Yeah .
Postdoc B: I think we could find that out .
PhD D: Yeah .
Grad G: Yep .
Postdoc B: I 'm {disfmarker} I 'm not sure that the beginning had more .
Professor E: So {disfmarker} so I was gonna ask , I guess about any {disfmarker} any other things that {disfmarker} that {disfmarker} that either of you wanted to talk about , especially since Andreas is leaving in five minutes , that {disfmarker} that you wanna go with .
PhD C: Can I just ask about the data , like very straightforward question is where we are on the amount of data and the amount of transcribed data , just cuz I 'm {disfmarker} I wanted to get a feel for that to sort of be able to know what {disfmarker} what can be done first and like how many meetings are we recording
Professor E: Right so there 's this {disfmarker} this {disfmarker} There 's this forty - five minute piece that Jane transcribed .
PhD C: and {disfmarker}
Professor E: That piece was then uh sent to IBM so they could transcribe so we have some comparison point . Then there 's s a larger piece that 's been recorded and uh put on CD - ROM and sent uh to IBM . Right ? And then we don't know .
PhD C: How many meetings is that ? Like {disfmarker} how many {disfmarker}
Grad G: What 's that ?
Professor E: That was about ten hours , and there was about {disfmarker}
PhD C: t ten {disfmarker} It 's like ten meetings or something ? Uh - huh .
Grad G: Yeah , something like that . And then {disfmarker} then we
PhD A: Ten meetings that have been sent to IBM ?
PhD C: And {disfmarker}
Professor E: Yeah .
Grad G: Well , I haven't sent them yet because I was having this problem with the {pause} missing files .
Professor E: Oh . Oh , that 's right , that had {disfmarker} those have not been sent .
PhD A: H how many total have we recorded now , altogether ?
Professor E: We 're saying about {pause} twelve hours .
Grad G: About twelve {pause} by now . Twelve or thirteen .
PhD C: Uh - huh . And we 're recording only this meeting , like continuously we 're only recording this one now ? or {disfmarker} ?
Professor E: No . No , so the {disfmarker} the {disfmarker} that 's the {disfmarker} that 's the biggest one {disfmarker} uh , chunk so far ,
Grad G: Nope .
PhD A: It was the morning one .
PhD C: OK .
Professor E: but there 's at least one meeting recorded of uh the uh uh natural language guys .
Grad G: Jerry .
PhD C: Do they meet every week ,
Professor E: And then there {disfmarker}
PhD C: or every {disfmarker}
Professor E: Uh , they do . w w And we talked to them about recording some more and we 're going to , uh , we 've started having a morning meeting , today uh i starting a w a week or two ago , on the uh front - end issues , and we 're recording those , uh there 's a network services and applications group here who 's agreed to have their meetings recorded ,
PhD C: Great .
Professor E: and we 're gonna start recording them . They 're {disfmarker} They meet on Tuesdays . We 're gonna start recording them next week . So actually , we 're gonna h start having a {disfmarker} a pretty significant chunk and so , you know , {vocalsound} Adam 's sort of struggling with trying to get things to be less buggy , and come up quicker when they do crash and stuff {disfmarker} things like that , now that uh {disfmarker} {vocalsound} the things are starting to happen . So right now , yeah , I th I 'd say the data is predominantly meeting meetings , but there are scattered other meetings in it and that {disfmarker} that amount is gonna grow uh so that the meeting meetings will probably ultimately {disfmarker} i if we 're {disfmarker} if we collect fifty or sixty hours , the meeting meetings it will probably be , you know , twenty or thirty percent of it , not {disfmarker} not {disfmarker} not eighty or ninety . But .
PhD C: So there 's probably {disfmarker} there 's three to four a week ,
Grad G: That 's what we 're aiming for .
PhD C: that we 're aiming for .
Professor E: Yeah .
PhD C: And they 're each about an hour or something .
Professor E: Yeah , yeah .
Grad G: Although {disfmarker} Yeah . We 'll find out tomorrow whether we can really do this or not .
PhD C: So {disfmarker} OK .
Professor E: Yeah and th the {disfmarker} the other thing is I 'm not pos I 'm sort of thinking as we 've been through this a few times , that I really don't know {disfmarker} maybe you wanna do it once for the novelty , but I don't know if in general we wanna have meetings that we record from outside this group do the digits .
Grad G: Right .
Professor E: Because it 's just an added bunch of weird stuff .
PhD C: Yeah .
Professor E: And , you know , we {disfmarker} we h we 're highly motivated . Uh in fact , the morning group is really motivated cuz they 're working on connected digits , so it 's {disfmarker}
Grad G: Actually that 's something I wanted to ask , is I have a bunch of scripts to help with the transcription of the digits .
Professor E: Yeah .
Grad G: We don't have to hand - transcribe the digits because we 're reading them and I have those .
PhD C: Right .
Professor E: Yeah .
Grad G: And so I have some scripts that let you very quickly extract the sections of each utterance . But I haven't been ru I haven't been doing that . Um , if I did that , is someone gonna be working on it ?
Professor E: Uh , yeah , I {disfmarker} I think definitely s so Absolutely .
Grad G: I mean , is it something of interest ?
Professor E: Yeah , whoever we have working on the acoustics for the Meeting Recorder are gonna start with that .
Grad G: OK . I mean , I I 'm {disfmarker} I 'm interested in it , I just don't have time to do it now .
PhD F: I was {disfmarker} these meetings {disfmarker} I 'm sure someone thought of this , but these {disfmarker} this uh reading of the numbers would be extremely helpful to do um adaptation .
Grad G: So
PhD F: Um .
Grad G: Yep . Yep .
PhD C: Actually I have o
Grad G: I {disfmarker} I would really like someone to do adaptation .
PhD F: Mm - hmm .
Grad G: So if we got someone interested in that , I think it would be great for Meeting Recorder .
Professor E: Well {disfmarker} I mean , one of the things I wanted to do , uh , that I I talked to {disfmarker} to Don about , is one of the possible things he could do or m also , we could have someone else do it , is to do block echo cancellation ,
Grad G: Since it 's the same people over and over .
PhD F: Mm - hmm .
Professor E: to try to get rid of some of the effects of the {disfmarker} the {disfmarker} the far - field effects . Um , I mean we have {disfmarker} the party line has been that echo cancellation is not the right way to handle the situation
PhD F: Mm - hmm .
Professor E: because people move around , and uh , if {disfmarker} if it 's {disfmarker} if it 's uh not a simple echo , like a cross - talk kind of echo , but it 's actually room acoustics , it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} you can't really do inversion ,
PhD F: Mm - hmm .
Professor E: and even echo cancellation is going to uh be something {disfmarker} It may {disfmarker} you {disfmarker} Someone may be moving enough that you are not able to adapt quickly and so the tack that we 've taken is more " lets come up with feature approaches and multi - stream approaches and so forth , that will be robust to it for the recognizer and not try to create a clean signal " .
PhD F: Mm - hmm .
Professor E: Uh , that 's the party line . But it occurred to me a few months ago that uh party lines are always , you know , sort of dangerous . It 's good {disfmarker} {vocalsound} good to sort of test them , actually . And so we haven't had anybody try to do a good serious job on echo cancellation and we should know how well that can do . So that 's something I 'd like somebody to do at some point , just take these digits , take the far - field mike signal , and the close uh mike signal , and apply really good echo cancellation . Um , there was a {disfmarker} have been some nice talks recently by {disfmarker} by Lucent on {disfmarker} on their b
PhD F: Hmm .
Professor E: the block echo cancellation particularly appealed to me , uh you know , trying and change it sample by sample , but you have some reasonable sized blocks . {comment} And um , you know , th
PhD A: W what is the um {disfmarker} the artifact you try to {disfmarker} you 're trying to get rid of when you do that ?
PhD F: Ciao .
Professor E: Uh so it 's {disfmarker} it {disfmarker} you have a {disfmarker} a direct uh {disfmarker} Uh , what 's the difference in {disfmarker} If you were trying to construct a linear filter , that would um {disfmarker}
PhD F: I 'm signing off .
Professor E: Yeah . that would subtract off {comment} the um uh parts of the signal that were the aspects of the signal that were different between the close - talk and the distant . You know , so {disfmarker} so uh um I guess in most echo cancellation {disfmarker} Yeah , so you {disfmarker} Given that um {disfmarker} Yeah , so you 're trying to {disfmarker} So you 'd {disfmarker} There 's a {disfmarker} a distance between the close and the distant mikes so there 's a time delay there , and after the time delay , there 's these various reflections . And if you figure out well what 's the {disfmarker} there 's a {disfmarker} a least squares algorithm that adjusts itself {disfmarker} adjusts the weight so that you try to subtract {disfmarker} essentially to subtract off uh different uh {disfmarker} different reflections . Right ? So let 's take the simple case where you just had {disfmarker} you had some uh some delay in a satellite connection or something and then there 's a {disfmarker} there 's an echo . It comes back . And you want to adjust this filter so that it will maximally reduce the effect of this echo .
PhD A: So that would mean like if you were listening to the data that was recorded on one of those . Uh , just the raw data , you would {disfmarker} you might hear kind of an echo ? And {disfmarker} and then this {disfmarker} noise cancellation would get
Professor E: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm saying {disfmarker} That 's a simplified version of what 's really happening . {comment} What 's really happening is {disfmarker} Well , when I 'm talking to you right now , you 're getting the direct sound from my speech , but you 're also getting , uh , the indirect sound that 's bounced around the room a number of times . OK ? So now , if you um try to r you {disfmarker} To completely remove the effect of that is sort of impractical for a number of technical reasons , but I {disfmarker} but {disfmarker} not to try to completely remove it , that is , invert the {disfmarker} the room response , but just to try to uh uh eliminate some of the {disfmarker} the effect of some of the echos . Um , a number of people have done this so that , say , if you 're talking to a speakerphone , uh it makes it more like it would be , if you were talking right up to it . So this is sort of the st the straight - forward approach . You say I {disfmarker} I {disfmarker} I want to use this uh {disfmarker} this item but I want to subtract off various kinds of echos . So you construct a filter , and you have this {disfmarker} this filtered version uh of the speech um gets uh uh {disfmarker} gets subtracted off from the original speech . Then you try to {disfmarker} you try to minimize the energy in some sense . And so um {disfmarker} uh with some constraints .
PhD A: Kind of a clean up thing , that {disfmarker}
Professor E: It 's a clean up thing . Right .
PhD A: OK .
Professor E: So , echo cancelling is {disfmarker} is , you know , commonly done in telephony , and {disfmarker} and {disfmarker} and it 's sort of the obvious thing to do in this situation if you {disfmarker} if , you know , you 're gonna be talking some distance from a mike .
PhD A: When uh , I would have meetings with the folks in Cambridge when I was at BBN over the phone , they had a um {disfmarker} some kind of a special speaker phone and when they would first connect me , it would come on and we 'd hear all this noise . And then it was uh {disfmarker} And then it would come on and it was very clear ,
Professor E: Yeah .
PhD A: you know .
Professor E: Right . So it 's taking samples , it 's doing adaptation , it 's adjusting weights , and then it 's getting the sum . So um , uh anyway that 's {disfmarker} that 's kind of a reasonable thing that I 'd like to have somebody try {disfmarker} somebody look {disfmarker} And {disfmarker} and the digits would be a reasonable thing to do that with . I think that 'd be enough data {disfmarker} plenty of data to do that with , and i for that sort of task you wouldn't care whether it was uh large vocabulary speech or anything . Uh . {vocalsound} Um
Postdoc B: Is Brian Kingsbury 's work related to that , or is it a different type of reverberation ?
Professor E: Brian 's {comment} Kingsbury 's work is an example of what we did f f from the opposite dogma . Right ? Which is what I was calling the " party line " , which is that uh doing that sort of thing is not really what we want . We want something more flexible , uh i i where people might change their position , and there might be , you know {disfmarker} There 's also um oh yeah , noise . So the echo cancellation does not really allow for noise . It 's if you have a clean situation but you just have some delays , Then we 'll figure out the right {disfmarker} the right set of weights for your taps for your filter in order to produce the effect of those {disfmarker} those echos . But um if there 's noise , then the very signal that it 's looking at is corrupted so that it 's decision about what the right {disfmarker} you know , right {disfmarker} right uh {disfmarker} delays are {disfmarker} is , uh {disfmarker} is {disfmarker} right delayed signal is {disfmarker} is {disfmarker} is {disfmarker} uh is incorrect . And so , in a noisy situation , um , also in a {disfmarker} in a situation that 's very reverberant {disfmarker} {comment} with long reverberation times {comment} and really long delays , it 's {disfmarker} it 's sort of typically impractical . So for those kind of reasons , and also a {disfmarker} a c a complete inversion , if you actually {disfmarker} I mentioned that it 's kind of hard to really do the inversion of the room acoustics . Um , that 's difficult because um often times the {disfmarker} the um {disfmarker} {vocalsound} the system transfer function is such that when it 's inverted you get something that 's unstable , and so , if you {disfmarker} you do your estimate of what the system is , and then you try to invert it , you get a filter that actually uh , you know , rings , and {disfmarker} and uh goes to infinity . So it 's {disfmarker} so there 's {disfmarker} there 's {disfmarker} there 's that sort of technical reason , and the fact that things move , and there 's air currents {disfmarker} I mean there 's all sorts of {disfmarker} all sorts of reasons why it 's not really practical . So for all those kinds of reasons , uh we {disfmarker} we {disfmarker} we sort of um , concluded we didn't want to in do inversion , and we 're even pretty skeptical of echo cancellation , which isn't really inversion , and um we decided to do this approach of taking {disfmarker} uh , just picking uh features , which were {disfmarker} uh will give you more {disfmarker} something that was more stable , in the presence of , or absence of , room reverberation , and that 's what Brian was trying to do . So , um , let me just say a couple things that I was {disfmarker} I was gonna bring up . Uh . Let 's see . I guess you {disfmarker} you actually already said this thing about the uh {disfmarker} about the consent forms , which was that we now don't have to {disfmarker} So this was the human subjects folks who said this , {comment} or that {disfmarker} that {disfmarker} ?
Postdoc B: The a apparently {disfmarker} I mean , we 're gonna do a revised form , of course . Um but once a person has signed it once , then that 's valid for a certain number of meetings . She wanted me to actually estimate how many meetings and put that on the consent form . I told her that would be a little bit difficult to say . So I think from a s practical standpoint , maybe we could have them do it once every ten meetings , or something . It won't be that many people who do it {pause} that often , but um just , you know , so long as they don't forget that they 've done it , I guess .
Professor E: OK . Um , back on the data thing , so there 's this sort of one hour , ten hour , a hundred hour sort of thing that {disfmarker} that we have . We have {disfmarker} we have an hour uh that {disfmarker} that is transcribed , we have {disfmarker} we have twelve hours that 's recorded but not transcribed , and at the rate we 're going , uh by the end of the semester we 'll have , I don't know , forty or fifty or something , if we {disfmarker} if this really uh {disfmarker} Well , do we have that much ?
PhD C: Not really . It 's three to four per week .
Professor E: Let 's see , we have {disfmarker}
PhD C: So that 's what {disfmarker} You know , that {disfmarker}
Professor E: uh eight weeks , uh is {disfmarker}
PhD C: So that 's not a lot of hours .
Professor E: Eight weeks times three hours is twenty - four , so that 's {disfmarker} Yeah , so like thirty {disfmarker} thirty hours ?
PhD A: Three {disfmarker} Three hours .
PhD C: Yeah . I mean , is there {disfmarker} I know this sounds {pause} tough but we 've got the room set up . Um I was starting to think of some projects where you would use well , similar to what we talked about with uh energy detection on the close - talking mikes . There are a number of interesting questions that you can ask about how interactions happen in a meeting , that don't require any transcription . So what are the patterns , the energy patterns over the meeting ? And I 'm really interested in this {vocalsound} but we don't have a whole lot of data . So I was thinking , you know , we 've got the room set up and you can always think of , also for political reasons , if ICSI collected you know , two hundred hours , that looks different than forty hours , even if we don't transcribe it ourselves ,
Professor E: But I don't think we 're gonna stop at the end of this semester .
PhD C: so {disfmarker}
Professor E: Right ? So , I th I think that if we are able to keep that up for a few months , we are gonna have more like a hundred hours .
PhD C: I mean , is there {disfmarker} Are there any other meetings here that we can record , especially meetings that have some kind of conflict in them {comment} or some kind of deci I mean , that are less well {disfmarker} I don't {disfmarker} uh , that have some more emotional aspects to them , or strong {disfmarker}
Grad G: We had some good ones earlier .
PhD C: There 's laughter , um I 'm talking more about strong differences of opinion meetings , maybe with manager types , or {disfmarker}
Grad G: I think it 's hard to record those .
PhD C: To be allowed to record them ?
Postdoc B: It 's also likely that people will cancel out afterwards .
PhD C: OK .
Professor E: Yeah , people will get {disfmarker}
Postdoc B: But I {disfmarker} but I wanted to raise the KPFA idea .
PhD C: OK . Well , if there is , anyway .
Professor E: Yeah , I was gonna mention that .
Grad G: Oh , that 's a good idea . That 's {disfmarker} That would be a good match .
Professor E: Yeah . So {disfmarker} Yeah . So I {disfmarker} I {disfmarker} uh , I {disfmarker} I 'd mentioned to Adam , and {disfmarker} that was another thing I was gonna talk {disfmarker} uh , mention to them before {disfmarker} {comment} that uh there 's uh {disfmarker} It {disfmarker} it oc it occurred to me that we might be able to get some additional data by talking to uh acquaintances in local broadcast media . Because , you know , we had talked before about the problem about using found data , {comment} that {disfmarker} that uh it 's just set up however they have it set up and we don't have any say about it and it 's typically one microphone , in a , uh , uh {disfmarker} or {disfmarker} and {disfmarker} and so it doesn't really give us the {disfmarker} the {disfmarker} the uh characteristics we want . Um and so I do think we 're gonna continue recording here and record what we can . But um , it did occur to me that we could go to friends in broadcast media and say " hey you have this panel show , {pause} or this {disfmarker} you know , this discussion show , and um can you record multi - channel ? " And uh they may be willing to record it uh with {disfmarker}
PhD C: With lapel mikes or something ?
Professor E: Well , they probably already use lapel , but they might be able to have it {disfmarker} it wouldn't be that weird for them to have another mike that was somewhat distant .
PhD C: Right .
Professor E: It wouldn't be exactly this setup , but it would be that sort of thing , and what we were gonna get from UW , you know , assuming they {disfmarker} they {disfmarker} they start recording , isn't {disfmarker} als also is not going to be this exact setup .
PhD C: Right . No , I think that 'd be great , if we can get more data .
Professor E: So , {comment} I {disfmarker} I {disfmarker} I {disfmarker} I was thinking of looking into that . the other thing that occurred to me after we had that discussion , in fact , is that it 's even possible , since of course , many radio shows are not live , {comment} uh that we could invite them to have like some of their {disfmarker} {comment} record some of their shows here .
Postdoc B: Wow !
PhD C: Well {disfmarker} Or {disfmarker} The thing is , they 're not as averse to wearing one of these head - mount I mean , they 're on the radio ,
Grad G: Right , as we are .
PhD C: right ? So . {comment} Um , I think that 'd be fantastic
Professor E: Right .
PhD C: cuz those kinds of panels and {disfmarker} Those have interesting
Professor E: Yeah .
PhD C: Th - that 's an {disfmarker} a side of style {disfmarker} a style that we 're not collecting here , so it 'd be great .
Professor E: And {disfmarker} and the {disfmarker} I mean , the other side to it was the {disfmarker} what {disfmarker} which is where we were coming from {disfmarker} I 'll {disfmarker} I 'll talk to you more about it later {comment} is that {disfmarker} is that there 's {disfmarker} there 's uh the radio stations and television stations already have stuff worked out presumably , uh related to , you know , legal issues and {disfmarker} and permissions and all that . I mean , they already do what they do {disfmarker} do whatever they do . So it 's {disfmarker} uh , it 's {disfmarker} So it 's {disfmarker} so it 's another source . So I think it 's something we should look into , you know , we 'll collect what we collect here hopefully they will collect more at UW also and um {disfmarker} and maybe we have this other source . But yeah I think that it 's not unreasonable to aim at getting , you know , significantly in excess of a hundred hours . I mean , that was sort of our goal . The thing was , I was hoping that we could {disfmarker} @ @ in the {disfmarker} under this controlled situation we could at least collect , you know , thirty to fifty hours . And at the rate we 're going we 'll get pretty close to that I think this semester . And if we continue to collect some next semester , I think we should , uh {disfmarker}
PhD C: Right . Yeah I was mostly trying to think , " OK , if you start a project , within say a month , you know , how much data do you have to work with . And you {disfmarker} you wanna s you wanna sort of fr freeze your {disfmarker} your data for awhile so um right now {disfmarker} and we don't have the transcripts back yet from IBM right ? Do {disfmarker} Oh , do we now ?
Professor E: Well , we don't even have it for this f you know , forty - five minutes , that was {disfmarker}
PhD C: So um , not complaining , I was just trying to think , you know , what kinds of projects can you do now versus uh six months from now
Professor E: Yeah .
PhD C: and they 're pretty different , because
Professor E: Yeah . So I was thinking right now it 's sort of this exploratory stuff where you {disfmarker} you look at the data , you use some primitive measures and get a feeling for what the scatter plots look like ,
Grad G: Right .
PhD C: um {disfmarker} Right . Right , right .
Professor E: and {disfmarker} and {disfmarker} and uh {disfmarker} and meanwhile we collect , and it 's more like yeah , three months from now , or six months from now you can {disfmarker} you can do a lot of other things .
PhD C: Cuz I 'm not actually sure , just logistically that I can spend {disfmarker} you know , I don't wanna charge the time that I have on the project too early , before there 's enough data to make good use of the time . And that 's {disfmarker} and especially with the student
Grad G: Right .
PhD C: uh for instance this guy who seems {disfmarker}
Professor E: Yeah .
PhD C: Uh anyway , I shouldn't say too much , but um if someone came that was great and wanted to do some real work and they have to end by the end of this school year in the spring , how much data will I have to work with , with that person . And so it 's {disfmarker}
Professor E: i Yeah , so I would think , exploratory things now . Uh , three months from now {disfmarker} Um , I mean the transcriptions I think are a bit of an unknown cuz we haven't gotten those back yet as far as the timing , but I think as far as the collection , it doesn't seem to me l like , uh , unreasonable to say that uh in January , you know , ro roughly uh {disfmarker} which is roughly three months from now , we should have at least something like , you know , twenty - five , thirty hours .
PhD C: And we just don't know about the transcription part of that ,
Professor E: So that 's {disfmarker}
Postdoc B: Yeah , we need to {disfmarker} I think that there 's a possibility that the transcript will need to be adjusted afterwards ,
PhD C: so . I mean , it {disfmarker}
Postdoc B: and uh es especially since these people won't be uh used to dealing with multi - channel uh transcriptions .
PhD C: Right .
Professor E: Yeah .
Postdoc B: So I think that we 'll need to adjust some {disfmarker} And also if we wanna add things like um , well , more refined coding of overlaps , then definitely I think we should count on having an extra pass through . I wanted to ask another a a aspect of the data collection . There 'd be no reason why a person couldn't get together several uh , you know , friends , and come and argue about a topic if they wanted to , right ?
Professor E: If they really have something they wanna talk about as opposed to something @ @ {disfmarker} I mean , what we 're trying to stay away from was artificial constructions , but I think if it 's a real {disfmarker} Why not ? Yeah .
PhD C: I mean , I 'm thinking , politically {disfmarker}
Grad G: Stage some political debates .
Postdoc B: You could do this ,
PhD C: Well yeah ,
Postdoc B: you know . You could .
PhD C: or just if you 're {disfmarker} if you ha If there are meetings here that happen that we can record even if we don't {pause} um have them do the digits , {comment} or maybe have them do a shorter {pause} digit thing {comment} like if it was , you know , uh , one string of digits , or something , they 'd probably be willing to do .
Grad G: We don't have to do the digits at all if we don't want to .
PhD C: Then , having the data is very valuable , cuz I think it 's um politically better for us to say we have this many hours of audio data , especially with the ITR , if we put in a proposal on it . It 'll just look like ICSI 's collected a lot more audio data . Um , whether it 's transcribed or not um , is another issue , but there 's {disfmarker} there are research questions you can answer without the transcriptions , or at least that you can start to answer .
Postdoc B: It seems like you could hold some meetings .
Grad G: Yep .
Postdoc B: You know , you and maybe Adam ?
PhD C: So .
Postdoc B: You {disfmarker} you could {disfmarker} you could maybe hold some additional meetings , if you wanted .
PhD A: Would it help at all {disfmarker} I mean , we 're already talking about sort of two levels of detail in meetings . One is uh um without doing the digits {disfmarker} Or , I guess the full - blown one is where you do the digits , and everything , and then talk about doing it without digits , what if we had another level , just to collect data , which is without the headsets and we just did the table - mounted stuff .
PhD C: Need the close - talking mikes .
PhD A: You do , OK .
PhD C: I mean , absolutely ,
Professor E: Yeah . Yeah .
PhD C: yeah . I 'm really scared {disfmarker}
Grad G: It seems like it 's a big part of this corpus is to have the close - talking mikes .
PhD A: I see , OK .
PhD C: Um or at least , like , me personally ? I would {disfmarker} {comment} I {disfmarker} couldn't use that data .
Professor E: Yeah .
Postdoc B: I agree . And Mari also ,
PhD C: Um .
Postdoc B: we had {disfmarker} This came up when she she was here . That 's important .
PhD C: So it 's a great idea ,
Professor E: Yeah , I {disfmarker} I {disfmarker} b By the {disfmarker} by the way , I don't think the transcriptions are actually , in the long run , such a big bottleneck .
PhD C: and if it were true than I would just do that , but it 's not that bad {disfmarker} like the room is not the bottleneck , and we have enough time in the room , it 's getting the people to come in and put on the {disfmarker} and get the setup going .
Professor E: I think the issue is just that we 're {disfmarker} we 're blazing that path . Right ? And {disfmarker} and um {disfmarker} d Do you have any idea when {disfmarker} when uh the {disfmarker} you 'll be able to send uh the ten hours to them ?
Grad G: Well , I 've been burning two C Ds a day , which is about all I can do with the time I have .
Professor E: Yeah . Yeah .
Grad G: So it 'll be early next week .
Professor E: Yeah , OK . So early next week we send it to them , and then {disfmarker} then we check with them to see if they 've got it and we {disfmarker} we start , you know asking about the timing for it .
Grad G: Yep .
Professor E: So I think once they get it sorted out about how they 're gonna do it , which I think they 're pretty well along on , cuz they were able to read the files and so on .
Grad G: Yep .
Professor E: Right ?
Grad G: Yeah , but {disfmarker}
Professor E: Well {disfmarker}
Grad G: Yeah , who knows where they are .
PhD A: Have they ever responded to you ?
Grad G: Nope .
Professor E: Yeah , but {disfmarker} You know , so they {disfmarker} they {disfmarker} they have {disfmarker} you know , they 're volunteering their time and they have a lot of other things to do ,
PhD C: What if {disfmarker}
Grad G: Yeah , you {disfmarker} we can't complain .
Professor E: right ? But they {disfmarker} But at any rate , they 'll {disfmarker} I {disfmarker} I think once they get that sorted out , they 're {disfmarker} they 're making cassettes there , then they 're handing it to someone who they {disfmarker} who 's {disfmarker} who is doing it , and uh I think it 's not going to be {disfmarker} I don't think it 's going to be that much more of a deal for them to do thirty hours then to do one hour , I think . It 's not going to be thirty
Grad G: Yep . I think that 's probably true .
PhD C: Really ? So it 's the amount of {disfmarker}
Professor E: It 's {disfmarker} it 's just getting it going .
Grad G: It 's pipeline , pipeline issues .
PhD C: Right . What about these lunch meetings {disfmarker}
Grad G: Once the pipeline fills .
PhD C: I mean , I don't know , if there 's any way without too much more overhead , even if we don't ship it right away to IBM even if we just collect it here for awhile , {comment} to record you know , two or three more meeting a week , just to have the data , even if they 're um not doing the digits , but they do wear the headphones ?
Professor E: But the lunch meetings are pretty much one person getting up and {disfmarker}
PhD C: No , I meant , um , sorry , the meetings where people eat their lunch downstairs , maybe they don't wanna be recorded , but {disfmarker}
Grad G: Oh , and we 're just chatting ?
PhD C: Just the ch the chatting .
Grad G: Yeah , we have a lot of those .
PhD C: I actually {disfmarker} I actually think that 's {pause} useful {pause} data , um {pause} the chatting ,
Grad G: Yeah , the problem with that is I would {disfmarker} I think I would feel a little constrained to {disfmarker} You know ? Uh , some of the meetings {disfmarker}
PhD C: but {disfmarker} OK . You don't wanna do it , cuz {disfmarker} OK .
Grad G: You know , our " soccer ball " meeting ?
PhD C: Alright .
Grad G: I guess none of you were there for our soccer ball meeting .
PhD C: Alright , {comment} so I 'll just throw it out there , if anyone knows of one more m or two more wee meetings per week that happen at ICSI , um that we could record , I think it would be worth it .
Grad G: That was hilarious .
Professor E: Yeah . Well , we should also check with Mari again , because they {disfmarker} because they were really intending , you know , maybe just didn't happen , but they were really intending to be duplicating this in some level . So then that would double {pause} what we had . Uh . And there 's a lot of different meetings at UW uh {disfmarker} I mean really m a lot more {comment} than we have here right cuz we 're not right on campus ,
Grad G: Right .
Professor E: so .
PhD A: Is the uh , notion of recording any of Chuck 's meetings dead in the water , or is that still a possibility ?
Professor E: Uh , {vocalsound} they seem to have some problems with it . We can {disfmarker} we can talk about that later . Um , but , again , Jerry is {disfmarker} Jerry 's open {disfmarker} So I mean , we have two speech meetings , one uh network meeting , uh Jerry was open to it but I {disfmarker} I s One of the things that I think is a little {disfmarker} a little bit of a limitation , there is a think when the people are not involved uh in our work , we probably can't do it every week . You know ? I {disfmarker} I {disfmarker} I {disfmarker} I think that {disfmarker} that people are gonna feel uh {disfmarker} are gonna feel a little bit constrained . Now , it might get a little better if we don't have them do the digits all the time . And the {disfmarker} then {disfmarker} so then they can just really sort of try to {disfmarker} put the mikes on and then just charge in and {disfmarker}
Grad G: Yep .
PhD C: What if we give people {disfmarker} you know , we cater a lunch in exchange for them having their meeting here or something ?
Postdoc B: Well , you know , I {disfmarker} I do think eating while you 're doing a meeting is going to be increasing the noise .
PhD C: OK .
Postdoc B: But I had another question , which is um , you know , in principle , w um , I know that you don't want artificial topics ,
PhD C: Alright , alright , alright .
Postdoc B: but um it does seem to me that we might be able to get subjects from campus to come down and do something that wouldn't be too artificial . I mean , we could {disfmarker} political discussions , or {disfmarker} or something or other ,
PhD C: No , definitely .
Postdoc B: and i you know , people who are {disfmarker} Because , you know , there 's also this constraint . We d it 's like , you know , the {disfmarker} the {disfmarker} uh goldibears {disfmarker} goldi goldilocks , it 's like you don't want meetings that are too large , but you don't want meetings that are too small . And um {disfmarker} a and it just seems like maybe we could exploit the subj human subject p p pool , in the positive sense of the word .
PhD A: Well , even {disfmarker} I mean , coming down from campus is sort of a big thing , but what about
Postdoc B: We could pay subjects .
PhD A: or what about people in the {disfmarker} in the building ?
PhD C: Yeah , I was thinking , there 's all these other peo
PhD A: I mean , there 's the State of California downstairs , and {disfmarker}
PhD C: Yeah . I mean {disfmarker}
Grad G: I just really doubt that uh any of the State of California meetings would be recordable and then releasable to the general public .
Postdoc B: Yeah .
PhD A: Oh .
PhD C: Mm - hmm .
Grad G: So I {disfmarker} I mean I talked with some people at the Haas Business School who are i who are interested in speech recognition
PhD C: Alright , well .
Grad G: and , they sort of hummed and hawed and said " well maybe we could have meetings down here " , but then I got email from them that said " no , we decided we 're not really interested and we don't wanna come down and hold meetings . " So , I think it 's gonna be a problem to get people regularly .
PhD A: What about Joachim , maybe he can {disfmarker}
Professor E: But {disfmarker} but we c But I think , you know , we get some scattered things from this and that . And I {disfmarker} I d I do think that maybe we can get somewhere with the {disfmarker} with the radio .
PhD C: Mm - hmm .
Professor E: Uh i I have better contacts in radio than in television , but {disfmarker}
PhD A: You could get a lot of lively discussions from those radio ones .
PhD C: Well , and they 're already {disfmarker} they 're {disfmarker} these things are already recorded ,
Grad G: Yep .
Professor E: Yeah .
PhD C: we don't have to ask them to {disfmarker} even {disfmarker} and I 'm not sure wh how they record it , but they must record from individual {disfmarker}
Professor E: n Well {disfmarker} No , I 'm not talking about ones that are already recorded . I 'm talking about new ones
PhD C: Why {disfmarker} why not ?
Professor E: because {disfmarker} because {disfmarker} because we would be asking them to do something different .
PhD C: Well , we can find out . I know for instance Mark Liberman was interested uh in {disfmarker} in LDC getting {pause} data , uh , and {disfmarker}
Professor E: Right , that 's the found data idea .
PhD C: Yeah .
Professor E: But what I 'm saying is uh if I talk to people that I know who do these th who produce these things we could ask them if they could record an extra channel , let 's say , of a distant mike .
PhD C: Mm - hmm .
Professor E: And u I think routinely they would not do this . So , since I 'm interested in the distant mike stuff , I wanna make sure that there is at least that somewhere
PhD C: Right . Great . OK .  
Professor E: and uh {disfmarker} But if we ask them to do that they might be intrigued enough by the idea that they uh might be e e willing to {disfmarker} the {disfmarker} I might be able to talk them into it .
PhD C: Mm - hmm .
Grad G: Um . We 're getting towards the end of our disk space , so we should think about trying to wrap up here .
PhD C: That 's a good way to end a meeting .
Professor E: OK . Well I don't {disfmarker} why don't we {disfmarker} why d u why don't we uh uh turn them {disfmarker} turn
Grad G: OK , leave {disfmarker} leave them on for a moment until I turn this off , cuz that 's when it crashed last time .
Postdoc B: Oh . That 's good to know .
Professor E: Turning off the microphone made it crash . Well {disfmarker}
Postdoc B: That 's good to know .
Professor E: OK .
2022-06-15 06:44:33 | INFO | __main__ | output #1: Efforts by speaker mn005 are in progress to detect overlapping speech. For a single transcribed meeting, speaker mn005 reported approximately 300 cases of overlap. Future work will involve manually deriving time marks from sections of overlapping speech for the same meeting, and then experimenting with different measures, e.g. energy increase, to determine a set of acoustically salient features for identifying speaker overlap. 
2022-06-15 06:44:33 | INFO | __main__ | 
Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:05<00:00,  5.25s/ba]Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:05<00:00,  5.25s/ba]
2022-06-15 06:44:38 | INFO | datasets.arrow_writer | Done writing 272 examples in 3878618 bytes .
2022-06-15 06:44:38 | INFO | __main__ | With --preprocess_only, exiting after preprocess_on the data
/disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/allenai-led-base-16384_global_1024_16_1e-3_4096_scrolls_qmsum_coffee-second-33
src/run.py --dataset_config_name qmsum --dataset_name tau/scrolls --do_eval True --do_train True --drop_duplicates_in_eval False --evaluation_strategy epoch --greater_is_better False --metric_for_best_model loss --model_name_or_path allenai/led-base-16384 --num_train_epochs 20 --predict_with_generate False --save_strategy epoch --adam_epsilon 1e-6 --adam_beta1 0.9 --adam_beta2 0.98 --weight_decay 0.001 --logging_steps 10 --gradient_checkpointing true --save_total_limit 2 --preprocessing_num_workers 1 --group_by_length true --load_best_model_at_end True --lr_scheduler linear --warmup_ratio 0.1 --prediction_loss_only True --attention_window 1024 --max_target_length 1024 --fp16 True --train_max_tokens 4096 --gradient_accumulation_steps 16 --per_device_eval_batch_size 2 --preprocess_only --learning_rate 1e-3 --global_attention_first_token True --folder_suffix global_attention_first_token$max_source_length$gradient_accumulation_steps$learning_rate$train_max_tokens --max_source_length 1024 --output_dir /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/allenai-led-base-16384_global_1024_16_1e-3_4096_scrolls_qmsum_coffee-second-33 --run_name allenai-led-base-16384_global_1024_16_1e-3_4096_scrolls_qmsum_coffee-second-33
Downloading and preparing dataset scrolls/qmsum (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac...
Dataset scrolls downloaded and prepared to experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac. Subsequent calls will reuse this data.
[INFO] 2022-06-15 06:44:42,242 run: Running torch.distributed.run with args: ['torch/distributed/run.py', '--nproc_per_node=2', '--master_port=41322', 'src/run.py', '--dataset_config_name', 'qmsum', '--dataset_name', 'tau/scrolls', '--do_eval', 'True', '--do_train', 'True', '--drop_duplicates_in_eval', 'False', '--evaluation_strategy', 'epoch', '--greater_is_better', 'False', '--metric_for_best_model', 'loss', '--model_name_or_path', 'allenai/led-base-16384', '--num_train_epochs', '20', '--predict_with_generate', 'False', '--save_strategy', 'epoch', '--adam_epsilon', '1e-6', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--weight_decay', '0.001', '--logging_steps', '10', '--gradient_checkpointing', 'true', '--save_total_limit', '2', '--preprocessing_num_workers', '1', '--group_by_length', 'true', '--load_best_model_at_end', 'True', '--lr_scheduler', 'linear', '--warmup_ratio', '0.1', '--prediction_loss_only', 'True', '--attention_window', '1024', '--max_target_length', '1024', '--fp16', 'True', '--train_max_tokens', '4096', '--gradient_accumulation_steps', '16', '--per_device_eval_batch_size', '2', '--learning_rate', '5e-05', '--global_attention_first_token', 'True', '--folder_suffix', 'global_attention_first_token$max_source_length$gradient_accumulation_steps$learning_rate$train_max_tokens', '--max_source_length', '1024', '--output_dir', '/disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/allenai-led-base-16384_global_1024_16_5e-05_4096_scrolls_qmsum_supermarket-shame-34', '--run_name', 'allenai-led-base-16384_global_1024_16_5e-05_4096_scrolls_qmsum_supermarket-shame-34', '--output_dir=experiments/output/qmsum_led-1024']
[INFO] 2022-06-15 06:44:42,243 run: Using nproc_per_node=2.
[INFO] 2022-06-15 06:44:42,243 api: Starting elastic_operator with launch configs:
  entrypoint       : src/run.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:41322
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

[INFO] 2022-06-15 06:44:42,245 local_elastic_agent: log directory set to: /tmp/torchelastic_p0qjj1zx/none_m5ouns89
[INFO] 2022-06-15 06:44:42,246 api: [default] starting workers for entrypoint: python
[INFO] 2022-06-15 06:44:42,246 api: [default] Rendezvous'ing worker group
[INFO] 2022-06-15 06:44:42,246 static_tcp_rendezvous: Creating TCPStore as the c10d::Store implementation
/home/s1970716/miniconda3/envs/scrolls_venv/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
[INFO] 2022-06-15 06:44:42,249 api: [default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=41322
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

[INFO] 2022-06-15 06:44:42,249 api: [default] Starting worker group
[INFO] 2022-06-15 06:44:42,250 __init__: Setting worker0 reply file to: /tmp/torchelastic_p0qjj1zx/none_m5ouns89/attempt_0/0/error.json
[INFO] 2022-06-15 06:44:42,250 __init__: Setting worker1 reply file to: /tmp/torchelastic_p0qjj1zx/none_m5ouns89/attempt_0/1/error.json
2022-06-15 06:44:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-06-15 06:44:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-06-15 06:44:49 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-15 06:44:49 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for 2 nodes.
2022-06-15 06:44:49 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-15 06:44:49 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for 2 nodes.
Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2022-06-15 06:44:49 | WARNING | __main__ | Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
2022-06-15 06:44:49 | WARNING | __main__ | Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
2022-06-15 06:44:49 | INFO | __main__ | Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.98,
adam_epsilon=1e-06,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fp16_padding=False,
gradient_accumulation_steps=16,
greater_is_better=False,
group_by_length=True,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=experiments/output/qmsum_led-1024/runs/Jun15_06-44-49_arnold.inf.ed.ac.uk,
logging_first_step=False,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=20.0,
output_dir=experiments/output/qmsum_led-1024,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=8,
per_device_train_max_batch_size=None,
predict_with_generate=False,
prediction_loss_only=True,
push_to_hub=False,
push_to_hub_model_id=qmsum_led-1024,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=allenai-led-base-16384_global_1024_16_5e-05_4096_scrolls_qmsum_supermarket-shame-34,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_max_tokens=4096,
use_legacy_prediction_loop=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.001,
)
2022-06-15 06:44:50 | WARNING | datasets.builder | Reusing dataset scrolls (experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)
2022-06-15 06:44:51 | INFO | datasets.load | Checking experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py for additional imports.
2022-06-15 06:44:51 | INFO | datasets.utils.filelock | Lock 139662155513520 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py.lock
2022-06-15 06:44:51 | INFO | datasets.load | Found main folder for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls
2022-06-15 06:44:51 | INFO | datasets.load | Found specific version folder for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
2022-06-15 06:44:51 | INFO | datasets.load | Found script file from https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py to experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac/scrolls.py
2022-06-15 06:44:51 | INFO | datasets.load | Couldn't find dataset infos file at https://huggingface.co/datasets/tau/scrolls/resolve/main/dataset_infos.json
2022-06-15 06:44:51 | INFO | datasets.load | Found metadata file for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac/scrolls.json
2022-06-15 06:44:51 | INFO | datasets.utils.filelock | Lock 139662155513520 released on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py.lock
2022-06-15 06:44:51 | INFO | datasets.utils.filelock | Lock 139662155510064 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 06:44:51 | INFO | datasets.builder | Overwrite dataset info from restored data version.
2022-06-15 06:44:51 | INFO | datasets.info | Loading Dataset info from experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
2022-06-15 06:44:51 | INFO | datasets.utils.filelock | Lock 139662155510064 released on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 06:44:51 | INFO | datasets.utils.filelock | Lock 139662155510640 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 06:44:51 | WARNING | datasets.builder | Reusing dataset scrolls (experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)
2022-06-15 06:44:51 | INFO | datasets.info | Loading Dataset info from experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
2022-06-15 06:44:51 | INFO | datasets.utils.filelock | Lock 139662155510640 released on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 06:44:51 | INFO | datasets.builder | Constructing Dataset for split train, validation, test, from experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 45.34it/s]
[INFO|configuration_utils.py:561] 2022-06-15 06:44:52,312 >> loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:598] 2022-06-15 06:44:52,317 >> Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": 1024,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": true,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:561] 2022-06-15 06:44:53,143 >> loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:598] 2022-06-15 06:44:53,144 >> Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": [
    1024,
    1024,
    1024,
    1024,
    1024,
    1024
  ],
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:44:56,072 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/vocab.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/4fb25bb1f9a942a2e2930029211b4a7deaeb18b62f6e5ce6d59730c90da51373.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:44:56,073 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/merges.txt from cache at experiments/data/qmsum_led-1024/huggingface/transformers/087e8f4306cbf22e21907929074344a3b0a46bd680a118eb6267cd5a2bcec5b2.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:44:56,073 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:44:56,073 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:44:56,073 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/special_tokens_map.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/05da652a7fca41c1c18027c1201e473217bb373e370d1283e3de49d5880cbf0c.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0
[INFO|tokenization_utils_base.py:1739] 2022-06-15 06:44:56,073 >> loading file https://huggingface.co/allenai/led-base-16384/resolve/main/tokenizer_config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/86288ba22bce9550d76e9b26722ee92ae5921ae9285ccbc2904e9a5ad7199b73.cfc08f03f72cde495bd6b3dd3252bca130b3437de370856d084d1453c58b6fea
[INFO|configuration_utils.py:561] 2022-06-15 06:44:56,479 >> loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:598] 2022-06-15 06:44:56,480 >> Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": [
    1024,
    1024,
    1024,
    1024,
    1024,
    1024
  ],
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:561] 2022-06-15 06:44:57,029 >> loading configuration file https://huggingface.co/allenai/led-base-16384/resolve/main/config.json from cache at experiments/data/qmsum_led-1024/huggingface/transformers/ec844bead6f5bbcd6ac727b57e595c2ba40b0970f91cb923423773f72fe1702f.898baac75d55d484b1b1de95b8ab791987c78591acf36ce6131b56d0d2d26af7
[INFO|configuration_utils.py:598] 2022-06-15 06:44:57,031 >> Model config LEDConfig {
  "_name_or_path": "./",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": [
    1024,
    1024,
    1024,
    1024,
    1024,
    1024
  ],
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1279] 2022-06-15 06:44:57,619 >> loading weights file https://huggingface.co/allenai/led-base-16384/resolve/main/pytorch_model.bin from cache at experiments/data/qmsum_led-1024/huggingface/transformers/c8f7e4603efbc329ce921b34057d78880dead50f45b2a1648b3a06ca6eb17f51.201222b06d46289037a8dccc57548abc8eb81ba042d3762214ac15c9691ff8c7
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[INFO|modeling_utils.py:1524] 2022-06-15 06:45:01,006 >> All model checkpoint weights were used when initializing LEDForConditionalGeneration.

[INFO|modeling_utils.py:1532] 2022-06-15 06:45:01,006 >> All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at allenai/led-base-16384.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.
2022-06-15 06:45:01 | INFO | __main__ | 
2022-06-15 06:45:01 | INFO | __main__ | Training examples before tokenization:
2022-06-15 06:45:01 | INFO | __main__ | input #0: How Did Project Manager and User Interface introduce the prototype of the remote control?

Project Manager: Yep . Soon as I get this . Okay . This is our last meeting . Um I'll go ahead and go through the minutes from the previous meeting . Uh and then we'll have a , the prototype presentation . {vocalsound} Um then we will um do an evaluation . Uh or we'll see what , what we need to have under the criteria for the evaluation . Then we'll go through the finance and see if we fall within the budget . Um then we'll do the evaluation , and then we can finish up after that with um any changes that we'll need to make , or hopefully everything will fall right in line . Um let's see , minutes from the last meeting . Um we looked at uh the the trends . We had uh the fashion trends that people want a fancy look-and-feel . It was twice as important as anything else . Um they liked fruit and vegetables in the new styles . Um and a spongy feel . So we were talking about trying to incorporate those into our prototype . Um they wanted limited buttons and simplicity . Um then we looked at the uh the method for coming up with our own remote . Um looking at other other devices . Um the iPod , we really liked the look of that . Um we also had uh the kid's remote for a simple idea . Um a two part remote , which was what were were originally looking at . Uh and then um there was talk of spee uh speech recognition um becoming more uh predominant and easier to use . But I think we've still decided not to go with that . {vocalsound} Then we looked at the components um the materials for the case , the different energy sources , the different types of chips , um and made a decision on what we were going to use to make our remote . Um and basically how , what were making for the prototype . So I'm going to leave it at that and let you guys take over .
User Interface: The prototype discussion .
Project Manager: The prototype yeah . Do you need a {disfmarker} this ?
User Interface: No . {vocalsound}
Project Manager: Okay .
Industrial Designer: {vocalsound} Can try to plug that in there
User Interface: There is our remo {gap} the banana .
Marketing: {vocalsound}
Industrial Designer: but {disfmarker}
User Interface: Um {vocalsound} yeah basically we we st went with the colour yellow . Um working on the principle of a fruit which was mentioned , it's basically designed around a banana .
Project Manager: {vocalsound}
User Interface: Um but it would be held in such a fashion ,
Marketing: {vocalsound}
User Interface: where it is , obviously it wouldn't be that floppy 'cause this would be hard plastic . These would be like the rubber , the rubber grips . So that's so that would hopefully help with grip , or like the ergonomics of it . Um but all the controlling would be done with this scroll wheel . You have to use your imagination a little bit . And this here represents the screen , where you , where you'd go through .
Project Manager: Very nice .
User Interface: And the the simplest functions would be um almost identical to an iPod , where that one way ch through channels , that way th other way through channels . Volume up and down . And then to access the more complicated functions you'd you sorta go , you press that and go through the menus . It's that that simple . That just represents the infrared uh beam . That's a simple on and off switch . Um I don't know , we could use the voice . T that blue bits should be yellow , that that'd be where the batteries would be I suppose . And um {vocalsound} that's about it . It's as simple as you , we could make it really .
Industrial Designer: Right .
User Interface: Is there anything you want to add ?
Industrial Designer: That's what we have there . That's plastic . Plastic covered with rubber . We might uh add some more underneath here . Maybe give it , give it a form . I mean you're supposed to hold it like that , but um just if you grab it , take it from somewhere ,
User Interface: Yeah .
Project Manager: Mm-hmm .
Industrial Designer: so {disfmarker} yeah ,
User Interface: Doesn't make much make much difference .
Industrial Designer: you have some rub yeah .
User Interface: You could work left-handed or right-handed I suppose .
Industrial Designer: Exactly , {gap} use both . Might as well think about {disfmarker}
User Interface: T the actual thing might be smaller .
Industrial Designer: Th think about the button as well . Like either put either one {gap} one on either side or
User Interface: {vocalsound} Yeah .
Project Manager: What but what's that button ?
Industrial Designer: not do it at all . It's a quick on-off button .
User Interface: Just the on and off .
Project Manager: Uh , 'kay .
Industrial Designer: That's um
Marketing: {vocalsound}
Industrial Designer: yeah I think it's pretty important . So you don't have to fiddle with that .
Project Manager: 'Kay .
Industrial Designer: Right ? Um that's not um {disfmarker}
Project Manager: {vocalsound}
Industrial Designer: I'd say a bit smaller would probably be nice . You wanna play with that over there .
User Interface: Yeah .
Industrial Designer: There you go .
User Interface: It's you know it's flimsy 'cause it's made out of heavy Play-Doh ,
Marketing: {vocalsound}
Project Manager: Would you like to uh {disfmarker}
Industrial Designer: Right .
User Interface: but {disfmarker}
Marketing: Pretty impressive .
Project Manager: Well done .
User Interface: {vocalsound}
Marketing: {vocalsound} Kind of a banana .
User Interface: And whether or not it would fall into the cost {gap} everything I suppose . With the scroll and the L_C_D_ .
Project Manager: Well luckily we are going to find out . Or not luckily . Um do you have a marketing presentation for us .
Industrial Designer: {vocalsound}
Marketing: {vocalsound} I do . Okay . You guys are gonna help me do an evaluation of the criteria . Um . Okay . So first I'll just discuss some of the criteria that I found . Just based on the past trend reports that I was looking at earlier . And then we'll do a group evaluation of the prototype . And then we will calculate the average score to see how we did . Um so the criteria we're gonna be looking at are the complaints um that we heard from the users who were interviewed earlier . So we're gonna be doing it based on a seven point scale . And one is going to mean true , that we did actually achieve that . With seven being false , we did not achieve that . {gap} . Okay . So for the first one , we need to decide , did we solved the problem of the users who complained about an ugly remote ? {vocalsound}
Industrial Designer: {vocalsound} {vocalsound} .
User Interface: {vocalsound}
Project Manager: I think it's definitely different than anything else out there .
User Interface: {vocalsound}
Marketing: Mm .
User Interface: Yeah .
Project Manager: So if they think that what is out there is ugly , then yes I would say , I would say most definitely .
Marketing: {vocalsound}
User Interface: I would {gap} .
Project Manager: It's bright .
User Interface: It's bright . It's {disfmarker}
Project Manager: It still has your traditional black .
User Interface: It's curved . It's not {disfmarker} there's no sharp
Industrial Designer: {vocalsound}
User Interface: angles to it .
Project Manager: Yep , not angular .
Marketing: Mm .
Industrial Designer: I'd say , when it comes to the ergonomics , the form and stuff , yes that's definitely more beautiful than your average .
Marketing: {vocalsound}
Industrial Designer: However the colour , we don't have a say in that .
Marketing: Yeah I think the colours detract a little bit . {vocalsound}
User Interface: Some people might say it . Yeah .
Industrial Designer: That has been , that has been dictated pretty much by the company .
Project Manager: Mm .
Industrial Designer: So uh to answer that honestly I would rather say like uh , we have not solved the problem completely with the ugly remote because the colour is ugly , definitely .
Project Manager: {vocalsound} Yep .
Marketing: That's true . Yeah .
Project Manager: {vocalsound}
User Interface: Yeah .
Industrial Designer: 'S nothing you can say about that . I mean I much prefer something like brushed chrome with that form .
User Interface: Yeah .
Industrial Designer: But {disfmarker}
Project Manager: Yeah something more modern to go {disfmarker} a a modern colour to go with the modern form .
Industrial Designer: Right . Right . It's different . You don't want your uh three feet huge L_C_D_ dis display in your living room that's hanging from the wall to be controlled with something like that .
Marketing: Um okay so , do you think , since we {disfmarker} This was a a sign criteria , do you think maybe we should put it somewhere in the middle then ?
Industrial Designer: Yeah .
Marketing: Does that sound good ?
Project Manager: Yeah .
User Interface: Yeah .
Industrial Designer: {vocalsound}
Marketing: What do you think ? Three ? Four ?
Project Manager: I would say
Marketing: Five ?
Project Manager: four . {vocalsound}
Industrial Designer: Yeah . {vocalsound}
Marketing: {vocalsound} Four is fair . Okay .
Project Manager: Very non-committal , four .
Marketing: Okay , the second one . Did we make it simple for new users ?
Industrial Designer: It's very intuitive , I think yeah .
User Interface: Yeah . I think that was the main aim , one of the main aims that we had .
Industrial Designer: {vocalsound} S give it a one .
Marketing: One ,
Project Manager: Yeah .
Marketing: 'kay . Okay . Um , do the controls now match the operating behaviour of the users ?
User Interface: Uh yeah . 'Cause we've we've brought it down to basically four controls {gap} most common , which are channel and volume .
Marketing: I'd say that {disfmarker}
Project Manager: Mm-hmm .
Industrial Designer: Right .
User Interface: And then the other ones are just a matter of just going , just scrolling further .
Project Manager: S scrolling through and selecting a few .
Industrial Designer: Right . So that's a one .
Marketing: So one ?
Project Manager: I think that's a one .
Marketing: Yeah ? {vocalsound} Okay . Okay um the fourth one . How about the problem of a remote being easily lost ? One of the number one complaints .
Industrial Designer: Something that big and that yellow you just don't lose anymore .
Project Manager: {vocalsound}
User Interface: {vocalsound} Yeah .
Marketing: {vocalsound} Whether you want to or not , you're not gonna lose it . {vocalsound}
User Interface: It's bright yellow .
Industrial Designer: {vocalsound}
User Interface: Bright yellow's hard to lose . But um if we were to , if we were , that , the speech recognition . That , we could maybe just use that solely for the the finding thing . That was what we'd we'd mentioned .
Project Manager: So if we incorporate speech recognition into it then it could {disfmarker}
User Interface: Just just to use , to find it when it was lost . But like I said , like I don't think you'd lose something so yellow so easily .
Industrial Designer: Oops . Hmm .
User Interface: And it's not gonna fall , like a rectangle would slip down behind things . That's gonna be a difficult shape to {disfmarker}
Industrial Designer: Well what {disfmarker}
Project Manager: And it is quite bright and {disfmarker}
User Interface: Yeah .
Marketing: {vocalsound}
User Interface: Maybe in the middle again , three or four or something ?
Project Manager: Uh {disfmarker}
Industrial Designer: S
Marketing: Okay .
User Interface: I mean you know {gap} loo losing things is one of those things that people can lose , I mean a million ways .
Project Manager: Yeah .
User Interface: You can pick it up and walk away with it and then you've lost it .
Industrial Designer: Mm .
Marketing: That's true .
Project Manager: But if we do go with the , with the speech recognition , then it , then our scale goes up quite a bit I think .
Marketing: Mm .
Industrial Designer: Oh yeah . You probably {disfmarker}
User Interface: Yeah .
Project Manager: Probably two . You know . If we eliminate the fact that you know it's impossible to guarantee that it's not gonna be lost then
User Interface: Yeah .
Industrial Designer: Mm .
Project Manager: I'd say two .
Industrial Designer: {vocalsound}
Marketing: {vocalsound}
Project Manager: With the speech recognition , which of course may be changed depending on budget .
User Interface: Yeah .
Industrial Designer: Y you could add an extra feature actually . Which makes this thing raise hell when you remove it too far from the television .
User Interface: Yeah .
Industrial Designer: We could add that but that's nothing we have thought of so far .
Project Manager: Which , which may be cheaper than speech recognition if it were just a {disfmarker}
Industrial Designer: Yes .
User Interface: Yeah true . But I mean d just those whistling , clapping key rings you have . They're cheap .
Marketing: Annoying alarm or something ?
Project Manager: {vocalsound}
Industrial Designer: It's it's {disfmarker}
Marketing: Yeah .
User Interface: So it can't be that
Industrial Designer: Um the {disfmarker} it's based on this anti anti-theft technology for suitcases and stuff ,
User Interface: expensive .
Project Manager: Some sort of proximity {vocalsound} {disfmarker}
User Interface: Yeah .
Industrial Designer: where you have one piece that's attached to your luggage , another piece that starts beeping . That can't cost much .
User Interface: Yeah .
Industrial Designer: So that can also easily be integrated because these things are small enough to to hide , so you have one piece , you have to glue somewhere behind your {disfmarker} stick it behind your T_V_ and the other {disfmarker}
User Interface: {gap} stick it on the T_V_ {gap} .
Project Manager: {vocalsound} Pray that you don't accidentally lose that piece . {vocalsound}
Industrial Designer: Right .
User Interface: {vocalsound}
Marketing: {vocalsound}
Industrial Designer: That'd be tough then . {vocalsound} Well also your remote would uh alarm you if somebody stole you t your television , yeah . Ran off with it without taking the beautiful remote control .
Project Manager: {vocalsound}
User Interface: Yeah . {vocalsound}
Marketing: So . Are we adding one of these two features ?
Industrial Designer: Let's add one of those features and say yes . {vocalsound}
Marketing: {gap} gonna say {disfmarker} okay .
Project Manager: Okay .
Marketing: So we're {vocalsound} back to a one ?
User Interface: Two .
Marketing: Or a two ?
Project Manager: Two .
Industrial Designer: Two .
Marketing: Two , 'kay . Okay . Are we technologically innovative ?
Industrial Designer: Uh {disfmarker}
User Interface: {vocalsound} I'd say so .
Industrial Designer: {vocalsound}
User Interface: Uh don't get many mo remote controls with
Industrial Designer: It's all just {disfmarker}
User Interface: screens on .
Industrial Designer: It's all just stolen technology when it comes down to {disfmarker} {vocalsound} {vocalsound}
User Interface: Yeah it's stolen technology .
Marketing: From iPod yeah . {vocalsound}
Project Manager: It's {disfmarker} {vocalsound}
User Interface: But we have {gap} .
Project Manager: But there's not a lot of yellow , there's not a lotta yellow .
Industrial Designer: right
Marketing: But for remotes {disfmarker} yeah .
Project Manager: Course that wasn't really {disfmarker}
Industrial Designer: right
User Interface: Fa
Industrial Designer: right right .
Project Manager: we were kinda forced to take that colour .
Marketing: Two ? Three ?
User Interface: {gap} 'cause it's stolen .
Project Manager: I don't know that we are that innovative , to tell you the truth . {vocalsound}
User Interface: No maybe not .
Industrial Designer: Yeah not really .
Marketing: But how many remotes do you see like this ?
User Interface: {vocalsound}
Project Manager: If we added the screaming factor {vocalsound} then we go up .
Industrial Designer: {vocalsound}
Marketing: Not so many .
Industrial Designer: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: {vocalsound} Um I would say we're probably at four .
Industrial Designer: Right .
Marketing: Really ? Okay . {vocalsound} That's gonna hurt us .
User Interface: {vocalsound}
Marketing: Okay . Um spongy material ?
Industrial Designer: Yeah well you have that , kind of , sort of .
Project Manager: We have some spongy , yeah .
User Interface: Yeah as much as as needed , I think .
Marketing: 'Kay .
Industrial Designer: It's not a one though .
Project Manager: No .
Industrial Designer: One would be the whole thing
Project Manager: Yeah . Because it's only got what , these parts are the grips and perhaps the back side {disfmarker} the bottom {disfmarker} the underneath on the back .
Industrial Designer: to fold and stuff . Yeah .
User Interface: Yeah .
Industrial Designer: So that's a four at most .
Project Manager: Probably a four at most . Possibly even a five .
Marketing: And lastly , did we put the fashion in electronics ?
Project Manager: {vocalsound}
Industrial Designer: Y yes .
User Interface: Yeah .
Marketing: I'd say we did .
Project Manager: If your fashion is b is Carmen Miranda , you betcha . {vocalsound}
Industrial Designer: More {disfmarker} {vocalsound}
User Interface: Yeah . {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound} Well the recent fashion is rather displayed in the in the L_C_D_ and the way you operate it than the form and the colour ,
User Interface: On the {disfmarker}
Project Manager: It's true .
User Interface: Yeah .
Industrial Designer: but it definitely is {disfmarker}
User Interface: Be what we were told , and they'd say yeah , definitely .
Industrial Designer: {gap} .
Marketing: 'Kay . Alright . Now we just gotta calculate . Six eight twelve sixteen . Seventeen divided by s
User Interface: {gap} .
Project Manager: Seven is {disfmarker}
Marketing: Eight .
Project Manager: Two point {disfmarker}
Marketing: {vocalsound}
Project Manager: {gap} two point four ?
User Interface: Is that some long division ? No .
Project Manager: Something .
Marketing: Well I haven't done math in years .
Industrial Designer: {vocalsound}
Marketing: What two {disfmarker}
User Interface: {vocalsound}
Marketing: {vocalsound} I dunno .
User Interface: Just , I'm sure there's a {gap} .
Marketing: Okay we'll say two point four two . Right ? How does that look ?
Industrial Designer: I'm impressed . I can't do that without a calculator . {vocalsound}
User Interface: No I can't do long {gap}
Marketing: {vocalsound} It's been a while .
User Interface: very impressive .
Project Manager: And what what is the acceptable criteria ? Is there like a scale that we have to hit ?
Marketing: Oh no . They just told me to
Industrial Designer: {vocalsound}
Marketing: {vocalsound} pick my own criteria and have you guys evaluate it {vocalsound} basically .
Project Manager: {vocalsound} Alright then .
Marketing: So that's that .
Project Manager: Okay . Well , let's see .
Marketing: {vocalsound}
Project Manager: Now we get to do the budget numbers . You didn't know that you were gonna have a budget . But we do . Okay .
User Interface: Yeah . Yeah so . You'd been going a long time dividing that . It's two point four two eight five se it just keeps going on .
Marketing: Oh my god .
User Interface: Two point four two basically .
Marketing: Okay . Yeah we'll go with that .
Project Manager: So I have here an {disfmarker}
Industrial Designer: {vocalsound} Fifty percent , you're kidding .
Marketing: Not too shabby .
Project Manager: Yeah .
Industrial Designer: {vocalsound} P
Project Manager: We want a fifty percent profit on this . Oh you can't really see that very well .
User Interface: {vocalsound} Charge about three hundred quid for it .
Project Manager: {vocalsound} Twelve and a half Euros is what supposed to cost us . Okay , so {disfmarker}
Industrial Designer: It's too much .
Project Manager: Well let's see .
Industrial Designer: Um {disfmarker}
Project Manager: The f the {disfmarker} Wonder if I can make this {disfmarker}
Industrial Designer: Uh {disfmarker}
Project Manager: What the {disfmarker} {vocalsound} Oh it won't let me do that . Okay . Alright so at top , I don't know if you guys can read that or not . I can't 'cause I don't have my glasses on ,
Industrial Designer: {vocalsound}
Project Manager: but so we've got the energy source . There's uh four , five , six categories .
Industrial Designer: Battery .
Project Manager: We have energy source , electronics , case . Then we have case material supplements , interface type , and then button supplements . Okay so {disfmarker} Uh first of all energy source , we picked battery . Um and how many batteries do we think this will probably take ?
User Interface: {vocalsound}
Project Manager: Probably some e either two or four .
Industrial Designer: Two .
Project Manager: Two ? {vocalsound} Like it . {vocalsound}
Industrial Designer: At four it's gonna be too heavy , so that that's not our problem . People can change it every month .
Project Manager: {vocalsound} Excellent .
Industrial Designer: {vocalsound} They won't know until after they bought it .
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: This is consumerism .
Industrial Designer: {vocalsound}
Project Manager: Alright so for the electronics our choices are simpl simple chip-on-print , regular chip-on-print , advanced chip-on-print , sample sensor , sample speaker .
Industrial Designer: {gap} .
User Interface: We're advanced chip are we ?
Industrial Designer: That's the advanced chip-on-print , yeah .
Project Manager: 'Kay , {gap} we have one of those . 'Kay then the case is a {disfmarker} Probably it's double curved .
Industrial Designer: Double curved , yes .
Project Manager: Case materials are
Industrial Designer: Plastic .
Project Manager: plastic . Um I guess it's two , since one for the top , one for the bottom .
Industrial Designer: N no .
Project Manager: Is that right or is it just one ?
Industrial Designer: No that's just one .
Project Manager: Maybe it's one because of the {disfmarker}
Industrial Designer: It's just one mo single mould , we can do that .
Project Manager: 'Kay .
User Interface: Yeah {gap} yeah .
Marketing: Right . {vocalsound}
Project Manager: I guess it doesn't matter 'cause the price on that one is zero , which is nice .
Industrial Designer: Exactly , right .
Marketing: Oh .
Project Manager: Special colour ?
Industrial Designer: That's not a special colour . It's a specially ugly colour , but it's not special .
Marketing: Bright yellow .
Project Manager: {vocalsound} Interface type . We have pushbutton , scroll-wheel interface , integrated scroll-wheel pushbutton , and an L_C_D_ display .
User Interface: {vocalsound}
Marketing: {vocalsound}
User Interface: S
Industrial Designer: S {vocalsound}
User Interface: That's {disfmarker} Yeah .
Project Manager: So we actually have the L_C_D_ display
Marketing: {vocalsound}
User Interface: And then {disfmarker}
Project Manager: and then is it the integrated or is it {disfmarker}
User Interface: I'd say the integrated .
Project Manager: Yeah .
Industrial Designer: Yes unfortunately .
Project Manager: 'Kay . Button supplement ? Special colour ?
User Interface: Mm .
Project Manager: Um special form ? Special material .
Industrial Designer: We could of course make the buttons wood .
Project Manager: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound} Say mahogany or so
Marketing: {vocalsound} It'd look really lovely .
Project Manager: Or titanium .
Industrial Designer: Mm-hmm or titanium .
Project Manager: They cost us all the same .
Marketing: {vocalsound} Yeah .
User Interface: {gap} remote control {gap} .
Project Manager: Well we only have one button so really we shouldn't be charged ,
Industrial Designer: Uh just {disfmarker}
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
Project Manager: {vocalsound} we shouldn't be charged anything for the the button supplements .
User Interface: No that's getting a bit tiny .
Project Manager: Um {disfmarker}
User Interface: Yeah .
Marketing: {vocalsound}
User Interface: I'd ignore that .
Marketing: Leave it blank .
Project Manager: Okay . We're gonna leave that one blank because we run on a L_C_D_ and scroll . So our total is fifteen point five . Which I believe is
Industrial Designer: Yeah that's too much .
Project Manager: by three Euros over .
Industrial Designer: It's hard to believe . So we'll go for the hand dynamo huh ? {vocalsound}
Project Manager: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: So the only thing better than um a banana-shaped remote is one that you shake .
User Interface: If it w What if we completely took out the the one single button we've got on .
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
User Interface: And just had a scroll wheel interface . And the L_C_D_ display . I suppose the L_C_D_ C_ display's the one that's pushing it up a bit though .
Project Manager: Yeah 'cause the {disfmarker}
Marketing: {vocalsound}
Project Manager: Well 'cause we have to have both right ?
User Interface: Yeah .
Industrial Designer: I mean let's let's face it , it also depends on the software on the on the television .
User Interface: Yeah .
Industrial Designer: You can have the the information that this thing transmits be being displayed on the on the screen .
Project Manager: Mm-hmm .
Industrial Designer: So s yeah let's take away the {disfmarker}
User Interface: Yeah you could maybe take out the L_C_D_ dis display even ,
Industrial Designer: Yeah . Yeah .
User Interface: if it if it comes up on the computer itsel on the T_V_ itself .
Industrial Designer: Right .
Project Manager: So we may not need the L_C_D_ display ?
User Interface: Uh that is possible yeah .
Industrial Designer: Right . We may not need it . There you go .
Project Manager: Well there we go .
Industrial Designer: Perfect .
Project Manager: Twelve point five .
User Interface: There we go .
Marketing: {vocalsound} Perfect .
Project Manager: Okay . So we just remove our {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
User Interface: Screen .
Marketing: {vocalsound}
Project Manager: screen here .
User Interface: Make it a bigger dial .
Industrial Designer: {vocalsound}
User Interface: Easier to use . Even easier to use then .
Project Manager: {vocalsound}
Industrial Designer: {vocalsound} Okay , the {disfmarker}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: Besides look at what the L_C_D_ does to our lovely remote .
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: Back to the design room boys .
Industrial Designer: So we can just take away a heck of a lot of the {disfmarker} {vocalsound}
Marketing: {vocalsound}
User Interface: {vocalsound}
Marketing: {gap} .
Industrial Designer: there you go . {gap} central ?
Marketing: What's the blue part ?
User Interface: That was just {disfmarker}
Industrial Designer: Oh that's just {disfmarker}
User Interface: we ran out of yellow . {vocalsound}
Marketing: Oh that's the batteries .
Industrial Designer: yeah .
Marketing: Okay . {vocalsound}
Industrial Designer: There you go
User Interface: There you go .
Industrial Designer: . Oops .
User Interface: Even simpler .
Marketing: {vocalsound} Looks more like a banana .
User Interface: Yeah .
Industrial Designer: There you go .
User Interface: For all those fruit lovers out there .
Industrial Designer: One more criteria .
Project Manager: {vocalsound}
Marketing: {vocalsound}
Project Manager: {vocalsound} Okay so the costs under twelve point five Euro . Was no . We redesigned it . Now it's yes .
User Interface: Yeah .
Marketing: {vocalsound}
Project Manager: Next slide . Project evaluation . Uh project process , satisfaction with , for example , room for creativity , leadership , teamwork , means , new ideas found . Um {disfmarker} So {disfmarker} I guess that {disfmarker} Let's see here . I think that perhaps the project evaluation's just supposed to be completed by me . But I'd like to hear your thoughts .
Marketing: {vocalsound}
Project Manager: {vocalsound}
Industrial Designer: {vocalsound} Fair enough . {vocalsound}
User Interface: {vocalsound}
Marketing: Trying to fill in some time there . {vocalsound}
Project Manager: Uh h what did you think of our project process ? {vocalsound}
Industrial Designer: Great . {vocalsound}
User Interface: I think we did {disfmarker} yeah I think we did quite well . Um {disfmarker}
Industrial Designer: Yeah .
Project Manager: Good .
Marketing: Good teamwork {gap} . {vocalsound}
Industrial Designer: Just half a day , you have a remote . There you go .
User Interface: Yeah . Right from the start of the day .
Project Manager: Yeah I think {disfmarker}
User Interface: We sort of knew where we were going straight away I thought .
Project Manager: {gap} we st we started off a little little weak . Our leadership was quite weak in the beginning .
Marketing: {vocalsound}
Project Manager: Um {vocalsound} um {disfmarker}
Marketing: {vocalsound}
Project Manager: But as the day went along we had more idea of what we were doing . Um room for creativity ? There was that . Um I think we tried a lotta different things and um I think it was um interesting as you guys brought up more um information and studies that we were right on with a lot of those things . Um you guys worked together well as a team . And um the means ? Which was the whiteboard and the pens .
User Interface: Yeah . We've used the whiteboard .
Industrial Designer: Super super .
Project Manager: I had some problem with the pen I think , but {vocalsound} minus your p
Marketing: Minus your PowerPoint fiasco .
Industrial Designer: {vocalsound} Well that's not my fault . That's obviously the people I work for uh that work for me ,
Marketing: No I know . I'm {disfmarker}
Project Manager: Well {disfmarker}
Marketing: yeah . Incom {vocalsound}
Industrial Designer: uh they've just you know {disfmarker}
User Interface: {vocalsound}
Project Manager: {vocalsound} Have a {disfmarker}
Industrial Designer: Heads are gonna roll , believe me .
Project Manager: we have a list of employees that you would like fired .
User Interface: {vocalsound}
Marketing: {vocalsound}
User Interface: {vocalsound}
Industrial Designer: Yes yes .
Project Manager: Okay . N new ideas found ? Um {disfmarker}
Industrial Designer: {vocalsound}
Marketing: Mm . Kinda .
Project Manager: Yes for the remote . Maybe no not f for
User Interface: Technology used .
Project Manager: technology . Alright . Closing . Costs are within the budget . Project is evaluated . Um complete the final questionnaire and meeting summary . That's it .
User Interface: Excellent .
Project Manager: And I still have to do my minutes for the last meeting . {vocalsound}
Marketing: {vocalsound}
Project Manager: Actually . Um so there will probably be another questionnaire coming up . And then we'll have to check with the main boss whether we can , what goes on after that .
Marketing: We might have a while though .
Industrial Designer: {gap} .
Project Manager: But that's the end of our meeting .
2022-06-15 06:45:01 | INFO | __main__ | output #0: Project Manager introduced that the prototype incorporated fashion trends that people prefer fancy looking products like fruit and vegetable. After That, User Interface presented the product which looked like a banana and was bright yellow except for the blue button. The style was as simple as possible in order to fit the customers' need for simplicity. Also, the product could be curved and used both-handed with advanced chips hidden inside, which seemed quite creative and identical to iPod features. In the end, Industrial Designer commented that the remote control could be smaller in size.
2022-06-15 06:45:01 | INFO | __main__ | input #1: How did Marketing design the product evaluation?

Project Manager: Yep . Soon as I get this . Okay . This is our last meeting . Um I'll go ahead and go through the minutes from the previous meeting . Uh and then we'll have a , the prototype presentation . {vocalsound} Um then we will um do an evaluation . Uh or we'll see what , what we need to have under the criteria for the evaluation . Then we'll go through the finance and see if we fall within the budget . Um then we'll do the evaluation , and then we can finish up after that with um any changes that we'll need to make , or hopefully everything will fall right in line . Um let's see , minutes from the last meeting . Um we looked at uh the the trends . We had uh the fashion trends that people want a fancy look-and-feel . It was twice as important as anything else . Um they liked fruit and vegetables in the new styles . Um and a spongy feel . So we were talking about trying to incorporate those into our prototype . Um they wanted limited buttons and simplicity . Um then we looked at the uh the method for coming up with our own remote . Um looking at other other devices . Um the iPod , we really liked the look of that . Um we also had uh the kid's remote for a simple idea . Um a two part remote , which was what were were originally looking at . Uh and then um there was talk of spee uh speech recognition um becoming more uh predominant and easier to use . But I think we've still decided not to go with that . {vocalsound} Then we looked at the components um the materials for the case , the different energy sources , the different types of chips , um and made a decision on what we were going to use to make our remote . Um and basically how , what were making for the prototype . So I'm going to leave it at that and let you guys take over .
User Interface: The prototype discussion .
Project Manager: The prototype yeah . Do you need a {disfmarker} this ?
User Interface: No . {vocalsound}
Project Manager: Okay .
Industrial Designer: {vocalsound} Can try to plug that in there
User Interface: There is our remo {gap} the banana .
Marketing: {vocalsound}
Industrial Designer: but {disfmarker}
User Interface: Um {vocalsound} yeah basically we we st went with the colour yellow . Um working on the principle of a fruit which was mentioned , it's basically designed around a banana .
Project Manager: {vocalsound}
User Interface: Um but it would be held in such a fashion ,
Marketing: {vocalsound}
User Interface: where it is , obviously it wouldn't be that floppy 'cause this would be hard plastic . These would be like the rubber , the rubber grips . So that's so that would hopefully help with grip , or like the ergonomics of it . Um but all the controlling would be done with this scroll wheel . You have to use your imagination a little bit . And this here represents the screen , where you , where you'd go through .
Project Manager: Very nice .
User Interface: And the the simplest functions would be um almost identical to an iPod , where that one way ch through channels , that way th other way through channels . Volume up and down . And then to access the more complicated functions you'd you sorta go , you press that and go through the menus . It's that that simple . That just represents the infrared uh beam . That's a simple on and off switch . Um I don't know , we could use the voice . T that blue bits should be yellow , that that'd be where the batteries would be I suppose . And um {vocalsound} that's about it . It's as simple as you , we could make it really .
Industrial Designer: Right .
User Interface: Is there anything you want to add ?
Industrial Designer: That's what we have there . That's plastic . Plastic covered with rubber . We might uh add some more underneath here . Maybe give it , give it a form . I mean you're supposed to hold it like that , but um just if you grab it , take it from somewhere ,
User Interface: Yeah .
Project Manager: Mm-hmm .
Industrial Designer: so {disfmarker} yeah ,
User Interface: Doesn't make much make much difference .
Industrial Designer: you have some rub yeah .
User Interface: You could work left-handed or right-handed I suppose .
Industrial Designer: Exactly , {gap} use both . Might as well think about {disfmarker}
User Interface: T the actual thing might be smaller .
Industrial Designer: Th think about the button as well . Like either put either one {gap} one on either side or
User Interface: {vocalsound} Yeah .
Project Manager: What but what's that button ?
Industrial Designer: not do it at all . It's a quick on-off button .
User Interface: Just the on and off .
Project Manager: Uh , 'kay .
Industrial Designer: That's um
Marketing: {vocalsound}
Industrial Designer: yeah I think it's pretty important . So you don't have to fiddle with that .
Project Manager: 'Kay .
Industrial Designer: Right ? Um that's not um {disfmarker}
Project Manager: {vocalsound}
Industrial Designer: I'd say a bit smaller would probably be nice . You wanna play with that over there .
User Interface: Yeah .
Industrial Designer: There you go .
User Interface: It's you know it's flimsy 'cause it's made out of heavy Play-Doh ,
Marketing: {vocalsound}
Project Manager: Would you like to uh {disfmarker}
Industrial Designer: Right .
User Interface: but {disfmarker}
Marketing: Pretty impressive .
Project Manager: Well done .
User Interface: {vocalsound}
Marketing: {vocalsound} Kind of a banana .
User Interface: And whether or not it would fall into the cost {gap} everything I suppose . With the scroll and the L_C_D_ .
Project Manager: Well luckily we are going to find out . Or not luckily . Um do you have a marketing presentation for us .
Industrial Designer: {vocalsound}
Marketing: {vocalsound} I do . Okay . You guys are gonna help me do an evaluation of the criteria . Um . Okay . So first I'll just discuss some of the criteria that I found . Just based on the past trend reports that I was looking at earlier . And then we'll do a group evaluation of the prototype . And then we will calculate the average score to see how we did . Um so the criteria we're gonna be looking at are the complaints um that we heard from the users who were interviewed earlier . So we're gonna be doing it based on a seven point scale . And one is going to mean true , that we did actually achieve that . With seven being false , we did not achieve that . {gap} . Okay . So for the first one , we need to decide , did we solved the problem of the users who complained about an ugly remote ? {vocalsound}
Industrial Designer: {vocalsound} {vocalsound} .
User Interface: {vocalsound}
Project Manager: I think it's definitely different than anything else out there .
User Interface: {vocalsound}
Marketing: Mm .
User Interface: Yeah .
Project Manager: So if they think that what is out there is ugly , then yes I would say , I would say most definitely .
Marketing: {vocalsound}
User Interface: I would {gap} .
Project Manager: It's bright .
User Interface: It's bright . It's {disfmarker}
Project Manager: It still has your traditional black .
User Interface: It's curved . It's not {disfmarker} there's no sharp
Industrial Designer: {vocalsound}
User Interface: angles to it .
Project Manager: Yep , not angular .
Marketing: Mm .
Industrial Designer: I'd say , when it comes to the ergonomics , the form and stuff , yes that's definitely more beautiful than your average .
Marketing: {vocalsound}
Industrial Designer: However the colour , we don't have a say in that .
Marketing: Yeah I think the colours detract a little bit . {vocalsound}
User Interface: Some people might say it . Yeah .
Industrial Designer: That has been , that has been dictated pretty much by the company .
Project Manager: Mm .
Industrial Designer: So uh to answer that honestly I would rather say like uh , we have not solved the problem completely with the ugly remote because the colour is ugly , definitely .
Project Manager: {vocalsound} Yep .
Marketing: That's true . Yeah .
Project Manager: {vocalsound}
User Interface: Yeah .
Industrial Designer: 'S nothing you can say about that . I mean I much prefer something like brushed chrome with that form .
User Interface: Yeah .
Industrial Designer: But {disfmarker}
Project Manager: Yeah something more modern to go {disfmarker} a a modern colour to go with the modern form .
Industrial Designer: Right . Right . It's different . You don't want your uh three feet huge L_C_D_ dis display in your living room that's hanging from the wall to be controlled with something like that .
Marketing: Um okay so , do you think , since we {disfmarker} This was a a sign criteria , do you think maybe we should put it somewhere in the middle then ?
Industrial Designer: Yeah .
Marketing: Does that sound good ?
Project Manager: Yeah .
User Interface: Yeah .
Industrial Designer: {vocalsound}
Marketing: What do you think ? Three ? Four ?
Project Manager: I would say
Marketing: Five ?
Project Manager: four . {vocalsound}
Industrial Designer: Yeah . {vocalsound}
Marketing: {vocalsound} Four is fair . Okay .
Project Manager: Very non-committal , four .
Marketing: Okay , the second one . Did we make it simple for new users ?
Industrial Designer: It's very intuitive , I think yeah .
User Interface: Yeah . I think that was the main aim , one of the main aims that we had .
Industrial Designer: {vocalsound} S give it a one .
Marketing: One ,
Project Manager: Yeah .
Marketing: 'kay . Okay . Um , do the controls now match the operating behaviour of the users ?
User Interface: Uh yeah . 'Cause we've we've brought it down to basically four controls {gap} most common , which are channel and volume .
Marketing: I'd say that {disfmarker}
Project Manager: Mm-hmm .
Industrial Designer: Right .
User Interface: And then the other ones are just a matter of just going , just scrolling further .
Project Manager: S scrolling through and selecting a few .
Industrial Designer: Right . So that's a one .
Marketing: So one ?
Project Manager: I think that's a one .
Marketing: Yeah ? {vocalsound} Okay . Okay um the fourth one . How about the problem of a remote being easily lost ? One of the number one complaints .
Industrial Designer: Something that big and that yellow you just don't lose anymore .
Project Manager: {vocalsound}
User Interface: {vocalsound} Yeah .
Marketing: {vocalsound} Whether you want to or not , you're not gonna lose it . {vocalsound}
User Interface: It's bright yellow .
Industrial Designer: {vocalsound}
User Interface: Bright yellow's hard to lose . But um if we were to , if we were , that , the speech recognition . That , we could maybe just use that solely for the the finding thing . That was what we'd we'd mentioned .
Project Manager: So if we incorporate speech recognition into it then it could {disfmarker}
User Interface: Just just to use , to find it when it was lost . But like I said , like I don't think you'd lose something so yellow so easily .
Industrial Designer: Oops . Hmm .
User Interface: And it's not gonna fall , like a rectangle would slip down behind things . That's gonna be a difficult shape to {disfmarker}
Industrial Designer: Well what {disfmarker}
Project Manager: And it is quite bright and {disfmarker}
User Interface: Yeah .
Marketing: {vocalsound}
User Interface: Maybe in the middle again , three or four or something ?
Project Manager: Uh {disfmarker}
Industrial Designer: S
Marketing: Okay .
User Interface: I mean you know {gap} loo losing things is one of those things that people can lose , I mean a million ways .
Project Manager: Yeah .
User Interface: You can pick it up and walk away with it and then you've lost it .
Industrial Designer: Mm .
Marketing: That's true .
Project Manager: But if we do go with the , with the speech recognition , then it , then our scale goes up quite a bit I think .
Marketing: Mm .
Industrial Designer: Oh yeah . You probably {disfmarker}
User Interface: Yeah .
Project Manager: Probably two . You know . If we eliminate the fact that you know it's impossible to guarantee that it's not gonna be lost then
User Interface: Yeah .
Industrial Designer: Mm .
Project Manager: I'd say two .
Industrial Designer: {vocalsound}
Marketing: {vocalsound}
Project Manager: With the speech recognition , which of course may be changed depending on budget .
User Interface: Yeah .
Industrial Designer: Y you could add an extra feature actually . Which makes this thing raise hell when you remove it too far from the television .
User Interface: Yeah .
Industrial Designer: We could add that but that's nothing we have thought of so far .
Project Manager: Which , which may be cheaper than speech recognition if it were just a {disfmarker}
Industrial Designer: Yes .
User Interface: Yeah true . But I mean d just those whistling , clapping key rings you have . They're cheap .
Marketing: Annoying alarm or something ?
Project Manager: {vocalsound}
Industrial Designer: It's it's {disfmarker}
Marketing: Yeah .
User Interface: So it can't be that
Industrial Designer: Um the {disfmarker} it's based on this anti anti-theft technology for suitcases and stuff ,
User Interface: expensive .
Project Manager: Some sort of proximity {vocalsound} {disfmarker}
User Interface: Yeah .
Industrial Designer: where you have one piece that's attached to your luggage , another piece that starts beeping . That can't cost much .
User Interface: Yeah .
Industrial Designer: So that can also easily be integrated because these things are small enough to to hide , so you have one piece , you have to glue somewhere behind your {disfmarker} stick it behind your T_V_ and the other {disfmarker}
User Interface: {gap} stick it on the T_V_ {gap} .
Project Manager: {vocalsound} Pray that you don't accidentally lose that piece . {vocalsound}
Industrial Designer: Right .
User Interface: {vocalsound}
Marketing: {vocalsound}
Industrial Designer: That'd be tough then . {vocalsound} Well also your remote would uh alarm you if somebody stole you t your television , yeah . Ran off with it without taking the beautiful remote control .
Project Manager: {vocalsound}
User Interface: Yeah . {vocalsound}
Marketing: So . Are we adding one of these two features ?
Industrial Designer: Let's add one of those features and say yes . {vocalsound}
Marketing: {gap} gonna say {disfmarker} okay .
Project Manager: Okay .
Marketing: So we're {vocalsound} back to a one ?
User Interface: Two .
Marketing: Or a two ?
Project Manager: Two .
Industrial Designer: Two .
Marketing: Two , 'kay . Okay . Are we technologically innovative ?
Industrial Designer: Uh {disfmarker}
User Interface: {vocalsound} I'd say so .
Industrial Designer: {vocalsound}
User Interface: Uh don't get many mo remote controls with
Industrial Designer: It's all just {disfmarker}
User Interface: screens on .
Industrial Designer: It's all just stolen technology when it comes down to {disfmarker} {vocalsound} {vocalsound}
User Interface: Yeah it's stolen technology .
Marketing: From iPod yeah . {vocalsound}
Project Manager: It's {disfmarker} {vocalsound}
User Interface: But we have {gap} .
Project Manager: But there's not a lot of yellow , there's not a lotta yellow .
Industrial Designer: right
Marketing: But for remotes {disfmarker} yeah .
Project Manager: Course that wasn't really {disfmarker}
Industrial Designer: right
User Interface: Fa
Industrial Designer: right right .
Project Manager: we were kinda forced to take that colour .
Marketing: Two ? Three ?
User Interface: {gap} 'cause it's stolen .
Project Manager: I don't know that we are that innovative , to tell you the truth . {vocalsound}
User Interface: No maybe not .
Industrial Designer: Yeah not really .
Marketing: But how many remotes do you see like this ?
User Interface: {vocalsound}
Project Manager: If we added the screaming factor {vocalsound} then we go up .
Industrial Designer: {vocalsound}
Marketing: Not so many .
Industrial Designer: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: {vocalsound} Um I would say we're probably at four .
Industrial Designer: Right .
Marketing: Really ? Okay . {vocalsound} That's gonna hurt us .
User Interface: {vocalsound}
Marketing: Okay . Um spongy material ?
Industrial Designer: Yeah well you have that , kind of , sort of .
Project Manager: We have some spongy , yeah .
User Interface: Yeah as much as as needed , I think .
Marketing: 'Kay .
Industrial Designer: It's not a one though .
Project Manager: No .
Industrial Designer: One would be the whole thing
Project Manager: Yeah . Because it's only got what , these parts are the grips and perhaps the back side {disfmarker} the bottom {disfmarker} the underneath on the back .
Industrial Designer: to fold and stuff . Yeah .
User Interface: Yeah .
Industrial Designer: So that's a four at most .
Project Manager: Probably a four at most . Possibly even a five .
Marketing: And lastly , did we put the fashion in electronics ?
Project Manager: {vocalsound}
Industrial Designer: Y yes .
User Interface: Yeah .
Marketing: I'd say we did .
Project Manager: If your fashion is b is Carmen Miranda , you betcha . {vocalsound}
Industrial Designer: More {disfmarker} {vocalsound}
User Interface: Yeah . {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound} Well the recent fashion is rather displayed in the in the L_C_D_ and the way you operate it than the form and the colour ,
User Interface: On the {disfmarker}
Project Manager: It's true .
User Interface: Yeah .
Industrial Designer: but it definitely is {disfmarker}
User Interface: Be what we were told , and they'd say yeah , definitely .
Industrial Designer: {gap} .
Marketing: 'Kay . Alright . Now we just gotta calculate . Six eight twelve sixteen . Seventeen divided by s
User Interface: {gap} .
Project Manager: Seven is {disfmarker}
Marketing: Eight .
Project Manager: Two point {disfmarker}
Marketing: {vocalsound}
Project Manager: {gap} two point four ?
User Interface: Is that some long division ? No .
Project Manager: Something .
Marketing: Well I haven't done math in years .
Industrial Designer: {vocalsound}
Marketing: What two {disfmarker}
User Interface: {vocalsound}
Marketing: {vocalsound} I dunno .
User Interface: Just , I'm sure there's a {gap} .
Marketing: Okay we'll say two point four two . Right ? How does that look ?
Industrial Designer: I'm impressed . I can't do that without a calculator . {vocalsound}
User Interface: No I can't do long {gap}
Marketing: {vocalsound} It's been a while .
User Interface: very impressive .
Project Manager: And what what is the acceptable criteria ? Is there like a scale that we have to hit ?
Marketing: Oh no . They just told me to
Industrial Designer: {vocalsound}
Marketing: {vocalsound} pick my own criteria and have you guys evaluate it {vocalsound} basically .
Project Manager: {vocalsound} Alright then .
Marketing: So that's that .
Project Manager: Okay . Well , let's see .
Marketing: {vocalsound}
Project Manager: Now we get to do the budget numbers . You didn't know that you were gonna have a budget . But we do . Okay .
User Interface: Yeah . Yeah so . You'd been going a long time dividing that . It's two point four two eight five se it just keeps going on .
Marketing: Oh my god .
User Interface: Two point four two basically .
Marketing: Okay . Yeah we'll go with that .
Project Manager: So I have here an {disfmarker}
Industrial Designer: {vocalsound} Fifty percent , you're kidding .
Marketing: Not too shabby .
Project Manager: Yeah .
Industrial Designer: {vocalsound} P
Project Manager: We want a fifty percent profit on this . Oh you can't really see that very well .
User Interface: {vocalsound} Charge about three hundred quid for it .
Project Manager: {vocalsound} Twelve and a half Euros is what supposed to cost us . Okay , so {disfmarker}
Industrial Designer: It's too much .
Project Manager: Well let's see .
Industrial Designer: Um {disfmarker}
Project Manager: The f the {disfmarker} Wonder if I can make this {disfmarker}
Industrial Designer: Uh {disfmarker}
Project Manager: What the {disfmarker} {vocalsound} Oh it won't let me do that . Okay . Alright so at top , I don't know if you guys can read that or not . I can't 'cause I don't have my glasses on ,
Industrial Designer: {vocalsound}
Project Manager: but so we've got the energy source . There's uh four , five , six categories .
Industrial Designer: Battery .
Project Manager: We have energy source , electronics , case . Then we have case material supplements , interface type , and then button supplements . Okay so {disfmarker} Uh first of all energy source , we picked battery . Um and how many batteries do we think this will probably take ?
User Interface: {vocalsound}
Project Manager: Probably some e either two or four .
Industrial Designer: Two .
Project Manager: Two ? {vocalsound} Like it . {vocalsound}
Industrial Designer: At four it's gonna be too heavy , so that that's not our problem . People can change it every month .
Project Manager: {vocalsound} Excellent .
Industrial Designer: {vocalsound} They won't know until after they bought it .
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: This is consumerism .
Industrial Designer: {vocalsound}
Project Manager: Alright so for the electronics our choices are simpl simple chip-on-print , regular chip-on-print , advanced chip-on-print , sample sensor , sample speaker .
Industrial Designer: {gap} .
User Interface: We're advanced chip are we ?
Industrial Designer: That's the advanced chip-on-print , yeah .
Project Manager: 'Kay , {gap} we have one of those . 'Kay then the case is a {disfmarker} Probably it's double curved .
Industrial Designer: Double curved , yes .
Project Manager: Case materials are
Industrial Designer: Plastic .
Project Manager: plastic . Um I guess it's two , since one for the top , one for the bottom .
Industrial Designer: N no .
Project Manager: Is that right or is it just one ?
Industrial Designer: No that's just one .
Project Manager: Maybe it's one because of the {disfmarker}
Industrial Designer: It's just one mo single mould , we can do that .
Project Manager: 'Kay .
User Interface: Yeah {gap} yeah .
Marketing: Right . {vocalsound}
Project Manager: I guess it doesn't matter 'cause the price on that one is zero , which is nice .
Industrial Designer: Exactly , right .
Marketing: Oh .
Project Manager: Special colour ?
Industrial Designer: That's not a special colour . It's a specially ugly colour , but it's not special .
Marketing: Bright yellow .
Project Manager: {vocalsound} Interface type . We have pushbutton , scroll-wheel interface , integrated scroll-wheel pushbutton , and an L_C_D_ display .
User Interface: {vocalsound}
Marketing: {vocalsound}
User Interface: S
Industrial Designer: S {vocalsound}
User Interface: That's {disfmarker} Yeah .
Project Manager: So we actually have the L_C_D_ display
Marketing: {vocalsound}
User Interface: And then {disfmarker}
Project Manager: and then is it the integrated or is it {disfmarker}
User Interface: I'd say the integrated .
Project Manager: Yeah .
Industrial Designer: Yes unfortunately .
Project Manager: 'Kay . Button supplement ? Special colour ?
User Interface: Mm .
Project Manager: Um special form ? Special material .
Industrial Designer: We could of course make the buttons wood .
Project Manager: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound} Say mahogany or so
Marketing: {vocalsound} It'd look really lovely .
Project Manager: Or titanium .
Industrial Designer: Mm-hmm or titanium .
Project Manager: They cost us all the same .
Marketing: {vocalsound} Yeah .
User Interface: {gap} remote control {gap} .
Project Manager: Well we only have one button so really we shouldn't be charged ,
Industrial Designer: Uh just {disfmarker}
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
Project Manager: {vocalsound} we shouldn't be charged anything for the the button supplements .
User Interface: No that's getting a bit tiny .
Project Manager: Um {disfmarker}
User Interface: Yeah .
Marketing: {vocalsound}
User Interface: I'd ignore that .
Marketing: Leave it blank .
Project Manager: Okay . We're gonna leave that one blank because we run on a L_C_D_ and scroll . So our total is fifteen point five . Which I believe is
Industrial Designer: Yeah that's too much .
Project Manager: by three Euros over .
Industrial Designer: It's hard to believe . So we'll go for the hand dynamo huh ? {vocalsound}
Project Manager: {vocalsound}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: So the only thing better than um a banana-shaped remote is one that you shake .
User Interface: If it w What if we completely took out the the one single button we've got on .
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
User Interface: And just had a scroll wheel interface . And the L_C_D_ display . I suppose the L_C_D_ C_ display's the one that's pushing it up a bit though .
Project Manager: Yeah 'cause the {disfmarker}
Marketing: {vocalsound}
Project Manager: Well 'cause we have to have both right ?
User Interface: Yeah .
Industrial Designer: I mean let's let's face it , it also depends on the software on the on the television .
User Interface: Yeah .
Industrial Designer: You can have the the information that this thing transmits be being displayed on the on the screen .
Project Manager: Mm-hmm .
Industrial Designer: So s yeah let's take away the {disfmarker}
User Interface: Yeah you could maybe take out the L_C_D_ dis display even ,
Industrial Designer: Yeah . Yeah .
User Interface: if it if it comes up on the computer itsel on the T_V_ itself .
Industrial Designer: Right .
Project Manager: So we may not need the L_C_D_ display ?
User Interface: Uh that is possible yeah .
Industrial Designer: Right . We may not need it . There you go .
Project Manager: Well there we go .
Industrial Designer: Perfect .
Project Manager: Twelve point five .
User Interface: There we go .
Marketing: {vocalsound} Perfect .
Project Manager: Okay . So we just remove our {vocalsound}
Marketing: {vocalsound}
Industrial Designer: {vocalsound}
User Interface: Screen .
Marketing: {vocalsound}
Project Manager: screen here .
User Interface: Make it a bigger dial .
Industrial Designer: {vocalsound}
User Interface: Easier to use . Even easier to use then .
Project Manager: {vocalsound}
Industrial Designer: {vocalsound} Okay , the {disfmarker}
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: Besides look at what the L_C_D_ does to our lovely remote .
User Interface: {vocalsound}
Marketing: {vocalsound}
Project Manager: Back to the design room boys .
Industrial Designer: So we can just take away a heck of a lot of the {disfmarker} {vocalsound}
Marketing: {vocalsound}
User Interface: {vocalsound}
Marketing: {gap} .
Industrial Designer: there you go . {gap} central ?
Marketing: What's the blue part ?
User Interface: That was just {disfmarker}
Industrial Designer: Oh that's just {disfmarker}
User Interface: we ran out of yellow . {vocalsound}
Marketing: Oh that's the batteries .
Industrial Designer: yeah .
Marketing: Okay . {vocalsound}
Industrial Designer: There you go
User Interface: There you go .
Industrial Designer: . Oops .
User Interface: Even simpler .
Marketing: {vocalsound} Looks more like a banana .
User Interface: Yeah .
Industrial Designer: There you go .
User Interface: For all those fruit lovers out there .
Industrial Designer: One more criteria .
Project Manager: {vocalsound}
Marketing: {vocalsound}
Project Manager: {vocalsound} Okay so the costs under twelve point five Euro . Was no . We redesigned it . Now it's yes .
User Interface: Yeah .
Marketing: {vocalsound}
Project Manager: Next slide . Project evaluation . Uh project process , satisfaction with , for example , room for creativity , leadership , teamwork , means , new ideas found . Um {disfmarker} So {disfmarker} I guess that {disfmarker} Let's see here . I think that perhaps the project evaluation's just supposed to be completed by me . But I'd like to hear your thoughts .
Marketing: {vocalsound}
Project Manager: {vocalsound}
Industrial Designer: {vocalsound} Fair enough . {vocalsound}
User Interface: {vocalsound}
Marketing: Trying to fill in some time there . {vocalsound}
Project Manager: Uh h what did you think of our project process ? {vocalsound}
Industrial Designer: Great . {vocalsound}
User Interface: I think we did {disfmarker} yeah I think we did quite well . Um {disfmarker}
Industrial Designer: Yeah .
Project Manager: Good .
Marketing: Good teamwork {gap} . {vocalsound}
Industrial Designer: Just half a day , you have a remote . There you go .
User Interface: Yeah . Right from the start of the day .
Project Manager: Yeah I think {disfmarker}
User Interface: We sort of knew where we were going straight away I thought .
Project Manager: {gap} we st we started off a little little weak . Our leadership was quite weak in the beginning .
Marketing: {vocalsound}
Project Manager: Um {vocalsound} um {disfmarker}
Marketing: {vocalsound}
Project Manager: But as the day went along we had more idea of what we were doing . Um room for creativity ? There was that . Um I think we tried a lotta different things and um I think it was um interesting as you guys brought up more um information and studies that we were right on with a lot of those things . Um you guys worked together well as a team . And um the means ? Which was the whiteboard and the pens .
User Interface: Yeah . We've used the whiteboard .
Industrial Designer: Super super .
Project Manager: I had some problem with the pen I think , but {vocalsound} minus your p
Marketing: Minus your PowerPoint fiasco .
Industrial Designer: {vocalsound} Well that's not my fault . That's obviously the people I work for uh that work for me ,
Marketing: No I know . I'm {disfmarker}
Project Manager: Well {disfmarker}
Marketing: yeah . Incom {vocalsound}
Industrial Designer: uh they've just you know {disfmarker}
User Interface: {vocalsound}
Project Manager: {vocalsound} Have a {disfmarker}
Industrial Designer: Heads are gonna roll , believe me .
Project Manager: we have a list of employees that you would like fired .
User Interface: {vocalsound}
Marketing: {vocalsound}
User Interface: {vocalsound}
Industrial Designer: Yes yes .
Project Manager: Okay . N new ideas found ? Um {disfmarker}
Industrial Designer: {vocalsound}
Marketing: Mm . Kinda .
Project Manager: Yes for the remote . Maybe no not f for
User Interface: Technology used .
Project Manager: technology . Alright . Closing . Costs are within the budget . Project is evaluated . Um complete the final questionnaire and meeting summary . That's it .
User Interface: Excellent .
Project Manager: And I still have to do my minutes for the last meeting . {vocalsound}
Marketing: {vocalsound}
Project Manager: Actually . Um so there will probably be another questionnaire coming up . And then we'll have to check with the main boss whether we can , what goes on after that .
Marketing: We might have a while though .
Industrial Designer: {gap} .
Project Manager: But that's the end of our meeting .
2022-06-15 06:45:01 | INFO | __main__ | output #1: Marketing had some evaluation criteria in mind, based on previous marketing strategy, on the latest trends, and on user preferences. The team should figure out whether their product could solve the complaints of the ugly remote control. There was a seven-point scale rating for each criterion. The team would give comments to each feature listed and agree on the final rating.
2022-06-15 06:45:01 | INFO | __main__ | 
Running tokenizer on train dataset:   0%|          | 0/2 [00:00<?, ?ba/s]Running tokenizer on train dataset:  50%|█████     | 1/2 [00:19<00:19, 19.77s/ba]Running tokenizer on train dataset: 100%|██████████| 2/2 [00:26<00:00, 15.91s/ba]Running tokenizer on train dataset: 100%|██████████| 2/2 [00:26<00:00, 13.39s/ba]
2022-06-15 06:45:27 | INFO | datasets.arrow_writer | Done writing 1257 examples in 17861718 bytes .
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
2022-06-15 06:45:33 | INFO | __main__ | 
2022-06-15 06:45:33 | INFO | __main__ | Validation examples before tokenization:
2022-06-15 06:45:33 | INFO | __main__ | input #0: What was agreed upon on sample transcripts?

Professor E: So . OK . Doesn't look like it crashed . That 's great .
Grad G: So I think maybe what 's causing it to crash is I keep starting it and then stopping it to see if it 's working . And so I think starting it and then stopping it and starting it again causes it to crash . So , I won't do that anymore .
Postdoc B: And it looks like you 've found a way of uh mapping the location to the {disfmarker} without having people have to give their names each time ?
PhD A: Sounds like an initialization thing .
Postdoc B: I mean it 's like you have the {disfmarker} So you know that {disfmarker}
Grad G: No .
Postdoc B: I mean , are you going to write down {pause} that I sat here ?
Grad G: I 'm gonna collect the digit forms and write it down .
Postdoc B: OK .
PhD C: Oh , OK .
Grad G: So {disfmarker} So they should be right with what 's on the digit forms . OK , so I 'll go ahead and start with digits . u And I should say that uh , you just pau you just read each line an and then pause briefly .
Professor E: And start by giving the transcript number .
PhD A: Tran
PhD D: Transcript {disfmarker} Uh . OK , OK .
PhD A: Oh sorry , go ahead .
Professor E: So uh , you see , Don , the unbridled excitement of the work that we have on this project .
Grad H: OK .
Professor E: It 's just uh {disfmarker}
Grad H: Umh .
Professor E: Uh , you know , it doesn't seem like a bad idea to have {comment} that information .
Grad G: And I 'm surprised I sort of {disfmarker} I 'm surprised I forgot that ,
Professor E: Yeah , I {disfmarker} I 'd {disfmarker} I think it 's some
Grad G: but uh I think that would be a good thing to add . After I just printed out a zillion of them .
Professor E: Yeah , well , that 's {disfmarker} Um , so I {disfmarker} I do have a {disfmarker} a an agenda suggestion . Uh , we {disfmarker} I think the things that we talk about in this meeting uh tend to be a mixture of uh procedural uh mundane things and uh research points and um I was thinking I think it was a meeting a couple of weeks ago that we {disfmarker} we spent much of the time talking about the mundane stuff cuz that 's easier to get out of the way and then we sort of drifted into the research and maybe five minutes into that Andreas had to leave . So {vocalsound} uh I 'm suggesting we turn it around and {disfmarker} and uh sort of we have {disfmarker} anybody has some mundane points that we could send an email later , uh hold them for a bit , and let 's talk about the {disfmarker} the research - y kind of things . Um , so um the one th one thing I know that we have on that is uh we had talked a {disfmarker} a couple weeks before um uh about the uh {disfmarker} the stuff you were doing with {disfmarker} with uh um uh l l attempting to locate events , we had a little go around trying to figure out what you meant by " events " but I think , you know , what we had meant by " events " I guess was uh points of overlap between speakers . But I th I gather from our discussion a little earlier today that you also mean uh interruptions with something else
PhD D: Yeah .
Professor E: like some other noise .
PhD D: Uh - huh . Yeah .
Professor E: Yes ? You mean that as an event also .
PhD D: To
Professor E: So at any rate you were {disfmarker} you 've {disfmarker} you 've done some work on that
PhD D: right .
Professor E: and um then the other thing would be it might be nice to have a preliminary discussion of some of the other uh research uh areas that uh we 're thinking about doing . Um , I think especially since you {disfmarker} you haven't been in {disfmarker} in these meetings for a little bit , maybe you have some discussion of some of the p the plausible things to look at now that we 're starting to get data , uh and one of the things I know that also came up uh is some discussions that {disfmarker} that uh {disfmarker} that uh Jane had with Lokendra uh about some {disfmarker} some {disfmarker} some um uh work about I {disfmarker} I {disfmarker} I d I {disfmarker} I don't want to try to say cuz I {disfmarker} I 'll say it wrong , but anyway some {disfmarker} some potential collaboration there about {disfmarker} about the {disfmarker} about the {disfmarker} working with these data .
PhD C: Oh . Sure .
Professor E: So . So , uh .
Grad G: You wanna just go around ?
Professor E: Uh . {pause} Well , I don't know if we {disfmarker} if this is sort of like everybody has something to contribute sort of thing , I think there 's just just a couple {disfmarker} a couple people primarily um but um Uh , wh why don't {disfmarker} Actually I think that {disfmarker} that last one I just said we could do fairly quickly so why don't you {disfmarker} you start with that .
Postdoc B: OK . Shall I {disfmarker} shall I just start ? OK .
Professor E: Yeah , just explain what it was .
Postdoc B: Um , so , uh , he was interested in the question of {disfmarker} you know , relating to his {disfmarker} to the research he presented recently , um of inference structures , and uh , the need to build in , um , this {disfmarker} this sort of uh mechanism for understanding of language . And he gave the example in his talk about how {pause} um , e a I 'm remembering it just off the top of my head right now , but it 's something about how um , i " Joe slipped " you know , " John had washed the floor " or something like that . And I don't have it quite right , but that kind of thing , where you have to draw the inference that , OK , there 's this time sequence , but also the {disfmarker} the {disfmarker} the causal aspects of the uh floor and {disfmarker} and how it might have been the cause of the fall and that um it was the other person who fell than the one who cleaned it and it {disfmarker} {comment} These sorts of things . So , I looked through the transcript that we have so far , {comment} and um , fou identified a couple different types of things of that type and um , one of them was something like uh , during the course of the transcript , um um , w we had gone through the part where everyone said which channel they were on and which device they were on , and um , the question was raised " Well , should we restart the recording at this point ? " And {disfmarker} and Dan Ellis said , " Well , we 're just so far ahead of the game right now {pause} we really don't need to " . Now , how would you interpret that without a lot of inference ? So , the inferences that are involved are things like , OK , so , how do you interpret " ahead of the game " ? You know . So it 's the {disfmarker} it 's {pause} i What you {disfmarker} what you int what you draw {disfmarker} you know , the conclusions that you need to draw are that space is involved in recording ,
Grad G: Hmm , metaphorically .
Postdoc B: that um , i that {pause} i we have enough space , and he continues , like " we 're so ahead of the game cuz now we have built - in downsampling " . So you have to sort of get the idea that um , " ahead of the game " is sp speaking with respect to space limitations , that um that in fact downsampling is gaining us enough space , and that therefore we can keep the recording we 've done so far . But there are a lot of different things like that .
Grad G: So , do you think his interest is in using this as {pause} a data source , or {pause} training material , or what ?
Professor E: Well , I {disfmarker} I should maybe interject to say this started off with a discussion that I had with him , so um we were trying to think of ways that his interests could interact with ours
Grad G: Mm - hmm .
Professor E: and um uh I thought that if we were going to project into the future when we had a lot of data , uh and um such things might be useful for that in or before we invested too much uh effort into that he should uh , with Jane 's help , look into some of the data that we 're {disfmarker} already have and see , is there anything to this at all ?
Grad G: Mm - hmm .
Professor E: Is there any point which you think that , you know , you could gain some advantage and some potential use for it . Cuz it could be that you 'd look through it and you say " well , this is just the wrong {pause} task for {disfmarker} for him to pursue his {disfmarker} "
Grad G: Wrong , yeah .
Professor E: And {disfmarker} and uh I got the impression from your mail that in fact there was enough things like this just in the little sample that {disfmarker} that you looked at that {disfmarker} that it 's plausible at least .
Postdoc B: It 's possible . Uh , he was {disfmarker} he {disfmarker} he {disfmarker} you know {disfmarker} We met and he was gonna go and uh you know , y look through them more systematically
Professor E: Yeah .
Postdoc B: and then uh meet again .
Professor E: Yeah .
Postdoc B: So it 's , you know , not a matter of a {disfmarker}
Professor E: Yeah .
Postdoc B: But , yeah , I think {disfmarker} I think it was optimistic .
Professor E: So anyway , that 's {disfmarker} that 's e a quite different thing from anything we 've talked about that , you know , might {disfmarker} might {disfmarker} might come out from some of this .
PhD C: But he can use text , basically . I mean , he 's talking about just using text
Postdoc B: That 's his major {disfmarker} I mentioned several that w had to do with implications drawn from intonational contours
PhD C: pretty much , or {disfmarker} ?
Postdoc B: and {pause} that wasn't as directly relevant to what he 's doing . He 's interested in these {disfmarker} these knowledge structures ,
PhD C: OK .
PhD D: Yeah , interesting .
Postdoc B: inferences that you draw {pause} i from {disfmarker}
Professor E: I mean , he certainly could use text , but we were in fact looking to see if there {disfmarker} is there {disfmarker} is there something in common between our interest in meetings and his interest in {disfmarker} in {disfmarker} in this stuff . So .
Grad G: And I imagine that transcripts of speech {disfmarker} I mean text that is speech {disfmarker} probably has more of those than sort of prepared writing . I {disfmarker} I don't know whether it would or not , but it seems like it would .
Professor E: I don't know , probably de probably depends on what the prepared writing was . But .
Postdoc B: Yeah , I don't think I would make that leap , because i in narratives , you know {disfmarker} I mean , if you spell out everything in a narrative , it can be really tedious ,
Grad G: Mm - hmm .
Postdoc B: so .
Grad G: Yeah , I 'm just thinking , you know , when you 're {disfmarker} when you 're face to face , you have a lot of backchannel and {disfmarker} And {disfmarker}
Postdoc B: Oh . That aspect .
Grad G: Yeah . And so I think it 's just easier to do that sort of broad inference jumping if it 's face to face . I mean , so , if I just read that Dan was saying " we 're ahead of the game " {comment} in that {disfmarker} in that context ,
Postdoc B: Well {disfmarker} Yeah .
Grad G: I might not realize that he was talking about disk space as opposed to anything else .
Postdoc B: I {disfmarker} you know , I {disfmarker} I had several that had to do with backchannels and this wasn't one of them .
Grad G: Uh - huh .
Postdoc B: This {disfmarker} this one really does um m make you leap from {disfmarker} So he said , you know , " we 're ahead of the game , w we have built - in downsampling " .
Grad G: Mm - hmm .
Postdoc B: And the inference , i if you had it written down , would be {disfmarker}
Grad G: I guess it would be the same .
Postdoc B: Uh - huh . But there are others that have backchannelling , it 's just he was less interested in those .
PhD F: Can I {disfmarker} Sorry to interrupt . Um , I f f f I 've {disfmarker} @ @ {comment} d A minute {disfmarker} uh , several minutes ago , I , like , briefly was {disfmarker} was not listening and {disfmarker} So who is " he " in this context ?
PhD C: Yeah , there 's a lot of pronoun {disfmarker}
PhD F: OK . So I was just realizing we 've {disfmarker} You guys have been talking about " he " um for at least uh , I don't know , three {disfmarker} three four minutes without ever mentioning the person 's name again .
PhD C: I believe it . Yeah . Actually to make it worse , {comment} uh , Morgan uses " you " and " you "
PhD F: So this is {disfmarker} this is {disfmarker} this is {disfmarker} gonna be a big , big problem if you want to later do uh , you know , indexing , or speech understanding of any sort .
Grad G: It 's in my notes .
PhD C: with gaze and no identification , or {disfmarker} I just wrote this down . Yeah , actually . Cuz Morgan will say well , " you had some ideas "
PhD D: Yeah .
PhD F: You just wrote this ?
PhD C: and he never said Li - He looked {disfmarker}
Grad G: Well , I think he 's doing that intentionally ,
PhD C: Right , so it 's great .
Grad G: aren't you ?
PhD C: So this is really great
PhD F: Right .
PhD C: because the thing is , because he 's looking at the per even for addressees in the conversation ,
PhD D: Yeah .
PhD F: Mm - hmm .
PhD C: I bet you could pick that up in the acoustics . Just because your gaze is also correlated with the directionality of your voice .
Professor E: Uh - huh . Could be .
Postdoc B: Can we
Professor E: Yeah . That would be tou
Grad G: Oh , that would be interesting .
PhD C: Yeah , so that , I mean , to even know um when {disfmarker}
PhD D: Yeah .
PhD C: Yeah , if you have the P Z Ms you should be able to pick up what a person is looking at from their voice .
Grad G: Well , especially with Morgan , with the way we have the microphones arranged . I 'm sort of right on axis and it would be very hard to tell .
PhD C: Right .
Grad G: Uh .
Postdoc B: Oh , but you 'd have the {disfmarker}
PhD C: Put Morgan always like this
Postdoc B: You 'd have fainter {disfmarker}
PhD C: and {disfmarker}
Postdoc B: Wouldn't you get fainter reception out here ?
Professor E: Well , these {disfmarker}
Grad G: Sure , but I think if I 'm talking like this ? Right now I 'm looking at Jane and talking , now I 'm looking at Chuck and talking , I don't think the microphones would pick up that difference .
PhD C: But you don't have this {disfmarker} this problem .
Postdoc B: I see .
PhD C: Morgan is the one who does this most .
Grad G: So if I 'm talking at you , or I 'm talking at you .
Professor E: I probably been affect No , I th I think I 've been affected by too many conversations where we were talking about lawyers and talking about {disfmarker} and concerns about " oh gee is somebody going to say something bad ? " and so on .
Grad G: Lawyers .
Professor E: And so I {disfmarker} so I 'm {disfmarker} I 'm tending to stay away from people 's names even though uh {disfmarker}
Postdoc B: I am too .
PhD C: Even though you could pick up later on , just from the acoustics who you were t who you were looking at .
Postdoc B: I am too .
Grad G: And we did mention who " he " was .
PhD C: Yeah .
Professor E: Yeah .
PhD F: Right , but I missed it .
Grad G: Early in the conversation .
PhD F: But {disfmarker} it was uh {disfmarker}
PhD C: Yeah , yeah .
Professor E: Yeah .
Grad G: Do {disfmarker} Sh - Can I say
Professor E: Yeah . No no , there 's {disfmarker}
PhD F: Yeah .
Grad G: or {disfmarker} or is that just too sensitive ?
Professor E: No no , it isn't sensitive at all .
Postdoc B: Well {disfmarker}
Professor E: I was just {disfmarker} I was just {disfmarker} I was overreacting just because we 've been talking about it .
Postdoc B: And in fact , it is {disfmarker} it is {disfmarker} it is sensitive .
PhD C: No , but that {disfmarker} it 's interesting .
Professor E: It 's OK to {disfmarker}
Postdoc B: I {disfmarker} I came up with something from the Human Subjects people that I wanted to mention . I mean , it fits into the m area of the mundane , but they did say {disfmarker} You know , I asked her very specifically about this clause of how , um , you know , it says " no individuals will be identified uh , " in any publication using the data . " OK , well , individuals being identified , let 's say you have a {disfmarker} a snippet that says , " Joe s uh thinks such - and - such about {disfmarker} about this field , but I think he 's wrongheaded . " Now I mean , we 're {disfmarker} we 're gonna be careful not to have the " wrongheaded " part in there , but {disfmarker} but you know , let 's say we say , you know , " Joe used to think so - and - so about this area , in his publication he says that but I think he 's changed his mind . " or whatever . Then the issue of {disfmarker} of being able to trace Joe , because we know he 's well - known in this field , and all this and {disfmarker} and tie it to the speaker , whose name was just mentioned a moment ago , can be sensitive .
Professor E: b But I {disfmarker}
Postdoc B: So I think it 's really {disfmarker} really kind of adaptive and wise to not mention names any more than we have to because if there 's a slanderous aspect to it , then how much to we wanna be able to have to remove ?
Professor E: Yeah , well , there 's that . But I {disfmarker} I mean I think also to some extent it 's just educating the Human Subjects people , in a way , because there 's {disfmarker} If uh {disfmarker} You know , there 's court transcripts , there 's {disfmarker} there 's transcripts of radio shows {disfmarker} I mean people say people 's names all the time . So I think it {disfmarker} it can't be bad to say people 's names . It 's just that {disfmarker} i I mean you 're right that there 's more poten If we never say anybody 's name , then there 's no chance of {disfmarker} of {disfmarker} of slandering anybody ,
PhD C: But , then it won't {disfmarker} I mean , if we {disfmarker} if we {disfmarker}
Professor E: but {disfmarker}
Grad G: It 's not a meeting .
PhD C: Yeah . I mean we should do whatever 's natural in a meeting if {disfmarker} if we weren't being recorded .
Professor E: Yeah . Right , so I {disfmarker} So my behavior is probably not natural .
PhD C: " If Person X {disfmarker} "
Professor E: So .
Postdoc B: Well , my feeling on it was that it wasn't really important who said it , you know .
Professor E: Yeah .
PhD F: Well , if you ha since you have to um go over the transcripts later anyway , you could make it one of the jobs of the {pause} people who do that to mark
Grad G: Well , we t we t we talked about this during the anon anonymization .
PhD F: Right .
Grad G: If we wanna go through and extract from the audio and the written every time someone says a name . And I thought that our conclusion was that we didn't want to do that .
Professor E: Yeah , we really can't . But a actually , I 'm sorry . I really would like to push {disfmarker} finish this off .
Postdoc B: I understand . No I just {disfmarker} I just was suggesting that it 's not a bad policy p potentially .
Professor E: So it 's {disfmarker}
Postdoc B: So , we need to talk about this later .
Professor E: Yeah , I di I didn't intend it an a policy though .
Postdoc B: Uh - huh .
Professor E: It was {disfmarker} it was just it was just unconscious {disfmarker} well , semi - conscious behavior . I sorta knew I was doing it but it was {disfmarker}
PhD F: Well , I still don't know who " he " is .
Professor E: I {disfmarker} I do I don't remember who " he " is .
PhD C: No , you have to say , you still don't know who " he " is , with that prosody .
Professor E: Ah . Uh , we were talking about Dan at one point {comment} and we were talking about Lokendra at another point .
Postdoc B: Yeah , depends on which one you mean .
Professor E: And I don't {disfmarker} I don't remember which {disfmarker} which part .
PhD F: Oh .
PhD C: It 's ambiguous , so it 's OK .
Professor E: Uh , I think {disfmarker}
Grad G: Well , the inference structures was Lokendra .
PhD F: But no . The inference stuff was {disfmarker} was {disfmarker} was Lokendra .
Professor E: Yeah . Yeah . Yeah .
PhD F: OK . That makes sense , yeah .
PhD C: And the downsampling must have been Dan .
Professor E: Um {disfmarker}
Grad G: Yeah .
Professor E: Good {disfmarker} Yeah .
PhD C: It 's an inference .
Professor E: Yeah , you could do all these inferences , yeah .
Grad G: Yeah .
Professor E: Yeah . Um , I {disfmarker} I would like to move it into {disfmarker} into uh what Jose uh has been doing
Postdoc B: Yeah .
Professor E: because he 's actually been doing something .
PhD D: Uh - huh . OK .
Professor E: So . {vocalsound} Right .
PhD F: As opposed to the rest of us .
PhD D: Well - {comment} {vocalsound} OK . I {disfmarker} I remind that me {disfmarker} my first objective eh , in the project is to {disfmarker} to study difference parameters to {disfmarker} to find a {disfmarker} a good solution to detect eh , the overlapping zone in eh speech recorded . But eh , {vocalsound} tsk , {comment} {vocalsound} ehhh {comment} In that way {comment} I {disfmarker} {vocalsound} I {disfmarker} {vocalsound} I begin to {disfmarker} to study and to analyze the ehn {disfmarker} the recorded speech eh the different session to {disfmarker} to find and to locate and to mark eh the {disfmarker} the different overlapping zone . And eh so eh I was eh {disfmarker} I am transcribing the {disfmarker} the first session and I {disfmarker} I have found eh , eh one thousand acoustic events , eh besides the overlapping zones , eh I {disfmarker} I {disfmarker} I mean the eh breaths eh aspiration eh , eh , talk eh , eh , clap , eh {disfmarker} {comment} I don't know what is the different names eh you use to {disfmarker} to name the {disfmarker} the {pause} n speech
PhD A: Nonspeech sounds ?
PhD D: Yeah .
Grad G: Oh , I don't think we 've been doing it at that level of detail . So .
PhD D: Yeah . Eh , {vocalsound} I {disfmarker} I {disfmarker} I do I don't need to {disfmarker} to {disfmarker} to mmm {vocalsound} {disfmarker} to m to label the {disfmarker} the different acoustic , but I prefer because eh I would like to {disfmarker} to study if eh , I {disfmarker} I will find eh , eh , a good eh parameters eh to detect overlapping I would like to {disfmarker} to {disfmarker} to test these parameters eh with the {disfmarker} another eh , eh acoustic events , to nnn {disfmarker} {vocalsound} to eh {disfmarker} to find what is the ehm {disfmarker} the false {disfmarker} eh , the false eh hypothesis eh , nnn , which eh are produced when we use the {disfmarker} the ehm {disfmarker} this eh parameter {disfmarker} eh I mean pitch eh , eh , difference eh , feature {disfmarker}
Grad G: Mm - hmm .
PhD A: You know {disfmarker} I think some of these um that are the nonspeech overlapping events may be difficult even for humans to tell that there 's two there .
Grad G: So it was {disfmarker}
PhD D: Yeah .
PhD A: I mean , if it 's a tapping sound , you wouldn't necessarily {disfmarker} or , you know , something like that , it 'd be {disfmarker} it might be hard to know that it was two separate events .
PhD D: Yeah . Yeah . Yeah . Yeah .
Grad G: Well {disfmarker} You weren't talking about just overlaps
PhD D: Ye
Grad G: were you ? You were just talking about acoustic events .
PhD D: I {disfmarker} I {disfmarker} I {disfmarker} I t I t I talk eh about eh acoustic events in general ,
Grad G: Someone starts , someone stops {disfmarker} Yeah .
PhD A: Oh .
PhD D: but eh my {disfmarker} my objective eh will be eh to study eh overlapping zone .
Grad G: Mm - hmm .
PhD D: Eh ? {comment} n Eh in twelve minutes I found eh , eh one thousand acoustic events .
Professor E: How many overlaps were there uh in it ? No no , how many of them were the overlaps of speech , though ?
PhD D: How many ? Eh almost eh three hundred eh in one session
Grad G: Oh , God !
PhD D: in five {disfmarker} eh in forty - five minutes .
PhD A: Three hundred overlapping speech {disfmarker}
PhD D: Alm - Three hundred overlapping zone .
Grad G: Ugh .
PhD C: Overlapping speech .
PhD D: With the overlapping zone , overlapping speech {disfmarker} speech what eh different duration .
PhD A: Mm - hmm .
Professor E: Sure .
Postdoc B: Does this {disfmarker} ? So if you had an overlap involving three people , how many times was that counted ?
PhD D: Yeah , three people , two people . Eh , um I would like to consider eh one people with difference noise eh in the background , be
Professor E: No no , but I think what she 's asking is {pause} if at some particular for some particular stretch you had three people talking , instead of two , did you call that one event ?
PhD D: Oh . Oh . Yeah . I consider one event eh for th for that eh for all the zone . This {disfmarker} th I {disfmarker} I {disfmarker} I con I consider {disfmarker} I consider eh an acoustic event , the overlapping zone , the period where three speaker or eh {disfmarker} are talking together .
Grad G: Well {disfmarker} So let 's {disfmarker}
Postdoc B: For
Grad G: So let 's say me and Jane are talking at the same time , and then Liz starts talking also over all of us . How many events would that be ?
PhD D: So - I don't understand .
Grad G: So , two people are talking , {comment} and then a third person starts talking .
PhD D: Yeah ?
Grad G: Is there an event right here ?
PhD D: Eh no . No no . For me is the overlapping zone , because {disfmarker} because you {disfmarker} you have s you have more one {disfmarker} eh , more one voice eh , eh produced in a {disfmarker} in {disfmarker} in a moment .
Professor E: I see .
Grad G: So i if two or more people are talking .
Professor E: OK . Yeah . So I think {disfmarker} Yeah . We just wanted to understand how you 're defining it .
PhD D: Yeah . If
Professor E: So then , in the region between {disfmarker} since there {disfmarker} there is some continuous region , in between regions where there is only one person speaking .
PhD D: Uh - huh .
Professor E: And one contiguous region like that you 're calling an event .
PhD D: Uh - huh .
Professor E: Is it {disfmarker} Are you calling the beginning or the end of it the event ,
PhD D: Yeah .
Professor E: or are you calling the entire length of it the event ?
PhD D: I consider the {disfmarker} the , nnn {disfmarker} the nnn , nnn {disfmarker} eh , the entirety eh , eh , all {disfmarker} all the time there were {disfmarker} the voice has overlapped .
Professor E: OK .
PhD D: This is the idea . But eh I {disfmarker} I don't distinguish between the {disfmarker} the numbers of eh speaker . Uh , I 'm not considering {vocalsound} eh the {disfmarker} the {disfmarker} ehm {vocalsound} eh , the fact of eh , eh , for example , what did you say ? Eh at first eh , eh two talkers are uh , eh speaking , and eh , eh a third person eh join to {disfmarker} to that . For me , it 's eh {disfmarker} it 's eh , all overlap zone , with eh several numbers of speakers is eh , eh the same acoustic event . Wi - but {disfmarker} uh , without any mark between the zone {disfmarker} of the overlapping zone with two speakers eh speaking together , and the zone with the three speakers .
Postdoc B: That would j just be one .
PhD D: It {disfmarker} One . One .
Postdoc B: OK .
PhD D: Eh , with eh , a beginning mark and the ending mark . Because eh {vocalsound} for me , is the {disfmarker} is the zone with eh some kind of eh distortion the spectral .
Professor E: Got it .
PhD D: I don't mind {disfmarker} By the moment , by the moment .
Grad G: Well , but {disfmarker} But you could imagine that three people talking has a different spectral characteristic than two .
PhD D: I {disfmarker} I don't {disfmarker} Yeah , but eh {disfmarker} but eh I have to study . {comment} What will happen in a general way ,
Professor E: Could .
Grad G: So . You had to start somewhere .
Professor E: Yeah . We just w
PhD C: So there 's a lot of overlap .
PhD D: I {disfmarker} {vocalsound} I don't know what eh will {disfmarker} will happen with the {disfmarker}
Grad G: Yep .
PhD C: So .
Grad G: That 's a lot of overlap ,
PhD D: Yeah ?
Professor E: So again , that 's {disfmarker} that 's three {disfmarker} three hundred in forty - five minutes that are {disfmarker} that are speakers , just speakers .
Grad G: yeah , for forty - five minutes .
PhD D: Yeah . Yeah .
Professor E: Uh - huh . OK . Yeah .
Postdoc B: But a {disfmarker} a {disfmarker} a th
Professor E: So that 's about eight per minute .
Postdoc B: But a thousand events in twelve minutes , that 's {disfmarker}
PhD D: Yeah , {pause} but {disfmarker} Yeah .
PhD C: But that can include taps .
PhD D: But {disfmarker}
Professor E: Uh . Yeah .
Postdoc B: Well , but a thousand taps in eight minutes is a l in twelve minutes is a lot .
PhD D: General .
PhD C: Actually {disfmarker}
PhD D: I {disfmarker} I con I consider {disfmarker} I consider acoustic events eh , the silent too .
Postdoc B: Silent .
Grad G: Silence starting or silence ending {disfmarker}
PhD D: Yeah , silent , ground to {disfmarker} bec to detect {disfmarker} eh because I consider acoustic event all the things are not eh speech .
PhD C: Oh , OK .
Professor E: Mm - hmm .
PhD A: Oh .
PhD D: In ge in {disfmarker} in {disfmarker} in a general point of view .
PhD C: Oh .
Professor E: OK , so how many of those thousand were silence ?
PhD C: Alright .
PhD D: in the per
PhD F: Not speech {disfmarker} not speech or too much speech .
PhD D: Too much speech .
Professor E: Right . So how many of those thousand were silence , silent sections ?
PhD D: Yeah . Uh silent , I {disfmarker} I {disfmarker} I {disfmarker} I don't {disfmarker} I {disfmarker} I haven't the {disfmarker} eh I {disfmarker} I would like to {disfmarker} to do a stylistic study
Professor E: Yeah .
PhD D: and give you eh with the report eh from eh the {disfmarker} the study from the {disfmarker} the {disfmarker} the session {disfmarker} one session .
Professor E: Yeah . Yeah .
PhD D: And I {disfmarker} I found that eh another thing . When eh {vocalsound} eh I w I {disfmarker} {vocalsound} I was eh look at eh nnn , the difference speech file , um , for example , eh if eh we use the ehm {disfmarker} the mixed file , to {disfmarker} to transcribe , the {disfmarker} the events and the words , I {disfmarker} I saw that eh the eh speech signal , collected by the eh this kind of mike {disfmarker} eh of this kind of mike , eh are different from the eh mixed signal eh , we eh {disfmarker} collected by headphone .
Grad G: Yep .
PhD D: And {disfmarker} It 's right .
Professor E: Yeah .
Grad G: Right .
PhD D: But the problem is {vocalsound} the following . The {disfmarker} the {disfmarker} the {disfmarker} I {disfmarker} I {disfmarker} I knew that eh the signal eh , eh would be different , but eh the {disfmarker} the problem is eh , eh we eh detected eh difference events in the speech file eh collected by {disfmarker} by that mike uh qui compared with the mixed file . And so if {disfmarker} when you transcribe eh only eh using the nnn {disfmarker} the mixed file , it 's possible {disfmarker} eh if you use the transcription to evaluate a different system , it 's possible you eh {disfmarker} in the eh i and you use the eh speech file collected by the eh fet mike , to eh {disfmarker} to nnn {disfmarker} to do the experiments {pause} with the {disfmarker} the system ,
Professor E: Mm - hmm .
Grad G: Right .
PhD D: its possible to evaluate eh , eh {disfmarker} or to consider eh acoustic events that {disfmarker} which you marked eh in the mixed file , but eh they don't appear in the eh speech signal eh collected by the {disfmarker} by the mike .
Grad G: Right . The {disfmarker} the reason that I generated the mixed file was for IBM to do word level transcription , not speech event transcription .
PhD D: Yeah . Yeah . Oh , it 's a good idea . It 's a good idea I think .
Grad G: So I agree that if someone wants to do speech event transcription , that the mixed signals here {disfmarker}
PhD D: Yeah .
Grad G: I mean , if I 'm tapping on the table , you it 's not gonna show up on any of the mikes , but it 's gonna show up rather loudly in the PZM .
PhD D: Yeah . Yeah . Yeah . So and I {disfmarker} I {disfmarker} {vocalsound} I say eh that eh , eh , or this eh only because eh I c I {disfmarker} I {disfmarker} {vocalsound} in my opinion , it 's necessary to eh {disfmarker} to eh {disfmarker} to put the transcription on the speech file , collected by the objective signal .
Grad G: So .
PhD D: I mean the {disfmarker} the {disfmarker} the signal collected by the {disfmarker} eh , the real mike in the future , in the prototype to {disfmarker} to eh correct the initial eh segmentation eh with the eh real speech
Professor E: Mm - hmm . The {disfmarker} the {disfmarker} the far - field , yeah .
PhD D: you have to {disfmarker} to analyze {disfmarker} you have to {disfmarker} to process . Because I {disfmarker} I found a difference .
Professor E: Yeah , well , just {disfmarker} I mean , just in that {disfmarker} that one s ten second , or whatever it was , example that Adam had that {disfmarker} that we {disfmarker} we passed on to others a few months ago , there was that business where I g I guess it was Adam and Jane were talking at the same time and {disfmarker} and uh , in the close - talking mikes you couldn't hear the overlap , and in the distant mike you could . So yeah , it 's clear that if you wanna study {disfmarker} if you wanna find all the places where there were overlap , it 's probably better to use a distant mike .
PhD F: That 's good .
Professor E: On the other hand , there 's other phenomena that are going on at the same time for which it might be useful to look at the close - talking mikes ,
PhD D: Yeah .
PhD C: But why can't you use the combination of the close - talking mikes , time aligned ?
Professor E: so it 's {disfmarker}
Grad G: If you use the combination of the close - talking mikes , you would hear Jane interrupting me , but you wouldn't hear the paper rustling . And so if you 're interested in {disfmarker}
PhD C: I {disfmarker} I mean if you 're interested in speakers overlapping other speakers and not the other kinds of nonspeech , that 's not a problem ,
Professor E: Some {comment} of it 's masking {disfmarker} masked .
PhD D: Yeah .
PhD A: Were you interrupting him or was he interrupting you ?
Professor E: Right .
PhD C: right ?
Grad G: Right .
PhD D: Yeah .
Grad G: Although the other issue is that the {pause} mixed close - talking mikes {disfmarker} I mean , I 'm doing weird normalizations and things like that .
PhD C: But it 's known .
PhD D: Yeah .
PhD C: I mean , the normalization you do is over the whole conversation
Grad G: Yep .
PhD C: isn't it , over the whole meeting .
Grad G: Right . Yep .
PhD C: So if you wanted to study people overlapping people , that 's not a problem .
PhD D: I {disfmarker} I {disfmarker} I think eh I saw the nnn {disfmarker} the {disfmarker} eh but eh I eh {disfmarker} I have eh any results . I {disfmarker} I {disfmarker} I saw the {disfmarker} the speech file collected by eh the fet mike , and eh eh signal eh to eh {disfmarker} to noise eh relation is eh low . It 's low .
Professor E: Mm - hmm .
PhD D: It 's very low . You would comp if we compare it with eh the headphone .
Grad G: Yep .
PhD D: And I {disfmarker} I found that nnn {disfmarker} that eh , {vocalsound} ehm , pr probably ,
Grad G: Did {disfmarker} Did you
PhD D: I 'm not sure eh by the moment , but it 's {disfmarker} it 's probably that eh a lot of eh , {vocalsound} eh for example , in the overlapping zone , on eh {disfmarker} in {disfmarker} in several eh parts of the files where you {disfmarker} you can find eh , eh {vocalsound} eh , smooth eh eh speech eh from eh one eh eh talker in the {disfmarker} in the meeting ,
Professor E: Mm - hmm . Mm - hmm .
PhD D: it 's probably in {disfmarker} in that eh {disfmarker} in {disfmarker} in those files you {disfmarker} you can not find {disfmarker} you can not process because eh it 's confused with {disfmarker} with noise .
Professor E: Mm - hmm .
PhD D: And there are {vocalsound} a lot of I think . But I have to study with more detail . But eh my idea is to {disfmarker} to process only {pause} nnn , this eh {disfmarker} nnn , this kind of s of eh speech . Because I think it 's more realistic . I 'm not sure it 's a good idea , but eh {disfmarker}
Professor E: No {disfmarker} i
Grad G: Well , it 's more realistic but it 'll {disfmarker} it 'll be a lot harder .
PhD D: Yeah .
Professor E: Well , it 'd be hard , but on the other hand as you point out , if your {disfmarker} if i if {disfmarker} if your concern is to get uh the overlapping people {disfmarker} people 's speech , you will {disfmarker} you will get that somewhat better .
PhD D: Mm - hmm . Yeah .
Professor E: Um , Are you making any use {disfmarker} uh you were {disfmarker} you were working with th the data that had already been transcribed .
PhD D: With {disfmarker} By Jane .
Professor E: Does it uh {disfmarker} Yes .
PhD D: Yeah .
Professor E: Now um did you make any use of that ? See I was wondering cuz we st we have these ten hours of other stuff that is not yet transcribed .
PhD D: Yeah . Yeah .
Professor E: Do you {disfmarker}
PhD D: The {disfmarker} the transcription by Jane , t eh i eh , I {disfmarker} I {disfmarker} I want to use to {disfmarker} to nnn , {vocalsound} eh to put {disfmarker} i i it 's a reference for me . But eh the transcription {disfmarker} eh for example , I {disfmarker} I don't {disfmarker} I {disfmarker} I 'm not interested in the {disfmarker} in the {disfmarker} in the words , transcription words , eh transcribed eh eh in {disfmarker} eh follow in the {disfmarker} {vocalsound} in the {disfmarker} in the speech file , but eh eh Jane eh for example eh put a mark eh at the beginning eh of each eh talker , in the {disfmarker} in the meeting , um eh she {disfmarker} she nnn includes information about the zone where eh there are eh {disfmarker} there is an overlapping zone . But eh there isn't any {disfmarker} any mark , time {disfmarker} temporal mark , to {disfmarker} to c eh {disfmarker} to mmm {vocalsound} {disfmarker} e - heh , to label {comment} the beginning and the end of the {disfmarker} of the
Professor E: Mm - hmm . OK . Right , so she is {disfmarker}
PhD D: ta I 'm {disfmarker} I {disfmarker} I {disfmarker} I think eh we need this information to
Professor E: Right . So the twelve {disfmarker} you {disfmarker} you {disfmarker} it took you twelve hours {disfmarker} of course this included maybe some {disfmarker} some time where you were learning about what {disfmarker} what you wanted to do , but {disfmarker} but uh , it took you something like twelve hours to mark the forty - five minutes , your
Grad G: Twelve minutes .
PhD D: Twelve minutes .
Professor E: s Twelve minutes !
PhD D: Twelve minutes . Twelve .
Professor E: I thought you did forty - five minutes of {disfmarker}
PhD D: No , forty - five minutes is the {disfmarker} is the session , all the session .
Postdoc B: Oh .
Professor E: Oh , you haven't done the whole session .
PhD D: Yeah , all is the {vocalsound} the session .
Professor E: This is just twelve minutes .
PhD D: Tw - twelve hours of work to {disfmarker} {vocalsound} to segment eh and label eh twelve minutes from a session of part {disfmarker} of f
Professor E: Oh . So {comment} let me back up again . So the {disfmarker} when you said there were three hundred speaker overlaps ,
PhD D: Yeah .
Professor E: that 's in twelve minutes ?
PhD D: No no no . I {disfmarker} I consider all the {disfmarker} all the session because eh I {disfmarker} I count the nnn {disfmarker} the nnn {disfmarker} the overlappings marked by {disfmarker} by Jane ,
Professor E: Oh , OK .
Postdoc B: Oh , I see .
PhD D: in {disfmarker} in {disfmarker} in {disfmarker} in the {pause} fin in {disfmarker} in the {pause} forty - five minutes .
Professor E: OK . So it 's three hundred in forty - five minutes , but you have {disfmarker} you have time uh , uh marked {disfmarker} twelve minute {disfmarker} the {disfmarker} the {disfmarker} the um overlaps in twelve minutes of it .
PhD D: Yeah .
Professor E: Got it .
PhD F: So , can I ask {disfmarker} {vocalsound} can I ask whether you found {disfmarker} uh , you know , how accurate uh Jane 's uh uh labels were as far as {disfmarker}
Grad G: Well , not just the overlaps , everything .
PhD F: you know , did she miss some overlaps ? or did she n ?
PhD D: But , by {disfmarker} by the moment , I {disfmarker} I don't compare , my {disfmarker} my temporal mark with eh Jane , but eh I {disfmarker} I want to do it . Because eh eh i per perhaps I have eh errors in the {disfmarker} in the marks , I {disfmarker} and if I {disfmarker} I compare with eh Jane , it 's probably I {disfmarker} I {disfmarker} I can correct and {disfmarker} and {disfmarker} and {disfmarker} to get eh eh a more accurately eh eh transcription in the file .
Professor E: Yeah .
Grad G: Well , also Jane {disfmarker} Jane was doing word level .
PhD D: Yeah .
Professor E: Yeah .
Grad G: So we weren't concerned with {comment} exactly when an overlap started and stopped .
PhD F: Right . Right .
PhD C: Well , not only a word level , but actually
PhD D: Well {disfmarker}
PhD F: I 'm expect I 'm not expecting {disfmarker}
PhD D: No , it 's {disfmarker}
PhD C: I mean , you didn't need to show the exact point of interruption , you just were showing at the level of the phrase or the level of the speech spurt , or {disfmarker}
Grad G: Right .
Professor E: Mm - hmm .
Grad G: Yep .
Postdoc B: Well {disfmarker}
PhD D: Yeah . Yeah .
Postdoc B: Well , yeah , b yeah , I would say time bin . So my {disfmarker} my goal is to get words with reference to a time bin , {pause} beginning and end point .
PhD C: Yeah .
PhD D: Yeah .
PhD C: Right .
PhD D: Yeah .
Postdoc B: And {disfmarker} and sometimes , you know , it was like you could have an overlap where someone said something in the middle ,
PhD D: Yeah .
Postdoc B: but , yeah , w it just wasn't important for our purposes to have it that {disfmarker} i disrupt that unit in order to have , you know , a the words in the order in which they were spoken , it would have {disfmarker} it would have been hard with the interface that we have .
PhD D: Yeah .
Postdoc B: Now , my {disfmarker} a Adam 's working on a of course , on a revised overlapping interface ,
PhD D: Uh - huh .
Grad G: Right .
PhD D: I {disfmarker} I {disfmarker} I think {disfmarker} It 's {disfmarker} it 's a good eh work ,
Postdoc B: but {disfmarker}
PhD D: but eh I think we need eh eh more information .
PhD F: No , of course .
Postdoc B: Yeah .
PhD F: I expect you to find more overlaps than {disfmarker} than Jane
Grad G: Always need more for {disfmarker}
Postdoc B: Yeah .
PhD D: No , no . I {disfmarker} I have to go to {disfmarker}
PhD F: because you 're looking at it at a much more detailed level .
PhD D: I want eh {disfmarker} I wanted to eh compare the {disfmarker} the transcription .
Professor E: I have {disfmarker}
Grad G: But if it takes sixty to one {disfmarker}
Professor E: Well , I but I have a suggestion about that . Um , obviously this is very , very time - consuming , and you 're finding lots of things which I 'm sure are gonna be very interesting , but in the interests of making progress , uh might I s how {disfmarker} how would it affect your time if you only marked speaker overlaps ?
PhD D: Only .
Professor E: Yes .
PhD D: Yeah .
Professor E: Do not mark any other events ,
PhD D: Uh - huh .
Professor E: but only mark speaker {disfmarker} Do you think that would speed it up quite a bit ?
PhD D: OK . OK . I {disfmarker} I {disfmarker} I {disfmarker} I w I {disfmarker} I wanted to {disfmarker}
Professor E: Do y do you think that would speed it up ? Uh , speed up your {disfmarker} your {disfmarker} your marking ?
PhD D: nnn , I don't understand very .
Professor E: It took you a long time {pause} to mark twelve minutes .
PhD D: Yeah . Oh , yeah , yeah .
Professor E: Now , my suggestion was for the other thirty - three {disfmarker}
PhD D: On - only to mark {disfmarker} only to mark overlapping zone , but {disfmarker}
Professor E: Yeah , and my question is , if you did that , if you followed my suggestion , would it take much less time ?
PhD D: Oh , yeah . Sure .
Professor E: Yeah OK .
PhD D: Yeah sure .
Professor E: Then I think it 's a good idea .
PhD D: Sure sure .
Professor E: Then I think it 's a good idea , because it
PhD D: Sure , because I {disfmarker} I need a lot of time to {disfmarker} to put the label or to do that . Yeah .
Professor E: Yeah , I mean , we we know that there 's noise .
Grad G: And
PhD D: Uh - huh .
Professor E: There 's {disfmarker} there 's uh continual noise uh from fans and so forth , and there is uh more impulsive noise from uh taps and so forth
PhD D: Yeah .
Professor E: and {disfmarker} and something in between with paper rustling . We know that all that 's there and it 's a g worthwhile thing to study , but obviously it takes a lot of time to mark all of these things .
PhD D: Yeah .
Professor E: Whereas th i I would think that uh you {disfmarker} we can study more or less as a distinct phenomenon the overlapping of people talking .
PhD D: Uh - huh . OK . OK .
Professor E: So . Then you can get the {disfmarker} Cuz you need {disfmarker} If it 's three hundred uh {disfmarker} i i it sounds like you probably only have fifty or sixty or seventy events right now that are really {disfmarker}
PhD D: Yeah .
Professor E: And {disfmarker} and you need to have a lot more than that to have any kind of uh even visual sense of {disfmarker} of what 's going on , much less any kind of reasonable statistics .
Grad G: Right .
PhD C: Now , why do you need to mark speaker overlap by hand if you can infer it from the relative energy in the {disfmarker}
Grad G: Well , that 's {disfmarker} That 's what I was gonna bring up .
PhD C: I mean , you shouldn't need to do this p completely by hand ,
Professor E: Um , OK , yeah . So let 's back up because you weren't here for an earlier conversation .
PhD C: right ? I 'm sorry .
Professor E: So the idea was that what he was going to be doing was experimenting with different measures such as the increase in energy , such as the energy in the LPC residuals , such as {disfmarker} I mean there 's a bunch of things {disfmarker} I mean , increased energy is - is sort of an obvious one .
PhD C: Mm - hmm . In the far - field mike .
Professor E: Yeah .
PhD C: Oh , OK .
Professor E: Um , and uh , it 's not obvious , I mean , you could {disfmarker} you could do the dumbest thing and get {disfmarker} get it ninety percent of the time . But when you start going past that and trying to do better , it 's not obvious what combination of features is gonna give you the {disfmarker} you know , the right detector . So the idea is to have some ground truth first . And so the i the idea of the manual marking was to say " OK this , i you know , it 's {disfmarker} it 's really here " .
PhD A: But I think Liz is saying why not get it out of the transcripts ?
PhD C: What I mean is {pause} get it from the close - talking mikes .
Professor E: Uh , yeah .
PhD C: A or ge get a first pass from those ,
Professor E: We t we t w we t we talked about that .
PhD C: and then go through sort of {disfmarker} It 'd be a lot faster probably to {disfmarker}
PhD F: And you can {disfmarker}
Grad G: Yeah , that 's his , uh {disfmarker}
Professor E: We {disfmarker} we {disfmarker} we talked about that . s But so it 's a bootstrapping thing and the thing is ,
PhD C: Yeah , I just {disfmarker}
Professor E: the idea was , i we i i we thought it would be useful for him to look at the data anyway , and {disfmarker} and then whatever he could mark would be helpful ,
PhD C: Right .
Professor E: and we could {disfmarker} Uh it 's a question of what you bootstrap from . You know , do you bootstrap from a simple measurement which is right most of the time and then you g do better , or do you bootstrap from some human being looking at it and then {disfmarker} then do your simple measurements , uh from the close - talking mike . I mean , even with the close - talking mike you 're not gonna get it right all the time .
PhD C: Well , that 's what I wonder , because um {disfmarker} or how bad it is ,
Professor E: Well
PhD C: be um , because that would be interesting
Grad G: I 'm working on a program to do that , and {disfmarker}
PhD C: especially because the bottleneck is the transcription . Right ? I mean , we 've got a lot more data than we have transcriptions for . We have the audio data , we have the close - talking mike ,
Professor E: Yeah .
PhD C: so I mean it seems like one kind of project that 's not perfect , but {disfmarker} um , that you can get the training data for pretty quickly is , you know , if you infer form the close - talking mikes where the on - off points are of speech ,
Professor E: Right , we discussed that .
PhD C: you know , how can we detect that from a far - field ?
Grad G: And {disfmarker}
Postdoc B: Oh .
Grad G: I 've {disfmarker} I 've written a program to do that ,
PhD C: OK , I 'm sorry I missed the {disfmarker}
Grad G: and it , uh {disfmarker}
Professor E: It 's OK .
Grad G: and {disfmarker} so {disfmarker} but it 's {disfmarker} it 's doing something very , very simple . It just takes a threshold , based on {disfmarker} on the volume ,
PhD C: Uh - huh .
PhD F: Or you can set the threshold low and then weed out the false alarms by hand .
PhD C: Right , by hand . Yeah .
PhD F: Yeah .
Grad G: um , and then it does a median filter , and then it looks for runs . And , it seems to work , I 've {disfmarker} I 'm sort of fiddling with the parameters , to get it to actually generate something , and I haven't {disfmarker} I don't {disfmarker} what I 'm working on {disfmarker} was working on {disfmarker} was getting it to a form where we can import it into the user interface that we have , {pause} into Transcriber . And so {disfmarker} I told {disfmarker} I said it would take about a day . I 've worked on it for about half a day ,
Grad H: I have to go .
Grad G: so give me another half day and I we 'll have something we can play with .
PhD C: OK .
Professor E: See , this is where we really need the Meeting Recorder query stuff to be working , because we 've had these meetings and we 've had this discussion about this , and I 'm sort of remembering a little bit about what we decided ,
PhD C: Right . I 'm sorry . I just {disfmarker}
Professor E: but I couldn't remember all of it .
PhD C: It
Professor E: So , I think it was partly that , you know , give somebody a chance to actually look at the data and see what these are like , partly that we have e some ground truth to compare against , you know , when {disfmarker} when he {disfmarker} he gets his thing going ,
Grad G: But {disfmarker}
Professor E: uh , and {disfmarker}
PhD C: Well , it 's definitely good to have somebody look at it . I was just thinking as a way to speed up you know , the amount of {disfmarker}
Postdoc B: Mm - hmm .
Professor E: That was {disfmarker} that was exactly the notion that {disfmarker} that {disfmarker} that we discussed .
PhD C: OK .
Grad G: Thanks .
Postdoc B: Another thing we discussed was um that {disfmarker}
PhD C: It looks good .
Professor E: So .
PhD C: I 'll be in touch . Thanks .
Professor E: S See ya . Yeah .
Postdoc B: Was that um there m {pause} there was this already a script I believe uh that Dan had written , {comment} that uh handle bleedthrough , I mean cuz you have this {disfmarker} this close {disfmarker} you have contamination from other people who speak loudly .
Grad G: Yeah , and I haven't tried using that . It would probably help the program that I 'm doing to first feed it through that . It 's a cross - correlation filter . So I {disfmarker} I haven't tried that , but that {disfmarker} If {disfmarker} It {disfmarker} it might be something {disfmarker} it might be a good way of cleaning it up a little .
Postdoc B: So , some thought of maybe having {disfmarker} Yeah , having that be a preprocessor and then run it through yours .
Grad G: Exactly . Yep .
Professor E: But {disfmarker} but that 's a refinement
Postdoc B: That 's what we were discussing .
Professor E: and I think we wanna see {disfmarker} try the simple thing first , cuz you add this complex thing up uh afterwards that does something good y y yo you sort of wanna see what the simple thing does first .
Grad G: Yep .
Professor E: But uh , having {disfmarker} having somebody have some experience , again , with {disfmarker} with uh {disfmarker} with marking it from a human standpoint , we 're {disfmarker} I mean , I don't expect Jose to {disfmarker} to do it for uh f fifty hours of {disfmarker} {comment} of speech , but I mean we {disfmarker} {comment} if uh {disfmarker} if he could speed up what he was doing by just getting the speaker overlaps so that we had it , say , for forty - five minutes , then at least we 'd have three hundred examples of it .
PhD D: Yeah . Sure . Sure .
Professor E: And when {disfmarker} when uh Adam was doing his automatic thing he could then compare to that and see what it was different .
PhD C: Oh yeah , definitely .
PhD A: You know , I did {disfmarker} I did uh something almost identical to this at one of my previous jobs , and it works pretty well . I mean , i almost exactly what you described , an energy detector with a median filter , you look for runs . And uh , you know , you can {disfmarker}
Grad G: It seemed like the right thing to do .
PhD A: Yeah . I mean , you {disfmarker} you can get y I mean , you get them pretty close .
Grad G: That was with zero literature search .
PhD A: And so I think doing that to generate these possibilities and then going through and saying yes or no on them would be a quick way to {disfmarker} to do it .
Grad G: That 's good validation .
PhD A: Yeah .
Postdoc B: Is this proprietary ?
PhD A: Uh . {comment} No . No .
Grad G: Yeah , do you have a patent on it ?
PhD A: It was when I was working for the government .
Professor E: Oh , then everybody owns it . It 's the people .
Postdoc B: Well , I mean , is this something that we could just co - opt , or is it {disfmarker} ?
PhD A: Nah .
Postdoc B: No . OK .
Professor E: Well , i i i he 's pretty close , anyway . I think {disfmarker} I think it 's {disfmarker}
PhD A: Yeah , he 's {disfmarker} it {disfmarker} it doesn't take a long time .
Postdoc B: Right . I just thought if it was tried and true , then {disfmarker} {comment} and he 's gone through additional levels of {disfmarker} of development .
Grad G: Just output . Although if you {disfmarker} if you have some parameters like what 's a good window size for the median filter {disfmarker}
PhD A: Oh ! {comment} I have to remember . I 'll think about it , and try to remember .
PhD F: And it might be different for government people .
Grad G: That 's alright .
Professor E: Yeah , good enough for government work , as they say .
PhD C: They {disfmarker} they {disfmarker}
PhD A: Di - dif different {disfmarker} different bandwidth .
PhD F: They
Grad G: I was doing pretty short , you know , tenth of a second , {comment} sorts of numbers .
PhD F: OK .
Professor E: Uh , I don't know , it {disfmarker} if {disfmarker} if we want to uh {disfmarker} So , uh , maybe we should move on to other {disfmarker} other things in limited time .
Postdoc B: Can I ask one question about his statistics ? So {disfmarker} so in the tw twelve minutes , um , if we took three hundred and divided it by four , which is about the length of twelve minutes , i Um , I 'd expect like there should be seventy - five overlaps .
Professor E: Yeah .
Postdoc B: Did you find uh more than seventy - five overlaps in that period , or {disfmarker} ?
PhD D: More than ?
Postdoc B: More than {disfmarker} How many overlaps in your twelve minutes ?
PhD D: How many ? Eh , not @ @ I Onl - only I {disfmarker} I transcribe eh only twelve minutes from the
Professor E: Yeah .
PhD D: but eh I {disfmarker} I don't co eh {disfmarker} I don't count eh the {disfmarker} the overlap .
Postdoc B: The overlaps . OK .
PhD D: I consider I {disfmarker} I {disfmarker} The {disfmarker} the nnn {disfmarker} The {disfmarker} the three hundred is eh considered only you {disfmarker} your transcription . I have to {disfmarker} {vocalsound} to finish transcribing . So .
Grad G: I b I bet they 're more , because the beginning of the meeting had a lot more overlaps than {disfmarker} than sort of the middle .
PhD D: Yeah .
Grad G: Middle or end .
Postdoc B: I 'm not sure .
PhD D: Yeah .
Grad G: Because i we 're {disfmarker} we 're dealing with the {disfmarker} Uh , in the early meetings , we 're recording while we 're saying who 's talking on what microphone , {comment} and things like that ,
PhD D: Yeah .
Grad G: and that seems to be a lot of overlap .
Postdoc B: I think it 's an empirical question .
PhD D: Yeah .
Postdoc B: I think we could find that out .
PhD D: Yeah .
Grad G: Yep .
Postdoc B: I 'm {disfmarker} I 'm not sure that the beginning had more .
Professor E: So {disfmarker} so I was gonna ask , I guess about any {disfmarker} any other things that {disfmarker} that {disfmarker} that either of you wanted to talk about , especially since Andreas is leaving in five minutes , that {disfmarker} that you wanna go with .
PhD C: Can I just ask about the data , like very straightforward question is where we are on the amount of data and the amount of transcribed data , just cuz I 'm {disfmarker} I wanted to get a feel for that to sort of be able to know what {disfmarker} what can be done first and like how many meetings are we recording
Professor E: Right so there 's this {disfmarker} this {disfmarker} There 's this forty - five minute piece that Jane transcribed .
PhD C: and {disfmarker}
Professor E: That piece was then uh sent to IBM so they could transcribe so we have some comparison point . Then there 's s a larger piece that 's been recorded and uh put on CD - ROM and sent uh to IBM . Right ? And then we don't know .
PhD C: How many meetings is that ? Like {disfmarker} how many {disfmarker}
Grad G: What 's that ?
Professor E: That was about ten hours , and there was about {disfmarker}
PhD C: t ten {disfmarker} It 's like ten meetings or something ? Uh - huh .
Grad G: Yeah , something like that . And then {disfmarker} then we
PhD A: Ten meetings that have been sent to IBM ?
PhD C: And {disfmarker}
Professor E: Yeah .
Grad G: Well , I haven't sent them yet because I was having this problem with the {pause} missing files .
Professor E: Oh . Oh , that 's right , that had {disfmarker} those have not been sent .
PhD A: H how many total have we recorded now , altogether ?
Professor E: We 're saying about {pause} twelve hours .
Grad G: About twelve {pause} by now . Twelve or thirteen .
PhD C: Uh - huh . And we 're recording only this meeting , like continuously we 're only recording this one now ? or {disfmarker} ?
Professor E: No . No , so the {disfmarker} the {disfmarker} that 's the {disfmarker} that 's the biggest one {disfmarker} uh , chunk so far ,
Grad G: Nope .
PhD A: It was the morning one .
PhD C: OK .
Professor E: but there 's at least one meeting recorded of uh the uh uh natural language guys .
Grad G: Jerry .
PhD C: Do they meet every week ,
Professor E: And then there {disfmarker}
PhD C: or every {disfmarker}
Professor E: Uh , they do . w w And we talked to them about recording some more and we 're going to , uh , we 've started having a morning meeting , today uh i starting a w a week or two ago , on the uh front - end issues , and we 're recording those , uh there 's a network services and applications group here who 's agreed to have their meetings recorded ,
PhD C: Great .
Professor E: and we 're gonna start recording them . They 're {disfmarker} They meet on Tuesdays . We 're gonna start recording them next week . So actually , we 're gonna h start having a {disfmarker} a pretty significant chunk and so , you know , {vocalsound} Adam 's sort of struggling with trying to get things to be less buggy , and come up quicker when they do crash and stuff {disfmarker} things like that , now that uh {disfmarker} {vocalsound} the things are starting to happen . So right now , yeah , I th I 'd say the data is predominantly meeting meetings , but there are scattered other meetings in it and that {disfmarker} that amount is gonna grow uh so that the meeting meetings will probably ultimately {disfmarker} i if we 're {disfmarker} if we collect fifty or sixty hours , the meeting meetings it will probably be , you know , twenty or thirty percent of it , not {disfmarker} not {disfmarker} not eighty or ninety . But .
PhD C: So there 's probably {disfmarker} there 's three to four a week ,
Grad G: That 's what we 're aiming for .
PhD C: that we 're aiming for .
Professor E: Yeah .
PhD C: And they 're each about an hour or something .
Professor E: Yeah , yeah .
Grad G: Although {disfmarker} Yeah . We 'll find out tomorrow whether we can really do this or not .
PhD C: So {disfmarker} OK .
Professor E: Yeah and th the {disfmarker} the other thing is I 'm not pos I 'm sort of thinking as we 've been through this a few times , that I really don't know {disfmarker} maybe you wanna do it once for the novelty , but I don't know if in general we wanna have meetings that we record from outside this group do the digits .
Grad G: Right .
Professor E: Because it 's just an added bunch of weird stuff .
PhD C: Yeah .
Professor E: And , you know , we {disfmarker} we h we 're highly motivated . Uh in fact , the morning group is really motivated cuz they 're working on connected digits , so it 's {disfmarker}
Grad G: Actually that 's something I wanted to ask , is I have a bunch of scripts to help with the transcription of the digits .
Professor E: Yeah .
Grad G: We don't have to hand - transcribe the digits because we 're reading them and I have those .
PhD C: Right .
Professor E: Yeah .
Grad G: And so I have some scripts that let you very quickly extract the sections of each utterance . But I haven't been ru I haven't been doing that . Um , if I did that , is someone gonna be working on it ?
Professor E: Uh , yeah , I {disfmarker} I think definitely s so Absolutely .
Grad G: I mean , is it something of interest ?
Professor E: Yeah , whoever we have working on the acoustics for the Meeting Recorder are gonna start with that .
Grad G: OK . I mean , I I 'm {disfmarker} I 'm interested in it , I just don't have time to do it now .
PhD F: I was {disfmarker} these meetings {disfmarker} I 'm sure someone thought of this , but these {disfmarker} this uh reading of the numbers would be extremely helpful to do um adaptation .
Grad G: So
PhD F: Um .
Grad G: Yep . Yep .
PhD C: Actually I have o
Grad G: I {disfmarker} I would really like someone to do adaptation .
PhD F: Mm - hmm .
Grad G: So if we got someone interested in that , I think it would be great for Meeting Recorder .
Professor E: Well {disfmarker} I mean , one of the things I wanted to do , uh , that I I talked to {disfmarker} to Don about , is one of the possible things he could do or m also , we could have someone else do it , is to do block echo cancellation ,
Grad G: Since it 's the same people over and over .
PhD F: Mm - hmm .
Professor E: to try to get rid of some of the effects of the {disfmarker} the {disfmarker} the far - field effects . Um , I mean we have {disfmarker} the party line has been that echo cancellation is not the right way to handle the situation
PhD F: Mm - hmm .
Professor E: because people move around , and uh , if {disfmarker} if it 's {disfmarker} if it 's uh not a simple echo , like a cross - talk kind of echo , but it 's actually room acoustics , it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} you can't really do inversion ,
PhD F: Mm - hmm .
Professor E: and even echo cancellation is going to uh be something {disfmarker} It may {disfmarker} you {disfmarker} Someone may be moving enough that you are not able to adapt quickly and so the tack that we 've taken is more " lets come up with feature approaches and multi - stream approaches and so forth , that will be robust to it for the recognizer and not try to create a clean signal " .
PhD F: Mm - hmm .
Professor E: Uh , that 's the party line . But it occurred to me a few months ago that uh party lines are always , you know , sort of dangerous . It 's good {disfmarker} {vocalsound} good to sort of test them , actually . And so we haven't had anybody try to do a good serious job on echo cancellation and we should know how well that can do . So that 's something I 'd like somebody to do at some point , just take these digits , take the far - field mike signal , and the close uh mike signal , and apply really good echo cancellation . Um , there was a {disfmarker} have been some nice talks recently by {disfmarker} by Lucent on {disfmarker} on their b
PhD F: Hmm .
Professor E: the block echo cancellation particularly appealed to me , uh you know , trying and change it sample by sample , but you have some reasonable sized blocks . {comment} And um , you know , th
PhD A: W what is the um {disfmarker} the artifact you try to {disfmarker} you 're trying to get rid of when you do that ?
PhD F: Ciao .
Professor E: Uh so it 's {disfmarker} it {disfmarker} you have a {disfmarker} a direct uh {disfmarker} Uh , what 's the difference in {disfmarker} If you were trying to construct a linear filter , that would um {disfmarker}
PhD F: I 'm signing off .
Professor E: Yeah . that would subtract off {comment} the um uh parts of the signal that were the aspects of the signal that were different between the close - talk and the distant . You know , so {disfmarker} so uh um I guess in most echo cancellation {disfmarker} Yeah , so you {disfmarker} Given that um {disfmarker} Yeah , so you 're trying to {disfmarker} So you 'd {disfmarker} There 's a {disfmarker} a distance between the close and the distant mikes so there 's a time delay there , and after the time delay , there 's these various reflections . And if you figure out well what 's the {disfmarker} there 's a {disfmarker} a least squares algorithm that adjusts itself {disfmarker} adjusts the weight so that you try to subtract {disfmarker} essentially to subtract off uh different uh {disfmarker} different reflections . Right ? So let 's take the simple case where you just had {disfmarker} you had some uh some delay in a satellite connection or something and then there 's a {disfmarker} there 's an echo . It comes back . And you want to adjust this filter so that it will maximally reduce the effect of this echo .
PhD A: So that would mean like if you were listening to the data that was recorded on one of those . Uh , just the raw data , you would {disfmarker} you might hear kind of an echo ? And {disfmarker} and then this {disfmarker} noise cancellation would get
Professor E: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm saying {disfmarker} That 's a simplified version of what 's really happening . {comment} What 's really happening is {disfmarker} Well , when I 'm talking to you right now , you 're getting the direct sound from my speech , but you 're also getting , uh , the indirect sound that 's bounced around the room a number of times . OK ? So now , if you um try to r you {disfmarker} To completely remove the effect of that is sort of impractical for a number of technical reasons , but I {disfmarker} but {disfmarker} not to try to completely remove it , that is , invert the {disfmarker} the room response , but just to try to uh uh eliminate some of the {disfmarker} the effect of some of the echos . Um , a number of people have done this so that , say , if you 're talking to a speakerphone , uh it makes it more like it would be , if you were talking right up to it . So this is sort of the st the straight - forward approach . You say I {disfmarker} I {disfmarker} I want to use this uh {disfmarker} this item but I want to subtract off various kinds of echos . So you construct a filter , and you have this {disfmarker} this filtered version uh of the speech um gets uh uh {disfmarker} gets subtracted off from the original speech . Then you try to {disfmarker} you try to minimize the energy in some sense . And so um {disfmarker} uh with some constraints .
PhD A: Kind of a clean up thing , that {disfmarker}
Professor E: It 's a clean up thing . Right .
PhD A: OK .
Professor E: So , echo cancelling is {disfmarker} is , you know , commonly done in telephony , and {disfmarker} and {disfmarker} and it 's sort of the obvious thing to do in this situation if you {disfmarker} if , you know , you 're gonna be talking some distance from a mike .
PhD A: When uh , I would have meetings with the folks in Cambridge when I was at BBN over the phone , they had a um {disfmarker} some kind of a special speaker phone and when they would first connect me , it would come on and we 'd hear all this noise . And then it was uh {disfmarker} And then it would come on and it was very clear ,
Professor E: Yeah .
PhD A: you know .
Professor E: Right . So it 's taking samples , it 's doing adaptation , it 's adjusting weights , and then it 's getting the sum . So um , uh anyway that 's {disfmarker} that 's kind of a reasonable thing that I 'd like to have somebody try {disfmarker} somebody look {disfmarker} And {disfmarker} and the digits would be a reasonable thing to do that with . I think that 'd be enough data {disfmarker} plenty of data to do that with , and i for that sort of task you wouldn't care whether it was uh large vocabulary speech or anything . Uh . {vocalsound} Um
Postdoc B: Is Brian Kingsbury 's work related to that , or is it a different type of reverberation ?
Professor E: Brian 's {comment} Kingsbury 's work is an example of what we did f f from the opposite dogma . Right ? Which is what I was calling the " party line " , which is that uh doing that sort of thing is not really what we want . We want something more flexible , uh i i where people might change their position , and there might be , you know {disfmarker} There 's also um oh yeah , noise . So the echo cancellation does not really allow for noise . It 's if you have a clean situation but you just have some delays , Then we 'll figure out the right {disfmarker} the right set of weights for your taps for your filter in order to produce the effect of those {disfmarker} those echos . But um if there 's noise , then the very signal that it 's looking at is corrupted so that it 's decision about what the right {disfmarker} you know , right {disfmarker} right uh {disfmarker} delays are {disfmarker} is , uh {disfmarker} is {disfmarker} right delayed signal is {disfmarker} is {disfmarker} is {disfmarker} uh is incorrect . And so , in a noisy situation , um , also in a {disfmarker} in a situation that 's very reverberant {disfmarker} {comment} with long reverberation times {comment} and really long delays , it 's {disfmarker} it 's sort of typically impractical . So for those kind of reasons , and also a {disfmarker} a c a complete inversion , if you actually {disfmarker} I mentioned that it 's kind of hard to really do the inversion of the room acoustics . Um , that 's difficult because um often times the {disfmarker} the um {disfmarker} {vocalsound} the system transfer function is such that when it 's inverted you get something that 's unstable , and so , if you {disfmarker} you do your estimate of what the system is , and then you try to invert it , you get a filter that actually uh , you know , rings , and {disfmarker} and uh goes to infinity . So it 's {disfmarker} so there 's {disfmarker} there 's {disfmarker} there 's that sort of technical reason , and the fact that things move , and there 's air currents {disfmarker} I mean there 's all sorts of {disfmarker} all sorts of reasons why it 's not really practical . So for all those kinds of reasons , uh we {disfmarker} we {disfmarker} we sort of um , concluded we didn't want to in do inversion , and we 're even pretty skeptical of echo cancellation , which isn't really inversion , and um we decided to do this approach of taking {disfmarker} uh , just picking uh features , which were {disfmarker} uh will give you more {disfmarker} something that was more stable , in the presence of , or absence of , room reverberation , and that 's what Brian was trying to do . So , um , let me just say a couple things that I was {disfmarker} I was gonna bring up . Uh . Let 's see . I guess you {disfmarker} you actually already said this thing about the uh {disfmarker} about the consent forms , which was that we now don't have to {disfmarker} So this was the human subjects folks who said this , {comment} or that {disfmarker} that {disfmarker} ?
Postdoc B: The a apparently {disfmarker} I mean , we 're gonna do a revised form , of course . Um but once a person has signed it once , then that 's valid for a certain number of meetings . She wanted me to actually estimate how many meetings and put that on the consent form . I told her that would be a little bit difficult to say . So I think from a s practical standpoint , maybe we could have them do it once every ten meetings , or something . It won't be that many people who do it {pause} that often , but um just , you know , so long as they don't forget that they 've done it , I guess .
Professor E: OK . Um , back on the data thing , so there 's this sort of one hour , ten hour , a hundred hour sort of thing that {disfmarker} that we have . We have {disfmarker} we have an hour uh that {disfmarker} that is transcribed , we have {disfmarker} we have twelve hours that 's recorded but not transcribed , and at the rate we 're going , uh by the end of the semester we 'll have , I don't know , forty or fifty or something , if we {disfmarker} if this really uh {disfmarker} Well , do we have that much ?
PhD C: Not really . It 's three to four per week .
Professor E: Let 's see , we have {disfmarker}
PhD C: So that 's what {disfmarker} You know , that {disfmarker}
Professor E: uh eight weeks , uh is {disfmarker}
PhD C: So that 's not a lot of hours .
Professor E: Eight weeks times three hours is twenty - four , so that 's {disfmarker} Yeah , so like thirty {disfmarker} thirty hours ?
PhD A: Three {disfmarker} Three hours .
PhD C: Yeah . I mean , is there {disfmarker} I know this sounds {pause} tough but we 've got the room set up . Um I was starting to think of some projects where you would use well , similar to what we talked about with uh energy detection on the close - talking mikes . There are a number of interesting questions that you can ask about how interactions happen in a meeting , that don't require any transcription . So what are the patterns , the energy patterns over the meeting ? And I 'm really interested in this {vocalsound} but we don't have a whole lot of data . So I was thinking , you know , we 've got the room set up and you can always think of , also for political reasons , if ICSI collected you know , two hundred hours , that looks different than forty hours , even if we don't transcribe it ourselves ,
Professor E: But I don't think we 're gonna stop at the end of this semester .
PhD C: so {disfmarker}
Professor E: Right ? So , I th I think that if we are able to keep that up for a few months , we are gonna have more like a hundred hours .
PhD C: I mean , is there {disfmarker} Are there any other meetings here that we can record , especially meetings that have some kind of conflict in them {comment} or some kind of deci I mean , that are less well {disfmarker} I don't {disfmarker} uh , that have some more emotional aspects to them , or strong {disfmarker}
Grad G: We had some good ones earlier .
PhD C: There 's laughter , um I 'm talking more about strong differences of opinion meetings , maybe with manager types , or {disfmarker}
Grad G: I think it 's hard to record those .
PhD C: To be allowed to record them ?
Postdoc B: It 's also likely that people will cancel out afterwards .
PhD C: OK .
Professor E: Yeah , people will get {disfmarker}
Postdoc B: But I {disfmarker} but I wanted to raise the KPFA idea .
PhD C: OK . Well , if there is , anyway .
Professor E: Yeah , I was gonna mention that .
Grad G: Oh , that 's a good idea . That 's {disfmarker} That would be a good match .
Professor E: Yeah . So {disfmarker} Yeah . So I {disfmarker} I {disfmarker} uh , I {disfmarker} I 'd mentioned to Adam , and {disfmarker} that was another thing I was gonna talk {disfmarker} uh , mention to them before {disfmarker} {comment} that uh there 's uh {disfmarker} It {disfmarker} it oc it occurred to me that we might be able to get some additional data by talking to uh acquaintances in local broadcast media . Because , you know , we had talked before about the problem about using found data , {comment} that {disfmarker} that uh it 's just set up however they have it set up and we don't have any say about it and it 's typically one microphone , in a , uh , uh {disfmarker} or {disfmarker} and {disfmarker} and so it doesn't really give us the {disfmarker} the {disfmarker} the uh characteristics we want . Um and so I do think we 're gonna continue recording here and record what we can . But um , it did occur to me that we could go to friends in broadcast media and say " hey you have this panel show , {pause} or this {disfmarker} you know , this discussion show , and um can you record multi - channel ? " And uh they may be willing to record it uh with {disfmarker}
PhD C: With lapel mikes or something ?
Professor E: Well , they probably already use lapel , but they might be able to have it {disfmarker} it wouldn't be that weird for them to have another mike that was somewhat distant .
PhD C: Right .
Professor E: It wouldn't be exactly this setup , but it would be that sort of thing , and what we were gonna get from UW , you know , assuming they {disfmarker} they {disfmarker} they start recording , isn't {disfmarker} als also is not going to be this exact setup .
PhD C: Right . No , I think that 'd be great , if we can get more data .
Professor E: So , {comment} I {disfmarker} I {disfmarker} I {disfmarker} I was thinking of looking into that . the other thing that occurred to me after we had that discussion , in fact , is that it 's even possible , since of course , many radio shows are not live , {comment} uh that we could invite them to have like some of their {disfmarker} {comment} record some of their shows here .
Postdoc B: Wow !
PhD C: Well {disfmarker} Or {disfmarker} The thing is , they 're not as averse to wearing one of these head - mount I mean , they 're on the radio ,
Grad G: Right , as we are .
PhD C: right ? So . {comment} Um , I think that 'd be fantastic
Professor E: Right .
PhD C: cuz those kinds of panels and {disfmarker} Those have interesting
Professor E: Yeah .
PhD C: Th - that 's an {disfmarker} a side of style {disfmarker} a style that we 're not collecting here , so it 'd be great .
Professor E: And {disfmarker} and the {disfmarker} I mean , the other side to it was the {disfmarker} what {disfmarker} which is where we were coming from {disfmarker} I 'll {disfmarker} I 'll talk to you more about it later {comment} is that {disfmarker} is that there 's {disfmarker} there 's uh the radio stations and television stations already have stuff worked out presumably , uh related to , you know , legal issues and {disfmarker} and permissions and all that . I mean , they already do what they do {disfmarker} do whatever they do . So it 's {disfmarker} uh , it 's {disfmarker} So it 's {disfmarker} so it 's another source . So I think it 's something we should look into , you know , we 'll collect what we collect here hopefully they will collect more at UW also and um {disfmarker} and maybe we have this other source . But yeah I think that it 's not unreasonable to aim at getting , you know , significantly in excess of a hundred hours . I mean , that was sort of our goal . The thing was , I was hoping that we could {disfmarker} @ @ in the {disfmarker} under this controlled situation we could at least collect , you know , thirty to fifty hours . And at the rate we 're going we 'll get pretty close to that I think this semester . And if we continue to collect some next semester , I think we should , uh {disfmarker}
PhD C: Right . Yeah I was mostly trying to think , " OK , if you start a project , within say a month , you know , how much data do you have to work with . And you {disfmarker} you wanna s you wanna sort of fr freeze your {disfmarker} your data for awhile so um right now {disfmarker} and we don't have the transcripts back yet from IBM right ? Do {disfmarker} Oh , do we now ?
Professor E: Well , we don't even have it for this f you know , forty - five minutes , that was {disfmarker}
PhD C: So um , not complaining , I was just trying to think , you know , what kinds of projects can you do now versus uh six months from now
Professor E: Yeah .
PhD C: and they 're pretty different , because
Professor E: Yeah . So I was thinking right now it 's sort of this exploratory stuff where you {disfmarker} you look at the data , you use some primitive measures and get a feeling for what the scatter plots look like ,
Grad G: Right .
PhD C: um {disfmarker} Right . Right , right .
Professor E: and {disfmarker} and {disfmarker} and uh {disfmarker} and meanwhile we collect , and it 's more like yeah , three months from now , or six months from now you can {disfmarker} you can do a lot of other things .
PhD C: Cuz I 'm not actually sure , just logistically that I can spend {disfmarker} you know , I don't wanna charge the time that I have on the project too early , before there 's enough data to make good use of the time . And that 's {disfmarker} and especially with the student
Grad G: Right .
PhD C: uh for instance this guy who seems {disfmarker}
Professor E: Yeah .
PhD C: Uh anyway , I shouldn't say too much , but um if someone came that was great and wanted to do some real work and they have to end by the end of this school year in the spring , how much data will I have to work with , with that person . And so it 's {disfmarker}
Professor E: i Yeah , so I would think , exploratory things now . Uh , three months from now {disfmarker} Um , I mean the transcriptions I think are a bit of an unknown cuz we haven't gotten those back yet as far as the timing , but I think as far as the collection , it doesn't seem to me l like , uh , unreasonable to say that uh in January , you know , ro roughly uh {disfmarker} which is roughly three months from now , we should have at least something like , you know , twenty - five , thirty hours .
PhD C: And we just don't know about the transcription part of that ,
Professor E: So that 's {disfmarker}
Postdoc B: Yeah , we need to {disfmarker} I think that there 's a possibility that the transcript will need to be adjusted afterwards ,
PhD C: so . I mean , it {disfmarker}
Postdoc B: and uh es especially since these people won't be uh used to dealing with multi - channel uh transcriptions .
PhD C: Right .
Professor E: Yeah .
Postdoc B: So I think that we 'll need to adjust some {disfmarker} And also if we wanna add things like um , well , more refined coding of overlaps , then definitely I think we should count on having an extra pass through . I wanted to ask another a a aspect of the data collection . There 'd be no reason why a person couldn't get together several uh , you know , friends , and come and argue about a topic if they wanted to , right ?
Professor E: If they really have something they wanna talk about as opposed to something @ @ {disfmarker} I mean , what we 're trying to stay away from was artificial constructions , but I think if it 's a real {disfmarker} Why not ? Yeah .
PhD C: I mean , I 'm thinking , politically {disfmarker}
Grad G: Stage some political debates .
Postdoc B: You could do this ,
PhD C: Well yeah ,
Postdoc B: you know . You could .
PhD C: or just if you 're {disfmarker} if you ha If there are meetings here that happen that we can record even if we don't {pause} um have them do the digits , {comment} or maybe have them do a shorter {pause} digit thing {comment} like if it was , you know , uh , one string of digits , or something , they 'd probably be willing to do .
Grad G: We don't have to do the digits at all if we don't want to .
PhD C: Then , having the data is very valuable , cuz I think it 's um politically better for us to say we have this many hours of audio data , especially with the ITR , if we put in a proposal on it . It 'll just look like ICSI 's collected a lot more audio data . Um , whether it 's transcribed or not um , is another issue , but there 's {disfmarker} there are research questions you can answer without the transcriptions , or at least that you can start to answer .
Postdoc B: It seems like you could hold some meetings .
Grad G: Yep .
Postdoc B: You know , you and maybe Adam ?
PhD C: So .
Postdoc B: You {disfmarker} you could {disfmarker} you could maybe hold some additional meetings , if you wanted .
PhD A: Would it help at all {disfmarker} I mean , we 're already talking about sort of two levels of detail in meetings . One is uh um without doing the digits {disfmarker} Or , I guess the full - blown one is where you do the digits , and everything , and then talk about doing it without digits , what if we had another level , just to collect data , which is without the headsets and we just did the table - mounted stuff .
PhD C: Need the close - talking mikes .
PhD A: You do , OK .
PhD C: I mean , absolutely ,
Professor E: Yeah . Yeah .
PhD C: yeah . I 'm really scared {disfmarker}
Grad G: It seems like it 's a big part of this corpus is to have the close - talking mikes .
PhD A: I see , OK .
PhD C: Um or at least , like , me personally ? I would {disfmarker} {comment} I {disfmarker} couldn't use that data .
Professor E: Yeah .
Postdoc B: I agree . And Mari also ,
PhD C: Um .
Postdoc B: we had {disfmarker} This came up when she she was here . That 's important .
PhD C: So it 's a great idea ,
Professor E: Yeah , I {disfmarker} I {disfmarker} b By the {disfmarker} by the way , I don't think the transcriptions are actually , in the long run , such a big bottleneck .
PhD C: and if it were true than I would just do that , but it 's not that bad {disfmarker} like the room is not the bottleneck , and we have enough time in the room , it 's getting the people to come in and put on the {disfmarker} and get the setup going .
Professor E: I think the issue is just that we 're {disfmarker} we 're blazing that path . Right ? And {disfmarker} and um {disfmarker} d Do you have any idea when {disfmarker} when uh the {disfmarker} you 'll be able to send uh the ten hours to them ?
Grad G: Well , I 've been burning two C Ds a day , which is about all I can do with the time I have .
Professor E: Yeah . Yeah .
Grad G: So it 'll be early next week .
Professor E: Yeah , OK . So early next week we send it to them , and then {disfmarker} then we check with them to see if they 've got it and we {disfmarker} we start , you know asking about the timing for it .
Grad G: Yep .
Professor E: So I think once they get it sorted out about how they 're gonna do it , which I think they 're pretty well along on , cuz they were able to read the files and so on .
Grad G: Yep .
Professor E: Right ?
Grad G: Yeah , but {disfmarker}
Professor E: Well {disfmarker}
Grad G: Yeah , who knows where they are .
PhD A: Have they ever responded to you ?
Grad G: Nope .
Professor E: Yeah , but {disfmarker} You know , so they {disfmarker} they {disfmarker} they have {disfmarker} you know , they 're volunteering their time and they have a lot of other things to do ,
PhD C: What if {disfmarker}
Grad G: Yeah , you {disfmarker} we can't complain .
Professor E: right ? But they {disfmarker} But at any rate , they 'll {disfmarker} I {disfmarker} I think once they get that sorted out , they 're {disfmarker} they 're making cassettes there , then they 're handing it to someone who they {disfmarker} who 's {disfmarker} who is doing it , and uh I think it 's not going to be {disfmarker} I don't think it 's going to be that much more of a deal for them to do thirty hours then to do one hour , I think . It 's not going to be thirty
Grad G: Yep . I think that 's probably true .
PhD C: Really ? So it 's the amount of {disfmarker}
Professor E: It 's {disfmarker} it 's just getting it going .
Grad G: It 's pipeline , pipeline issues .
PhD C: Right . What about these lunch meetings {disfmarker}
Grad G: Once the pipeline fills .
PhD C: I mean , I don't know , if there 's any way without too much more overhead , even if we don't ship it right away to IBM even if we just collect it here for awhile , {comment} to record you know , two or three more meeting a week , just to have the data , even if they 're um not doing the digits , but they do wear the headphones ?
Professor E: But the lunch meetings are pretty much one person getting up and {disfmarker}
PhD C: No , I meant , um , sorry , the meetings where people eat their lunch downstairs , maybe they don't wanna be recorded , but {disfmarker}
Grad G: Oh , and we 're just chatting ?
PhD C: Just the ch the chatting .
Grad G: Yeah , we have a lot of those .
PhD C: I actually {disfmarker} I actually think that 's {pause} useful {pause} data , um {pause} the chatting ,
Grad G: Yeah , the problem with that is I would {disfmarker} I think I would feel a little constrained to {disfmarker} You know ? Uh , some of the meetings {disfmarker}
PhD C: but {disfmarker} OK . You don't wanna do it , cuz {disfmarker} OK .
Grad G: You know , our " soccer ball " meeting ?
PhD C: Alright .
Grad G: I guess none of you were there for our soccer ball meeting .
PhD C: Alright , {comment} so I 'll just throw it out there , if anyone knows of one more m or two more wee meetings per week that happen at ICSI , um that we could record , I think it would be worth it .
Grad G: That was hilarious .
Professor E: Yeah . Well , we should also check with Mari again , because they {disfmarker} because they were really intending , you know , maybe just didn't happen , but they were really intending to be duplicating this in some level . So then that would double {pause} what we had . Uh . And there 's a lot of different meetings at UW uh {disfmarker} I mean really m a lot more {comment} than we have here right cuz we 're not right on campus ,
Grad G: Right .
Professor E: so .
PhD A: Is the uh , notion of recording any of Chuck 's meetings dead in the water , or is that still a possibility ?
Professor E: Uh , {vocalsound} they seem to have some problems with it . We can {disfmarker} we can talk about that later . Um , but , again , Jerry is {disfmarker} Jerry 's open {disfmarker} So I mean , we have two speech meetings , one uh network meeting , uh Jerry was open to it but I {disfmarker} I s One of the things that I think is a little {disfmarker} a little bit of a limitation , there is a think when the people are not involved uh in our work , we probably can't do it every week . You know ? I {disfmarker} I {disfmarker} I {disfmarker} I think that {disfmarker} that people are gonna feel uh {disfmarker} are gonna feel a little bit constrained . Now , it might get a little better if we don't have them do the digits all the time . And the {disfmarker} then {disfmarker} so then they can just really sort of try to {disfmarker} put the mikes on and then just charge in and {disfmarker}
Grad G: Yep .
PhD C: What if we give people {disfmarker} you know , we cater a lunch in exchange for them having their meeting here or something ?
Postdoc B: Well , you know , I {disfmarker} I do think eating while you 're doing a meeting is going to be increasing the noise .
PhD C: OK .
Postdoc B: But I had another question , which is um , you know , in principle , w um , I know that you don't want artificial topics ,
PhD C: Alright , alright , alright .
Postdoc B: but um it does seem to me that we might be able to get subjects from campus to come down and do something that wouldn't be too artificial . I mean , we could {disfmarker} political discussions , or {disfmarker} or something or other ,
PhD C: No , definitely .
Postdoc B: and i you know , people who are {disfmarker} Because , you know , there 's also this constraint . We d it 's like , you know , the {disfmarker} the {disfmarker} uh goldibears {disfmarker} goldi goldilocks , it 's like you don't want meetings that are too large , but you don't want meetings that are too small . And um {disfmarker} a and it just seems like maybe we could exploit the subj human subject p p pool , in the positive sense of the word .
PhD A: Well , even {disfmarker} I mean , coming down from campus is sort of a big thing , but what about
Postdoc B: We could pay subjects .
PhD A: or what about people in the {disfmarker} in the building ?
PhD C: Yeah , I was thinking , there 's all these other peo
PhD A: I mean , there 's the State of California downstairs , and {disfmarker}
PhD C: Yeah . I mean {disfmarker}
Grad G: I just really doubt that uh any of the State of California meetings would be recordable and then releasable to the general public .
Postdoc B: Yeah .
PhD A: Oh .
PhD C: Mm - hmm .
Grad G: So I {disfmarker} I mean I talked with some people at the Haas Business School who are i who are interested in speech recognition
PhD C: Alright , well .
Grad G: and , they sort of hummed and hawed and said " well maybe we could have meetings down here " , but then I got email from them that said " no , we decided we 're not really interested and we don't wanna come down and hold meetings . " So , I think it 's gonna be a problem to get people regularly .
PhD A: What about Joachim , maybe he can {disfmarker}
Professor E: But {disfmarker} but we c But I think , you know , we get some scattered things from this and that . And I {disfmarker} I d I do think that maybe we can get somewhere with the {disfmarker} with the radio .
PhD C: Mm - hmm .
Professor E: Uh i I have better contacts in radio than in television , but {disfmarker}
PhD A: You could get a lot of lively discussions from those radio ones .
PhD C: Well , and they 're already {disfmarker} they 're {disfmarker} these things are already recorded ,
Grad G: Yep .
Professor E: Yeah .
PhD C: we don't have to ask them to {disfmarker} even {disfmarker} and I 'm not sure wh how they record it , but they must record from individual {disfmarker}
Professor E: n Well {disfmarker} No , I 'm not talking about ones that are already recorded . I 'm talking about new ones
PhD C: Why {disfmarker} why not ?
Professor E: because {disfmarker} because {disfmarker} because we would be asking them to do something different .
PhD C: Well , we can find out . I know for instance Mark Liberman was interested uh in {disfmarker} in LDC getting {pause} data , uh , and {disfmarker}
Professor E: Right , that 's the found data idea .
PhD C: Yeah .
Professor E: But what I 'm saying is uh if I talk to people that I know who do these th who produce these things we could ask them if they could record an extra channel , let 's say , of a distant mike .
PhD C: Mm - hmm .
Professor E: And u I think routinely they would not do this . So , since I 'm interested in the distant mike stuff , I wanna make sure that there is at least that somewhere
PhD C: Right . Great . OK .  
Professor E: and uh {disfmarker} But if we ask them to do that they might be intrigued enough by the idea that they uh might be e e willing to {disfmarker} the {disfmarker} I might be able to talk them into it .
PhD C: Mm - hmm .
Grad G: Um . We 're getting towards the end of our disk space , so we should think about trying to wrap up here .
PhD C: That 's a good way to end a meeting .
Professor E: OK . Well I don't {disfmarker} why don't we {disfmarker} why d u why don't we uh uh turn them {disfmarker} turn
Grad G: OK , leave {disfmarker} leave them on for a moment until I turn this off , cuz that 's when it crashed last time .
Postdoc B: Oh . That 's good to know .
Professor E: Turning off the microphone made it crash . Well {disfmarker}
Postdoc B: That 's good to know .
Professor E: OK .
2022-06-15 06:45:33 | INFO | __main__ | output #0: To save time, speaker mn005 will only mark the sample of transcribed data for regions of overlapping speech, as opposed to marking all acoustic events. The digits extraction task will be delegated to whomever is working on acoustics for the Meeting Recorder project.
2022-06-15 06:45:33 | INFO | __main__ | input #1: What was said on speech overlap?

Professor E: So . OK . Doesn't look like it crashed . That 's great .
Grad G: So I think maybe what 's causing it to crash is I keep starting it and then stopping it to see if it 's working . And so I think starting it and then stopping it and starting it again causes it to crash . So , I won't do that anymore .
Postdoc B: And it looks like you 've found a way of uh mapping the location to the {disfmarker} without having people have to give their names each time ?
PhD A: Sounds like an initialization thing .
Postdoc B: I mean it 's like you have the {disfmarker} So you know that {disfmarker}
Grad G: No .
Postdoc B: I mean , are you going to write down {pause} that I sat here ?
Grad G: I 'm gonna collect the digit forms and write it down .
Postdoc B: OK .
PhD C: Oh , OK .
Grad G: So {disfmarker} So they should be right with what 's on the digit forms . OK , so I 'll go ahead and start with digits . u And I should say that uh , you just pau you just read each line an and then pause briefly .
Professor E: And start by giving the transcript number .
PhD A: Tran
PhD D: Transcript {disfmarker} Uh . OK , OK .
PhD A: Oh sorry , go ahead .
Professor E: So uh , you see , Don , the unbridled excitement of the work that we have on this project .
Grad H: OK .
Professor E: It 's just uh {disfmarker}
Grad H: Umh .
Professor E: Uh , you know , it doesn't seem like a bad idea to have {comment} that information .
Grad G: And I 'm surprised I sort of {disfmarker} I 'm surprised I forgot that ,
Professor E: Yeah , I {disfmarker} I 'd {disfmarker} I think it 's some
Grad G: but uh I think that would be a good thing to add . After I just printed out a zillion of them .
Professor E: Yeah , well , that 's {disfmarker} Um , so I {disfmarker} I do have a {disfmarker} a an agenda suggestion . Uh , we {disfmarker} I think the things that we talk about in this meeting uh tend to be a mixture of uh procedural uh mundane things and uh research points and um I was thinking I think it was a meeting a couple of weeks ago that we {disfmarker} we spent much of the time talking about the mundane stuff cuz that 's easier to get out of the way and then we sort of drifted into the research and maybe five minutes into that Andreas had to leave . So {vocalsound} uh I 'm suggesting we turn it around and {disfmarker} and uh sort of we have {disfmarker} anybody has some mundane points that we could send an email later , uh hold them for a bit , and let 's talk about the {disfmarker} the research - y kind of things . Um , so um the one th one thing I know that we have on that is uh we had talked a {disfmarker} a couple weeks before um uh about the uh {disfmarker} the stuff you were doing with {disfmarker} with uh um uh l l attempting to locate events , we had a little go around trying to figure out what you meant by " events " but I think , you know , what we had meant by " events " I guess was uh points of overlap between speakers . But I th I gather from our discussion a little earlier today that you also mean uh interruptions with something else
PhD D: Yeah .
Professor E: like some other noise .
PhD D: Uh - huh . Yeah .
Professor E: Yes ? You mean that as an event also .
PhD D: To
Professor E: So at any rate you were {disfmarker} you 've {disfmarker} you 've done some work on that
PhD D: right .
Professor E: and um then the other thing would be it might be nice to have a preliminary discussion of some of the other uh research uh areas that uh we 're thinking about doing . Um , I think especially since you {disfmarker} you haven't been in {disfmarker} in these meetings for a little bit , maybe you have some discussion of some of the p the plausible things to look at now that we 're starting to get data , uh and one of the things I know that also came up uh is some discussions that {disfmarker} that uh {disfmarker} that uh Jane had with Lokendra uh about some {disfmarker} some {disfmarker} some um uh work about I {disfmarker} I {disfmarker} I d I {disfmarker} I don't want to try to say cuz I {disfmarker} I 'll say it wrong , but anyway some {disfmarker} some potential collaboration there about {disfmarker} about the {disfmarker} about the {disfmarker} working with these data .
PhD C: Oh . Sure .
Professor E: So . So , uh .
Grad G: You wanna just go around ?
Professor E: Uh . {pause} Well , I don't know if we {disfmarker} if this is sort of like everybody has something to contribute sort of thing , I think there 's just just a couple {disfmarker} a couple people primarily um but um Uh , wh why don't {disfmarker} Actually I think that {disfmarker} that last one I just said we could do fairly quickly so why don't you {disfmarker} you start with that .
Postdoc B: OK . Shall I {disfmarker} shall I just start ? OK .
Professor E: Yeah , just explain what it was .
Postdoc B: Um , so , uh , he was interested in the question of {disfmarker} you know , relating to his {disfmarker} to the research he presented recently , um of inference structures , and uh , the need to build in , um , this {disfmarker} this sort of uh mechanism for understanding of language . And he gave the example in his talk about how {pause} um , e a I 'm remembering it just off the top of my head right now , but it 's something about how um , i " Joe slipped " you know , " John had washed the floor " or something like that . And I don't have it quite right , but that kind of thing , where you have to draw the inference that , OK , there 's this time sequence , but also the {disfmarker} the {disfmarker} the causal aspects of the uh floor and {disfmarker} and how it might have been the cause of the fall and that um it was the other person who fell than the one who cleaned it and it {disfmarker} {comment} These sorts of things . So , I looked through the transcript that we have so far , {comment} and um , fou identified a couple different types of things of that type and um , one of them was something like uh , during the course of the transcript , um um , w we had gone through the part where everyone said which channel they were on and which device they were on , and um , the question was raised " Well , should we restart the recording at this point ? " And {disfmarker} and Dan Ellis said , " Well , we 're just so far ahead of the game right now {pause} we really don't need to " . Now , how would you interpret that without a lot of inference ? So , the inferences that are involved are things like , OK , so , how do you interpret " ahead of the game " ? You know . So it 's the {disfmarker} it 's {pause} i What you {disfmarker} what you int what you draw {disfmarker} you know , the conclusions that you need to draw are that space is involved in recording ,
Grad G: Hmm , metaphorically .
Postdoc B: that um , i that {pause} i we have enough space , and he continues , like " we 're so ahead of the game cuz now we have built - in downsampling " . So you have to sort of get the idea that um , " ahead of the game " is sp speaking with respect to space limitations , that um that in fact downsampling is gaining us enough space , and that therefore we can keep the recording we 've done so far . But there are a lot of different things like that .
Grad G: So , do you think his interest is in using this as {pause} a data source , or {pause} training material , or what ?
Professor E: Well , I {disfmarker} I should maybe interject to say this started off with a discussion that I had with him , so um we were trying to think of ways that his interests could interact with ours
Grad G: Mm - hmm .
Professor E: and um uh I thought that if we were going to project into the future when we had a lot of data , uh and um such things might be useful for that in or before we invested too much uh effort into that he should uh , with Jane 's help , look into some of the data that we 're {disfmarker} already have and see , is there anything to this at all ?
Grad G: Mm - hmm .
Professor E: Is there any point which you think that , you know , you could gain some advantage and some potential use for it . Cuz it could be that you 'd look through it and you say " well , this is just the wrong {pause} task for {disfmarker} for him to pursue his {disfmarker} "
Grad G: Wrong , yeah .
Professor E: And {disfmarker} and uh I got the impression from your mail that in fact there was enough things like this just in the little sample that {disfmarker} that you looked at that {disfmarker} that it 's plausible at least .
Postdoc B: It 's possible . Uh , he was {disfmarker} he {disfmarker} he {disfmarker} you know {disfmarker} We met and he was gonna go and uh you know , y look through them more systematically
Professor E: Yeah .
Postdoc B: and then uh meet again .
Professor E: Yeah .
Postdoc B: So it 's , you know , not a matter of a {disfmarker}
Professor E: Yeah .
Postdoc B: But , yeah , I think {disfmarker} I think it was optimistic .
Professor E: So anyway , that 's {disfmarker} that 's e a quite different thing from anything we 've talked about that , you know , might {disfmarker} might {disfmarker} might come out from some of this .
PhD C: But he can use text , basically . I mean , he 's talking about just using text
Postdoc B: That 's his major {disfmarker} I mentioned several that w had to do with implications drawn from intonational contours
PhD C: pretty much , or {disfmarker} ?
Postdoc B: and {pause} that wasn't as directly relevant to what he 's doing . He 's interested in these {disfmarker} these knowledge structures ,
PhD C: OK .
PhD D: Yeah , interesting .
Postdoc B: inferences that you draw {pause} i from {disfmarker}
Professor E: I mean , he certainly could use text , but we were in fact looking to see if there {disfmarker} is there {disfmarker} is there something in common between our interest in meetings and his interest in {disfmarker} in {disfmarker} in this stuff . So .
Grad G: And I imagine that transcripts of speech {disfmarker} I mean text that is speech {disfmarker} probably has more of those than sort of prepared writing . I {disfmarker} I don't know whether it would or not , but it seems like it would .
Professor E: I don't know , probably de probably depends on what the prepared writing was . But .
Postdoc B: Yeah , I don't think I would make that leap , because i in narratives , you know {disfmarker} I mean , if you spell out everything in a narrative , it can be really tedious ,
Grad G: Mm - hmm .
Postdoc B: so .
Grad G: Yeah , I 'm just thinking , you know , when you 're {disfmarker} when you 're face to face , you have a lot of backchannel and {disfmarker} And {disfmarker}
Postdoc B: Oh . That aspect .
Grad G: Yeah . And so I think it 's just easier to do that sort of broad inference jumping if it 's face to face . I mean , so , if I just read that Dan was saying " we 're ahead of the game " {comment} in that {disfmarker} in that context ,
Postdoc B: Well {disfmarker} Yeah .
Grad G: I might not realize that he was talking about disk space as opposed to anything else .
Postdoc B: I {disfmarker} you know , I {disfmarker} I had several that had to do with backchannels and this wasn't one of them .
Grad G: Uh - huh .
Postdoc B: This {disfmarker} this one really does um m make you leap from {disfmarker} So he said , you know , " we 're ahead of the game , w we have built - in downsampling " .
Grad G: Mm - hmm .
Postdoc B: And the inference , i if you had it written down , would be {disfmarker}
Grad G: I guess it would be the same .
Postdoc B: Uh - huh . But there are others that have backchannelling , it 's just he was less interested in those .
PhD F: Can I {disfmarker} Sorry to interrupt . Um , I f f f I 've {disfmarker} @ @ {comment} d A minute {disfmarker} uh , several minutes ago , I , like , briefly was {disfmarker} was not listening and {disfmarker} So who is " he " in this context ?
PhD C: Yeah , there 's a lot of pronoun {disfmarker}
PhD F: OK . So I was just realizing we 've {disfmarker} You guys have been talking about " he " um for at least uh , I don't know , three {disfmarker} three four minutes without ever mentioning the person 's name again .
PhD C: I believe it . Yeah . Actually to make it worse , {comment} uh , Morgan uses " you " and " you "
PhD F: So this is {disfmarker} this is {disfmarker} this is {disfmarker} gonna be a big , big problem if you want to later do uh , you know , indexing , or speech understanding of any sort .
Grad G: It 's in my notes .
PhD C: with gaze and no identification , or {disfmarker} I just wrote this down . Yeah , actually . Cuz Morgan will say well , " you had some ideas "
PhD D: Yeah .
PhD F: You just wrote this ?
PhD C: and he never said Li - He looked {disfmarker}
Grad G: Well , I think he 's doing that intentionally ,
PhD C: Right , so it 's great .
Grad G: aren't you ?
PhD C: So this is really great
PhD F: Right .
PhD C: because the thing is , because he 's looking at the per even for addressees in the conversation ,
PhD D: Yeah .
PhD F: Mm - hmm .
PhD C: I bet you could pick that up in the acoustics . Just because your gaze is also correlated with the directionality of your voice .
Professor E: Uh - huh . Could be .
Postdoc B: Can we
Professor E: Yeah . That would be tou
Grad G: Oh , that would be interesting .
PhD C: Yeah , so that , I mean , to even know um when {disfmarker}
PhD D: Yeah .
PhD C: Yeah , if you have the P Z Ms you should be able to pick up what a person is looking at from their voice .
Grad G: Well , especially with Morgan , with the way we have the microphones arranged . I 'm sort of right on axis and it would be very hard to tell .
PhD C: Right .
Grad G: Uh .
Postdoc B: Oh , but you 'd have the {disfmarker}
PhD C: Put Morgan always like this
Postdoc B: You 'd have fainter {disfmarker}
PhD C: and {disfmarker}
Postdoc B: Wouldn't you get fainter reception out here ?
Professor E: Well , these {disfmarker}
Grad G: Sure , but I think if I 'm talking like this ? Right now I 'm looking at Jane and talking , now I 'm looking at Chuck and talking , I don't think the microphones would pick up that difference .
PhD C: But you don't have this {disfmarker} this problem .
Postdoc B: I see .
PhD C: Morgan is the one who does this most .
Grad G: So if I 'm talking at you , or I 'm talking at you .
Professor E: I probably been affect No , I th I think I 've been affected by too many conversations where we were talking about lawyers and talking about {disfmarker} and concerns about " oh gee is somebody going to say something bad ? " and so on .
Grad G: Lawyers .
Professor E: And so I {disfmarker} so I 'm {disfmarker} I 'm tending to stay away from people 's names even though uh {disfmarker}
Postdoc B: I am too .
PhD C: Even though you could pick up later on , just from the acoustics who you were t who you were looking at .
Postdoc B: I am too .
Grad G: And we did mention who " he " was .
PhD C: Yeah .
Professor E: Yeah .
PhD F: Right , but I missed it .
Grad G: Early in the conversation .
PhD F: But {disfmarker} it was uh {disfmarker}
PhD C: Yeah , yeah .
Professor E: Yeah .
Grad G: Do {disfmarker} Sh - Can I say
Professor E: Yeah . No no , there 's {disfmarker}
PhD F: Yeah .
Grad G: or {disfmarker} or is that just too sensitive ?
Professor E: No no , it isn't sensitive at all .
Postdoc B: Well {disfmarker}
Professor E: I was just {disfmarker} I was just {disfmarker} I was overreacting just because we 've been talking about it .
Postdoc B: And in fact , it is {disfmarker} it is {disfmarker} it is sensitive .
PhD C: No , but that {disfmarker} it 's interesting .
Professor E: It 's OK to {disfmarker}
Postdoc B: I {disfmarker} I came up with something from the Human Subjects people that I wanted to mention . I mean , it fits into the m area of the mundane , but they did say {disfmarker} You know , I asked her very specifically about this clause of how , um , you know , it says " no individuals will be identified uh , " in any publication using the data . " OK , well , individuals being identified , let 's say you have a {disfmarker} a snippet that says , " Joe s uh thinks such - and - such about {disfmarker} about this field , but I think he 's wrongheaded . " Now I mean , we 're {disfmarker} we 're gonna be careful not to have the " wrongheaded " part in there , but {disfmarker} but you know , let 's say we say , you know , " Joe used to think so - and - so about this area , in his publication he says that but I think he 's changed his mind . " or whatever . Then the issue of {disfmarker} of being able to trace Joe , because we know he 's well - known in this field , and all this and {disfmarker} and tie it to the speaker , whose name was just mentioned a moment ago , can be sensitive .
Professor E: b But I {disfmarker}
Postdoc B: So I think it 's really {disfmarker} really kind of adaptive and wise to not mention names any more than we have to because if there 's a slanderous aspect to it , then how much to we wanna be able to have to remove ?
Professor E: Yeah , well , there 's that . But I {disfmarker} I mean I think also to some extent it 's just educating the Human Subjects people , in a way , because there 's {disfmarker} If uh {disfmarker} You know , there 's court transcripts , there 's {disfmarker} there 's transcripts of radio shows {disfmarker} I mean people say people 's names all the time . So I think it {disfmarker} it can't be bad to say people 's names . It 's just that {disfmarker} i I mean you 're right that there 's more poten If we never say anybody 's name , then there 's no chance of {disfmarker} of {disfmarker} of slandering anybody ,
PhD C: But , then it won't {disfmarker} I mean , if we {disfmarker} if we {disfmarker}
Professor E: but {disfmarker}
Grad G: It 's not a meeting .
PhD C: Yeah . I mean we should do whatever 's natural in a meeting if {disfmarker} if we weren't being recorded .
Professor E: Yeah . Right , so I {disfmarker} So my behavior is probably not natural .
PhD C: " If Person X {disfmarker} "
Professor E: So .
Postdoc B: Well , my feeling on it was that it wasn't really important who said it , you know .
Professor E: Yeah .
PhD F: Well , if you ha since you have to um go over the transcripts later anyway , you could make it one of the jobs of the {pause} people who do that to mark
Grad G: Well , we t we t we talked about this during the anon anonymization .
PhD F: Right .
Grad G: If we wanna go through and extract from the audio and the written every time someone says a name . And I thought that our conclusion was that we didn't want to do that .
Professor E: Yeah , we really can't . But a actually , I 'm sorry . I really would like to push {disfmarker} finish this off .
Postdoc B: I understand . No I just {disfmarker} I just was suggesting that it 's not a bad policy p potentially .
Professor E: So it 's {disfmarker}
Postdoc B: So , we need to talk about this later .
Professor E: Yeah , I di I didn't intend it an a policy though .
Postdoc B: Uh - huh .
Professor E: It was {disfmarker} it was just it was just unconscious {disfmarker} well , semi - conscious behavior . I sorta knew I was doing it but it was {disfmarker}
PhD F: Well , I still don't know who " he " is .
Professor E: I {disfmarker} I do I don't remember who " he " is .
PhD C: No , you have to say , you still don't know who " he " is , with that prosody .
Professor E: Ah . Uh , we were talking about Dan at one point {comment} and we were talking about Lokendra at another point .
Postdoc B: Yeah , depends on which one you mean .
Professor E: And I don't {disfmarker} I don't remember which {disfmarker} which part .
PhD F: Oh .
PhD C: It 's ambiguous , so it 's OK .
Professor E: Uh , I think {disfmarker}
Grad G: Well , the inference structures was Lokendra .
PhD F: But no . The inference stuff was {disfmarker} was {disfmarker} was Lokendra .
Professor E: Yeah . Yeah . Yeah .
PhD F: OK . That makes sense , yeah .
PhD C: And the downsampling must have been Dan .
Professor E: Um {disfmarker}
Grad G: Yeah .
Professor E: Good {disfmarker} Yeah .
PhD C: It 's an inference .
Professor E: Yeah , you could do all these inferences , yeah .
Grad G: Yeah .
Professor E: Yeah . Um , I {disfmarker} I would like to move it into {disfmarker} into uh what Jose uh has been doing
Postdoc B: Yeah .
Professor E: because he 's actually been doing something .
PhD D: Uh - huh . OK .
Professor E: So . {vocalsound} Right .
PhD F: As opposed to the rest of us .
PhD D: Well - {comment} {vocalsound} OK . I {disfmarker} I remind that me {disfmarker} my first objective eh , in the project is to {disfmarker} to study difference parameters to {disfmarker} to find a {disfmarker} a good solution to detect eh , the overlapping zone in eh speech recorded . But eh , {vocalsound} tsk , {comment} {vocalsound} ehhh {comment} In that way {comment} I {disfmarker} {vocalsound} I {disfmarker} {vocalsound} I begin to {disfmarker} to study and to analyze the ehn {disfmarker} the recorded speech eh the different session to {disfmarker} to find and to locate and to mark eh the {disfmarker} the different overlapping zone . And eh so eh I was eh {disfmarker} I am transcribing the {disfmarker} the first session and I {disfmarker} I have found eh , eh one thousand acoustic events , eh besides the overlapping zones , eh I {disfmarker} I {disfmarker} I mean the eh breaths eh aspiration eh , eh , talk eh , eh , clap , eh {disfmarker} {comment} I don't know what is the different names eh you use to {disfmarker} to name the {disfmarker} the {pause} n speech
PhD A: Nonspeech sounds ?
PhD D: Yeah .
Grad G: Oh , I don't think we 've been doing it at that level of detail . So .
PhD D: Yeah . Eh , {vocalsound} I {disfmarker} I {disfmarker} I do I don't need to {disfmarker} to {disfmarker} to mmm {vocalsound} {disfmarker} to m to label the {disfmarker} the different acoustic , but I prefer because eh I would like to {disfmarker} to study if eh , I {disfmarker} I will find eh , eh , a good eh parameters eh to detect overlapping I would like to {disfmarker} to {disfmarker} to test these parameters eh with the {disfmarker} another eh , eh acoustic events , to nnn {disfmarker} {vocalsound} to eh {disfmarker} to find what is the ehm {disfmarker} the false {disfmarker} eh , the false eh hypothesis eh , nnn , which eh are produced when we use the {disfmarker} the ehm {disfmarker} this eh parameter {disfmarker} eh I mean pitch eh , eh , difference eh , feature {disfmarker}
Grad G: Mm - hmm .
PhD A: You know {disfmarker} I think some of these um that are the nonspeech overlapping events may be difficult even for humans to tell that there 's two there .
Grad G: So it was {disfmarker}
PhD D: Yeah .
PhD A: I mean , if it 's a tapping sound , you wouldn't necessarily {disfmarker} or , you know , something like that , it 'd be {disfmarker} it might be hard to know that it was two separate events .
PhD D: Yeah . Yeah . Yeah . Yeah .
Grad G: Well {disfmarker} You weren't talking about just overlaps
PhD D: Ye
Grad G: were you ? You were just talking about acoustic events .
PhD D: I {disfmarker} I {disfmarker} I {disfmarker} I t I t I talk eh about eh acoustic events in general ,
Grad G: Someone starts , someone stops {disfmarker} Yeah .
PhD A: Oh .
PhD D: but eh my {disfmarker} my objective eh will be eh to study eh overlapping zone .
Grad G: Mm - hmm .
PhD D: Eh ? {comment} n Eh in twelve minutes I found eh , eh one thousand acoustic events .
Professor E: How many overlaps were there uh in it ? No no , how many of them were the overlaps of speech , though ?
PhD D: How many ? Eh almost eh three hundred eh in one session
Grad G: Oh , God !
PhD D: in five {disfmarker} eh in forty - five minutes .
PhD A: Three hundred overlapping speech {disfmarker}
PhD D: Alm - Three hundred overlapping zone .
Grad G: Ugh .
PhD C: Overlapping speech .
PhD D: With the overlapping zone , overlapping speech {disfmarker} speech what eh different duration .
PhD A: Mm - hmm .
Professor E: Sure .
Postdoc B: Does this {disfmarker} ? So if you had an overlap involving three people , how many times was that counted ?
PhD D: Yeah , three people , two people . Eh , um I would like to consider eh one people with difference noise eh in the background , be
Professor E: No no , but I think what she 's asking is {pause} if at some particular for some particular stretch you had three people talking , instead of two , did you call that one event ?
PhD D: Oh . Oh . Yeah . I consider one event eh for th for that eh for all the zone . This {disfmarker} th I {disfmarker} I {disfmarker} I con I consider {disfmarker} I consider eh an acoustic event , the overlapping zone , the period where three speaker or eh {disfmarker} are talking together .
Grad G: Well {disfmarker} So let 's {disfmarker}
Postdoc B: For
Grad G: So let 's say me and Jane are talking at the same time , and then Liz starts talking also over all of us . How many events would that be ?
PhD D: So - I don't understand .
Grad G: So , two people are talking , {comment} and then a third person starts talking .
PhD D: Yeah ?
Grad G: Is there an event right here ?
PhD D: Eh no . No no . For me is the overlapping zone , because {disfmarker} because you {disfmarker} you have s you have more one {disfmarker} eh , more one voice eh , eh produced in a {disfmarker} in {disfmarker} in a moment .
Professor E: I see .
Grad G: So i if two or more people are talking .
Professor E: OK . Yeah . So I think {disfmarker} Yeah . We just wanted to understand how you 're defining it .
PhD D: Yeah . If
Professor E: So then , in the region between {disfmarker} since there {disfmarker} there is some continuous region , in between regions where there is only one person speaking .
PhD D: Uh - huh .
Professor E: And one contiguous region like that you 're calling an event .
PhD D: Uh - huh .
Professor E: Is it {disfmarker} Are you calling the beginning or the end of it the event ,
PhD D: Yeah .
Professor E: or are you calling the entire length of it the event ?
PhD D: I consider the {disfmarker} the , nnn {disfmarker} the nnn , nnn {disfmarker} eh , the entirety eh , eh , all {disfmarker} all the time there were {disfmarker} the voice has overlapped .
Professor E: OK .
PhD D: This is the idea . But eh I {disfmarker} I don't distinguish between the {disfmarker} the numbers of eh speaker . Uh , I 'm not considering {vocalsound} eh the {disfmarker} the {disfmarker} ehm {vocalsound} eh , the fact of eh , eh , for example , what did you say ? Eh at first eh , eh two talkers are uh , eh speaking , and eh , eh a third person eh join to {disfmarker} to that . For me , it 's eh {disfmarker} it 's eh , all overlap zone , with eh several numbers of speakers is eh , eh the same acoustic event . Wi - but {disfmarker} uh , without any mark between the zone {disfmarker} of the overlapping zone with two speakers eh speaking together , and the zone with the three speakers .
Postdoc B: That would j just be one .
PhD D: It {disfmarker} One . One .
Postdoc B: OK .
PhD D: Eh , with eh , a beginning mark and the ending mark . Because eh {vocalsound} for me , is the {disfmarker} is the zone with eh some kind of eh distortion the spectral .
Professor E: Got it .
PhD D: I don't mind {disfmarker} By the moment , by the moment .
Grad G: Well , but {disfmarker} But you could imagine that three people talking has a different spectral characteristic than two .
PhD D: I {disfmarker} I don't {disfmarker} Yeah , but eh {disfmarker} but eh I have to study . {comment} What will happen in a general way ,
Professor E: Could .
Grad G: So . You had to start somewhere .
Professor E: Yeah . We just w
PhD C: So there 's a lot of overlap .
PhD D: I {disfmarker} {vocalsound} I don't know what eh will {disfmarker} will happen with the {disfmarker}
Grad G: Yep .
PhD C: So .
Grad G: That 's a lot of overlap ,
PhD D: Yeah ?
Professor E: So again , that 's {disfmarker} that 's three {disfmarker} three hundred in forty - five minutes that are {disfmarker} that are speakers , just speakers .
Grad G: yeah , for forty - five minutes .
PhD D: Yeah . Yeah .
Professor E: Uh - huh . OK . Yeah .
Postdoc B: But a {disfmarker} a {disfmarker} a th
Professor E: So that 's about eight per minute .
Postdoc B: But a thousand events in twelve minutes , that 's {disfmarker}
PhD D: Yeah , {pause} but {disfmarker} Yeah .
PhD C: But that can include taps .
PhD D: But {disfmarker}
Professor E: Uh . Yeah .
Postdoc B: Well , but a thousand taps in eight minutes is a l in twelve minutes is a lot .
PhD D: General .
PhD C: Actually {disfmarker}
PhD D: I {disfmarker} I con I consider {disfmarker} I consider acoustic events eh , the silent too .
Postdoc B: Silent .
Grad G: Silence starting or silence ending {disfmarker}
PhD D: Yeah , silent , ground to {disfmarker} bec to detect {disfmarker} eh because I consider acoustic event all the things are not eh speech .
PhD C: Oh , OK .
Professor E: Mm - hmm .
PhD A: Oh .
PhD D: In ge in {disfmarker} in {disfmarker} in a general point of view .
PhD C: Oh .
Professor E: OK , so how many of those thousand were silence ?
PhD C: Alright .
PhD D: in the per
PhD F: Not speech {disfmarker} not speech or too much speech .
PhD D: Too much speech .
Professor E: Right . So how many of those thousand were silence , silent sections ?
PhD D: Yeah . Uh silent , I {disfmarker} I {disfmarker} I {disfmarker} I don't {disfmarker} I {disfmarker} I haven't the {disfmarker} eh I {disfmarker} I would like to {disfmarker} to do a stylistic study
Professor E: Yeah .
PhD D: and give you eh with the report eh from eh the {disfmarker} the study from the {disfmarker} the {disfmarker} the session {disfmarker} one session .
Professor E: Yeah . Yeah .
PhD D: And I {disfmarker} I found that eh another thing . When eh {vocalsound} eh I w I {disfmarker} {vocalsound} I was eh look at eh nnn , the difference speech file , um , for example , eh if eh we use the ehm {disfmarker} the mixed file , to {disfmarker} to transcribe , the {disfmarker} the events and the words , I {disfmarker} I saw that eh the eh speech signal , collected by the eh this kind of mike {disfmarker} eh of this kind of mike , eh are different from the eh mixed signal eh , we eh {disfmarker} collected by headphone .
Grad G: Yep .
PhD D: And {disfmarker} It 's right .
Professor E: Yeah .
Grad G: Right .
PhD D: But the problem is {vocalsound} the following . The {disfmarker} the {disfmarker} the {disfmarker} I {disfmarker} I {disfmarker} I knew that eh the signal eh , eh would be different , but eh the {disfmarker} the problem is eh , eh we eh detected eh difference events in the speech file eh collected by {disfmarker} by that mike uh qui compared with the mixed file . And so if {disfmarker} when you transcribe eh only eh using the nnn {disfmarker} the mixed file , it 's possible {disfmarker} eh if you use the transcription to evaluate a different system , it 's possible you eh {disfmarker} in the eh i and you use the eh speech file collected by the eh fet mike , to eh {disfmarker} to nnn {disfmarker} to do the experiments {pause} with the {disfmarker} the system ,
Professor E: Mm - hmm .
Grad G: Right .
PhD D: its possible to evaluate eh , eh {disfmarker} or to consider eh acoustic events that {disfmarker} which you marked eh in the mixed file , but eh they don't appear in the eh speech signal eh collected by the {disfmarker} by the mike .
Grad G: Right . The {disfmarker} the reason that I generated the mixed file was for IBM to do word level transcription , not speech event transcription .
PhD D: Yeah . Yeah . Oh , it 's a good idea . It 's a good idea I think .
Grad G: So I agree that if someone wants to do speech event transcription , that the mixed signals here {disfmarker}
PhD D: Yeah .
Grad G: I mean , if I 'm tapping on the table , you it 's not gonna show up on any of the mikes , but it 's gonna show up rather loudly in the PZM .
PhD D: Yeah . Yeah . Yeah . So and I {disfmarker} I {disfmarker} {vocalsound} I say eh that eh , eh , or this eh only because eh I c I {disfmarker} I {disfmarker} {vocalsound} in my opinion , it 's necessary to eh {disfmarker} to eh {disfmarker} to put the transcription on the speech file , collected by the objective signal .
Grad G: So .
PhD D: I mean the {disfmarker} the {disfmarker} the signal collected by the {disfmarker} eh , the real mike in the future , in the prototype to {disfmarker} to eh correct the initial eh segmentation eh with the eh real speech
Professor E: Mm - hmm . The {disfmarker} the {disfmarker} the far - field , yeah .
PhD D: you have to {disfmarker} to analyze {disfmarker} you have to {disfmarker} to process . Because I {disfmarker} I found a difference .
Professor E: Yeah , well , just {disfmarker} I mean , just in that {disfmarker} that one s ten second , or whatever it was , example that Adam had that {disfmarker} that we {disfmarker} we passed on to others a few months ago , there was that business where I g I guess it was Adam and Jane were talking at the same time and {disfmarker} and uh , in the close - talking mikes you couldn't hear the overlap , and in the distant mike you could . So yeah , it 's clear that if you wanna study {disfmarker} if you wanna find all the places where there were overlap , it 's probably better to use a distant mike .
PhD F: That 's good .
Professor E: On the other hand , there 's other phenomena that are going on at the same time for which it might be useful to look at the close - talking mikes ,
PhD D: Yeah .
PhD C: But why can't you use the combination of the close - talking mikes , time aligned ?
Professor E: so it 's {disfmarker}
Grad G: If you use the combination of the close - talking mikes , you would hear Jane interrupting me , but you wouldn't hear the paper rustling . And so if you 're interested in {disfmarker}
PhD C: I {disfmarker} I mean if you 're interested in speakers overlapping other speakers and not the other kinds of nonspeech , that 's not a problem ,
Professor E: Some {comment} of it 's masking {disfmarker} masked .
PhD D: Yeah .
PhD A: Were you interrupting him or was he interrupting you ?
Professor E: Right .
PhD C: right ?
Grad G: Right .
PhD D: Yeah .
Grad G: Although the other issue is that the {pause} mixed close - talking mikes {disfmarker} I mean , I 'm doing weird normalizations and things like that .
PhD C: But it 's known .
PhD D: Yeah .
PhD C: I mean , the normalization you do is over the whole conversation
Grad G: Yep .
PhD C: isn't it , over the whole meeting .
Grad G: Right . Yep .
PhD C: So if you wanted to study people overlapping people , that 's not a problem .
PhD D: I {disfmarker} I {disfmarker} I think eh I saw the nnn {disfmarker} the {disfmarker} eh but eh I eh {disfmarker} I have eh any results . I {disfmarker} I {disfmarker} I saw the {disfmarker} the speech file collected by eh the fet mike , and eh eh signal eh to eh {disfmarker} to noise eh relation is eh low . It 's low .
Professor E: Mm - hmm .
PhD D: It 's very low . You would comp if we compare it with eh the headphone .
Grad G: Yep .
PhD D: And I {disfmarker} I found that nnn {disfmarker} that eh , {vocalsound} ehm , pr probably ,
Grad G: Did {disfmarker} Did you
PhD D: I 'm not sure eh by the moment , but it 's {disfmarker} it 's probably that eh a lot of eh , {vocalsound} eh for example , in the overlapping zone , on eh {disfmarker} in {disfmarker} in several eh parts of the files where you {disfmarker} you can find eh , eh {vocalsound} eh , smooth eh eh speech eh from eh one eh eh talker in the {disfmarker} in the meeting ,
Professor E: Mm - hmm . Mm - hmm .
PhD D: it 's probably in {disfmarker} in that eh {disfmarker} in {disfmarker} in those files you {disfmarker} you can not find {disfmarker} you can not process because eh it 's confused with {disfmarker} with noise .
Professor E: Mm - hmm .
PhD D: And there are {vocalsound} a lot of I think . But I have to study with more detail . But eh my idea is to {disfmarker} to process only {pause} nnn , this eh {disfmarker} nnn , this kind of s of eh speech . Because I think it 's more realistic . I 'm not sure it 's a good idea , but eh {disfmarker}
Professor E: No {disfmarker} i
Grad G: Well , it 's more realistic but it 'll {disfmarker} it 'll be a lot harder .
PhD D: Yeah .
Professor E: Well , it 'd be hard , but on the other hand as you point out , if your {disfmarker} if i if {disfmarker} if your concern is to get uh the overlapping people {disfmarker} people 's speech , you will {disfmarker} you will get that somewhat better .
PhD D: Mm - hmm . Yeah .
Professor E: Um , Are you making any use {disfmarker} uh you were {disfmarker} you were working with th the data that had already been transcribed .
PhD D: With {disfmarker} By Jane .
Professor E: Does it uh {disfmarker} Yes .
PhD D: Yeah .
Professor E: Now um did you make any use of that ? See I was wondering cuz we st we have these ten hours of other stuff that is not yet transcribed .
PhD D: Yeah . Yeah .
Professor E: Do you {disfmarker}
PhD D: The {disfmarker} the transcription by Jane , t eh i eh , I {disfmarker} I {disfmarker} I want to use to {disfmarker} to nnn , {vocalsound} eh to put {disfmarker} i i it 's a reference for me . But eh the transcription {disfmarker} eh for example , I {disfmarker} I don't {disfmarker} I {disfmarker} I 'm not interested in the {disfmarker} in the {disfmarker} in the words , transcription words , eh transcribed eh eh in {disfmarker} eh follow in the {disfmarker} {vocalsound} in the {disfmarker} in the speech file , but eh eh Jane eh for example eh put a mark eh at the beginning eh of each eh talker , in the {disfmarker} in the meeting , um eh she {disfmarker} she nnn includes information about the zone where eh there are eh {disfmarker} there is an overlapping zone . But eh there isn't any {disfmarker} any mark , time {disfmarker} temporal mark , to {disfmarker} to c eh {disfmarker} to mmm {vocalsound} {disfmarker} e - heh , to label {comment} the beginning and the end of the {disfmarker} of the
Professor E: Mm - hmm . OK . Right , so she is {disfmarker}
PhD D: ta I 'm {disfmarker} I {disfmarker} I {disfmarker} I think eh we need this information to
Professor E: Right . So the twelve {disfmarker} you {disfmarker} you {disfmarker} it took you twelve hours {disfmarker} of course this included maybe some {disfmarker} some time where you were learning about what {disfmarker} what you wanted to do , but {disfmarker} but uh , it took you something like twelve hours to mark the forty - five minutes , your
Grad G: Twelve minutes .
PhD D: Twelve minutes .
Professor E: s Twelve minutes !
PhD D: Twelve minutes . Twelve .
Professor E: I thought you did forty - five minutes of {disfmarker}
PhD D: No , forty - five minutes is the {disfmarker} is the session , all the session .
Postdoc B: Oh .
Professor E: Oh , you haven't done the whole session .
PhD D: Yeah , all is the {vocalsound} the session .
Professor E: This is just twelve minutes .
PhD D: Tw - twelve hours of work to {disfmarker} {vocalsound} to segment eh and label eh twelve minutes from a session of part {disfmarker} of f
Professor E: Oh . So {comment} let me back up again . So the {disfmarker} when you said there were three hundred speaker overlaps ,
PhD D: Yeah .
Professor E: that 's in twelve minutes ?
PhD D: No no no . I {disfmarker} I consider all the {disfmarker} all the session because eh I {disfmarker} I count the nnn {disfmarker} the nnn {disfmarker} the overlappings marked by {disfmarker} by Jane ,
Professor E: Oh , OK .
Postdoc B: Oh , I see .
PhD D: in {disfmarker} in {disfmarker} in {disfmarker} in the {pause} fin in {disfmarker} in the {pause} forty - five minutes .
Professor E: OK . So it 's three hundred in forty - five minutes , but you have {disfmarker} you have time uh , uh marked {disfmarker} twelve minute {disfmarker} the {disfmarker} the {disfmarker} the um overlaps in twelve minutes of it .
PhD D: Yeah .
Professor E: Got it .
PhD F: So , can I ask {disfmarker} {vocalsound} can I ask whether you found {disfmarker} uh , you know , how accurate uh Jane 's uh uh labels were as far as {disfmarker}
Grad G: Well , not just the overlaps , everything .
PhD F: you know , did she miss some overlaps ? or did she n ?
PhD D: But , by {disfmarker} by the moment , I {disfmarker} I don't compare , my {disfmarker} my temporal mark with eh Jane , but eh I {disfmarker} I want to do it . Because eh eh i per perhaps I have eh errors in the {disfmarker} in the marks , I {disfmarker} and if I {disfmarker} I compare with eh Jane , it 's probably I {disfmarker} I {disfmarker} I can correct and {disfmarker} and {disfmarker} and {disfmarker} to get eh eh a more accurately eh eh transcription in the file .
Professor E: Yeah .
Grad G: Well , also Jane {disfmarker} Jane was doing word level .
PhD D: Yeah .
Professor E: Yeah .
Grad G: So we weren't concerned with {comment} exactly when an overlap started and stopped .
PhD F: Right . Right .
PhD C: Well , not only a word level , but actually
PhD D: Well {disfmarker}
PhD F: I 'm expect I 'm not expecting {disfmarker}
PhD D: No , it 's {disfmarker}
PhD C: I mean , you didn't need to show the exact point of interruption , you just were showing at the level of the phrase or the level of the speech spurt , or {disfmarker}
Grad G: Right .
Professor E: Mm - hmm .
Grad G: Yep .
Postdoc B: Well {disfmarker}
PhD D: Yeah . Yeah .
Postdoc B: Well , yeah , b yeah , I would say time bin . So my {disfmarker} my goal is to get words with reference to a time bin , {pause} beginning and end point .
PhD C: Yeah .
PhD D: Yeah .
PhD C: Right .
PhD D: Yeah .
Postdoc B: And {disfmarker} and sometimes , you know , it was like you could have an overlap where someone said something in the middle ,
PhD D: Yeah .
Postdoc B: but , yeah , w it just wasn't important for our purposes to have it that {disfmarker} i disrupt that unit in order to have , you know , a the words in the order in which they were spoken , it would have {disfmarker} it would have been hard with the interface that we have .
PhD D: Yeah .
Postdoc B: Now , my {disfmarker} a Adam 's working on a of course , on a revised overlapping interface ,
PhD D: Uh - huh .
Grad G: Right .
PhD D: I {disfmarker} I {disfmarker} I think {disfmarker} It 's {disfmarker} it 's a good eh work ,
Postdoc B: but {disfmarker}
PhD D: but eh I think we need eh eh more information .
PhD F: No , of course .
Postdoc B: Yeah .
PhD F: I expect you to find more overlaps than {disfmarker} than Jane
Grad G: Always need more for {disfmarker}
Postdoc B: Yeah .
PhD D: No , no . I {disfmarker} I have to go to {disfmarker}
PhD F: because you 're looking at it at a much more detailed level .
PhD D: I want eh {disfmarker} I wanted to eh compare the {disfmarker} the transcription .
Professor E: I have {disfmarker}
Grad G: But if it takes sixty to one {disfmarker}
Professor E: Well , I but I have a suggestion about that . Um , obviously this is very , very time - consuming , and you 're finding lots of things which I 'm sure are gonna be very interesting , but in the interests of making progress , uh might I s how {disfmarker} how would it affect your time if you only marked speaker overlaps ?
PhD D: Only .
Professor E: Yes .
PhD D: Yeah .
Professor E: Do not mark any other events ,
PhD D: Uh - huh .
Professor E: but only mark speaker {disfmarker} Do you think that would speed it up quite a bit ?
PhD D: OK . OK . I {disfmarker} I {disfmarker} I {disfmarker} I w I {disfmarker} I wanted to {disfmarker}
Professor E: Do y do you think that would speed it up ? Uh , speed up your {disfmarker} your {disfmarker} your marking ?
PhD D: nnn , I don't understand very .
Professor E: It took you a long time {pause} to mark twelve minutes .
PhD D: Yeah . Oh , yeah , yeah .
Professor E: Now , my suggestion was for the other thirty - three {disfmarker}
PhD D: On - only to mark {disfmarker} only to mark overlapping zone , but {disfmarker}
Professor E: Yeah , and my question is , if you did that , if you followed my suggestion , would it take much less time ?
PhD D: Oh , yeah . Sure .
Professor E: Yeah OK .
PhD D: Yeah sure .
Professor E: Then I think it 's a good idea .
PhD D: Sure sure .
Professor E: Then I think it 's a good idea , because it
PhD D: Sure , because I {disfmarker} I need a lot of time to {disfmarker} to put the label or to do that . Yeah .
Professor E: Yeah , I mean , we we know that there 's noise .
Grad G: And
PhD D: Uh - huh .
Professor E: There 's {disfmarker} there 's uh continual noise uh from fans and so forth , and there is uh more impulsive noise from uh taps and so forth
PhD D: Yeah .
Professor E: and {disfmarker} and something in between with paper rustling . We know that all that 's there and it 's a g worthwhile thing to study , but obviously it takes a lot of time to mark all of these things .
PhD D: Yeah .
Professor E: Whereas th i I would think that uh you {disfmarker} we can study more or less as a distinct phenomenon the overlapping of people talking .
PhD D: Uh - huh . OK . OK .
Professor E: So . Then you can get the {disfmarker} Cuz you need {disfmarker} If it 's three hundred uh {disfmarker} i i it sounds like you probably only have fifty or sixty or seventy events right now that are really {disfmarker}
PhD D: Yeah .
Professor E: And {disfmarker} and you need to have a lot more than that to have any kind of uh even visual sense of {disfmarker} of what 's going on , much less any kind of reasonable statistics .
Grad G: Right .
PhD C: Now , why do you need to mark speaker overlap by hand if you can infer it from the relative energy in the {disfmarker}
Grad G: Well , that 's {disfmarker} That 's what I was gonna bring up .
PhD C: I mean , you shouldn't need to do this p completely by hand ,
Professor E: Um , OK , yeah . So let 's back up because you weren't here for an earlier conversation .
PhD C: right ? I 'm sorry .
Professor E: So the idea was that what he was going to be doing was experimenting with different measures such as the increase in energy , such as the energy in the LPC residuals , such as {disfmarker} I mean there 's a bunch of things {disfmarker} I mean , increased energy is - is sort of an obvious one .
PhD C: Mm - hmm . In the far - field mike .
Professor E: Yeah .
PhD C: Oh , OK .
Professor E: Um , and uh , it 's not obvious , I mean , you could {disfmarker} you could do the dumbest thing and get {disfmarker} get it ninety percent of the time . But when you start going past that and trying to do better , it 's not obvious what combination of features is gonna give you the {disfmarker} you know , the right detector . So the idea is to have some ground truth first . And so the i the idea of the manual marking was to say " OK this , i you know , it 's {disfmarker} it 's really here " .
PhD A: But I think Liz is saying why not get it out of the transcripts ?
PhD C: What I mean is {pause} get it from the close - talking mikes .
Professor E: Uh , yeah .
PhD C: A or ge get a first pass from those ,
Professor E: We t we t w we t we talked about that .
PhD C: and then go through sort of {disfmarker} It 'd be a lot faster probably to {disfmarker}
PhD F: And you can {disfmarker}
Grad G: Yeah , that 's his , uh {disfmarker}
Professor E: We {disfmarker} we {disfmarker} we talked about that . s But so it 's a bootstrapping thing and the thing is ,
PhD C: Yeah , I just {disfmarker}
Professor E: the idea was , i we i i we thought it would be useful for him to look at the data anyway , and {disfmarker} and then whatever he could mark would be helpful ,
PhD C: Right .
Professor E: and we could {disfmarker} Uh it 's a question of what you bootstrap from . You know , do you bootstrap from a simple measurement which is right most of the time and then you g do better , or do you bootstrap from some human being looking at it and then {disfmarker} then do your simple measurements , uh from the close - talking mike . I mean , even with the close - talking mike you 're not gonna get it right all the time .
PhD C: Well , that 's what I wonder , because um {disfmarker} or how bad it is ,
Professor E: Well
PhD C: be um , because that would be interesting
Grad G: I 'm working on a program to do that , and {disfmarker}
PhD C: especially because the bottleneck is the transcription . Right ? I mean , we 've got a lot more data than we have transcriptions for . We have the audio data , we have the close - talking mike ,
Professor E: Yeah .
PhD C: so I mean it seems like one kind of project that 's not perfect , but {disfmarker} um , that you can get the training data for pretty quickly is , you know , if you infer form the close - talking mikes where the on - off points are of speech ,
Professor E: Right , we discussed that .
PhD C: you know , how can we detect that from a far - field ?
Grad G: And {disfmarker}
Postdoc B: Oh .
Grad G: I 've {disfmarker} I 've written a program to do that ,
PhD C: OK , I 'm sorry I missed the {disfmarker}
Grad G: and it , uh {disfmarker}
Professor E: It 's OK .
Grad G: and {disfmarker} so {disfmarker} but it 's {disfmarker} it 's doing something very , very simple . It just takes a threshold , based on {disfmarker} on the volume ,
PhD C: Uh - huh .
PhD F: Or you can set the threshold low and then weed out the false alarms by hand .
PhD C: Right , by hand . Yeah .
PhD F: Yeah .
Grad G: um , and then it does a median filter , and then it looks for runs . And , it seems to work , I 've {disfmarker} I 'm sort of fiddling with the parameters , to get it to actually generate something , and I haven't {disfmarker} I don't {disfmarker} what I 'm working on {disfmarker} was working on {disfmarker} was getting it to a form where we can import it into the user interface that we have , {pause} into Transcriber . And so {disfmarker} I told {disfmarker} I said it would take about a day . I 've worked on it for about half a day ,
Grad H: I have to go .
Grad G: so give me another half day and I we 'll have something we can play with .
PhD C: OK .
Professor E: See , this is where we really need the Meeting Recorder query stuff to be working , because we 've had these meetings and we 've had this discussion about this , and I 'm sort of remembering a little bit about what we decided ,
PhD C: Right . I 'm sorry . I just {disfmarker}
Professor E: but I couldn't remember all of it .
PhD C: It
Professor E: So , I think it was partly that , you know , give somebody a chance to actually look at the data and see what these are like , partly that we have e some ground truth to compare against , you know , when {disfmarker} when he {disfmarker} he gets his thing going ,
Grad G: But {disfmarker}
Professor E: uh , and {disfmarker}
PhD C: Well , it 's definitely good to have somebody look at it . I was just thinking as a way to speed up you know , the amount of {disfmarker}
Postdoc B: Mm - hmm .
Professor E: That was {disfmarker} that was exactly the notion that {disfmarker} that {disfmarker} that we discussed .
PhD C: OK .
Grad G: Thanks .
Postdoc B: Another thing we discussed was um that {disfmarker}
PhD C: It looks good .
Professor E: So .
PhD C: I 'll be in touch . Thanks .
Professor E: S See ya . Yeah .
Postdoc B: Was that um there m {pause} there was this already a script I believe uh that Dan had written , {comment} that uh handle bleedthrough , I mean cuz you have this {disfmarker} this close {disfmarker} you have contamination from other people who speak loudly .
Grad G: Yeah , and I haven't tried using that . It would probably help the program that I 'm doing to first feed it through that . It 's a cross - correlation filter . So I {disfmarker} I haven't tried that , but that {disfmarker} If {disfmarker} It {disfmarker} it might be something {disfmarker} it might be a good way of cleaning it up a little .
Postdoc B: So , some thought of maybe having {disfmarker} Yeah , having that be a preprocessor and then run it through yours .
Grad G: Exactly . Yep .
Professor E: But {disfmarker} but that 's a refinement
Postdoc B: That 's what we were discussing .
Professor E: and I think we wanna see {disfmarker} try the simple thing first , cuz you add this complex thing up uh afterwards that does something good y y yo you sort of wanna see what the simple thing does first .
Grad G: Yep .
Professor E: But uh , having {disfmarker} having somebody have some experience , again , with {disfmarker} with uh {disfmarker} with marking it from a human standpoint , we 're {disfmarker} I mean , I don't expect Jose to {disfmarker} to do it for uh f fifty hours of {disfmarker} {comment} of speech , but I mean we {disfmarker} {comment} if uh {disfmarker} if he could speed up what he was doing by just getting the speaker overlaps so that we had it , say , for forty - five minutes , then at least we 'd have three hundred examples of it .
PhD D: Yeah . Sure . Sure .
Professor E: And when {disfmarker} when uh Adam was doing his automatic thing he could then compare to that and see what it was different .
PhD C: Oh yeah , definitely .
PhD A: You know , I did {disfmarker} I did uh something almost identical to this at one of my previous jobs , and it works pretty well . I mean , i almost exactly what you described , an energy detector with a median filter , you look for runs . And uh , you know , you can {disfmarker}
Grad G: It seemed like the right thing to do .
PhD A: Yeah . I mean , you {disfmarker} you can get y I mean , you get them pretty close .
Grad G: That was with zero literature search .
PhD A: And so I think doing that to generate these possibilities and then going through and saying yes or no on them would be a quick way to {disfmarker} to do it .
Grad G: That 's good validation .
PhD A: Yeah .
Postdoc B: Is this proprietary ?
PhD A: Uh . {comment} No . No .
Grad G: Yeah , do you have a patent on it ?
PhD A: It was when I was working for the government .
Professor E: Oh , then everybody owns it . It 's the people .
Postdoc B: Well , I mean , is this something that we could just co - opt , or is it {disfmarker} ?
PhD A: Nah .
Postdoc B: No . OK .
Professor E: Well , i i i he 's pretty close , anyway . I think {disfmarker} I think it 's {disfmarker}
PhD A: Yeah , he 's {disfmarker} it {disfmarker} it doesn't take a long time .
Postdoc B: Right . I just thought if it was tried and true , then {disfmarker} {comment} and he 's gone through additional levels of {disfmarker} of development .
Grad G: Just output . Although if you {disfmarker} if you have some parameters like what 's a good window size for the median filter {disfmarker}
PhD A: Oh ! {comment} I have to remember . I 'll think about it , and try to remember .
PhD F: And it might be different for government people .
Grad G: That 's alright .
Professor E: Yeah , good enough for government work , as they say .
PhD C: They {disfmarker} they {disfmarker}
PhD A: Di - dif different {disfmarker} different bandwidth .
PhD F: They
Grad G: I was doing pretty short , you know , tenth of a second , {comment} sorts of numbers .
PhD F: OK .
Professor E: Uh , I don't know , it {disfmarker} if {disfmarker} if we want to uh {disfmarker} So , uh , maybe we should move on to other {disfmarker} other things in limited time .
Postdoc B: Can I ask one question about his statistics ? So {disfmarker} so in the tw twelve minutes , um , if we took three hundred and divided it by four , which is about the length of twelve minutes , i Um , I 'd expect like there should be seventy - five overlaps .
Professor E: Yeah .
Postdoc B: Did you find uh more than seventy - five overlaps in that period , or {disfmarker} ?
PhD D: More than ?
Postdoc B: More than {disfmarker} How many overlaps in your twelve minutes ?
PhD D: How many ? Eh , not @ @ I Onl - only I {disfmarker} I transcribe eh only twelve minutes from the
Professor E: Yeah .
PhD D: but eh I {disfmarker} I don't co eh {disfmarker} I don't count eh the {disfmarker} the overlap .
Postdoc B: The overlaps . OK .
PhD D: I consider I {disfmarker} I {disfmarker} The {disfmarker} the nnn {disfmarker} The {disfmarker} the three hundred is eh considered only you {disfmarker} your transcription . I have to {disfmarker} {vocalsound} to finish transcribing . So .
Grad G: I b I bet they 're more , because the beginning of the meeting had a lot more overlaps than {disfmarker} than sort of the middle .
PhD D: Yeah .
Grad G: Middle or end .
Postdoc B: I 'm not sure .
PhD D: Yeah .
Grad G: Because i we 're {disfmarker} we 're dealing with the {disfmarker} Uh , in the early meetings , we 're recording while we 're saying who 's talking on what microphone , {comment} and things like that ,
PhD D: Yeah .
Grad G: and that seems to be a lot of overlap .
Postdoc B: I think it 's an empirical question .
PhD D: Yeah .
Postdoc B: I think we could find that out .
PhD D: Yeah .
Grad G: Yep .
Postdoc B: I 'm {disfmarker} I 'm not sure that the beginning had more .
Professor E: So {disfmarker} so I was gonna ask , I guess about any {disfmarker} any other things that {disfmarker} that {disfmarker} that either of you wanted to talk about , especially since Andreas is leaving in five minutes , that {disfmarker} that you wanna go with .
PhD C: Can I just ask about the data , like very straightforward question is where we are on the amount of data and the amount of transcribed data , just cuz I 'm {disfmarker} I wanted to get a feel for that to sort of be able to know what {disfmarker} what can be done first and like how many meetings are we recording
Professor E: Right so there 's this {disfmarker} this {disfmarker} There 's this forty - five minute piece that Jane transcribed .
PhD C: and {disfmarker}
Professor E: That piece was then uh sent to IBM so they could transcribe so we have some comparison point . Then there 's s a larger piece that 's been recorded and uh put on CD - ROM and sent uh to IBM . Right ? And then we don't know .
PhD C: How many meetings is that ? Like {disfmarker} how many {disfmarker}
Grad G: What 's that ?
Professor E: That was about ten hours , and there was about {disfmarker}
PhD C: t ten {disfmarker} It 's like ten meetings or something ? Uh - huh .
Grad G: Yeah , something like that . And then {disfmarker} then we
PhD A: Ten meetings that have been sent to IBM ?
PhD C: And {disfmarker}
Professor E: Yeah .
Grad G: Well , I haven't sent them yet because I was having this problem with the {pause} missing files .
Professor E: Oh . Oh , that 's right , that had {disfmarker} those have not been sent .
PhD A: H how many total have we recorded now , altogether ?
Professor E: We 're saying about {pause} twelve hours .
Grad G: About twelve {pause} by now . Twelve or thirteen .
PhD C: Uh - huh . And we 're recording only this meeting , like continuously we 're only recording this one now ? or {disfmarker} ?
Professor E: No . No , so the {disfmarker} the {disfmarker} that 's the {disfmarker} that 's the biggest one {disfmarker} uh , chunk so far ,
Grad G: Nope .
PhD A: It was the morning one .
PhD C: OK .
Professor E: but there 's at least one meeting recorded of uh the uh uh natural language guys .
Grad G: Jerry .
PhD C: Do they meet every week ,
Professor E: And then there {disfmarker}
PhD C: or every {disfmarker}
Professor E: Uh , they do . w w And we talked to them about recording some more and we 're going to , uh , we 've started having a morning meeting , today uh i starting a w a week or two ago , on the uh front - end issues , and we 're recording those , uh there 's a network services and applications group here who 's agreed to have their meetings recorded ,
PhD C: Great .
Professor E: and we 're gonna start recording them . They 're {disfmarker} They meet on Tuesdays . We 're gonna start recording them next week . So actually , we 're gonna h start having a {disfmarker} a pretty significant chunk and so , you know , {vocalsound} Adam 's sort of struggling with trying to get things to be less buggy , and come up quicker when they do crash and stuff {disfmarker} things like that , now that uh {disfmarker} {vocalsound} the things are starting to happen . So right now , yeah , I th I 'd say the data is predominantly meeting meetings , but there are scattered other meetings in it and that {disfmarker} that amount is gonna grow uh so that the meeting meetings will probably ultimately {disfmarker} i if we 're {disfmarker} if we collect fifty or sixty hours , the meeting meetings it will probably be , you know , twenty or thirty percent of it , not {disfmarker} not {disfmarker} not eighty or ninety . But .
PhD C: So there 's probably {disfmarker} there 's three to four a week ,
Grad G: That 's what we 're aiming for .
PhD C: that we 're aiming for .
Professor E: Yeah .
PhD C: And they 're each about an hour or something .
Professor E: Yeah , yeah .
Grad G: Although {disfmarker} Yeah . We 'll find out tomorrow whether we can really do this or not .
PhD C: So {disfmarker} OK .
Professor E: Yeah and th the {disfmarker} the other thing is I 'm not pos I 'm sort of thinking as we 've been through this a few times , that I really don't know {disfmarker} maybe you wanna do it once for the novelty , but I don't know if in general we wanna have meetings that we record from outside this group do the digits .
Grad G: Right .
Professor E: Because it 's just an added bunch of weird stuff .
PhD C: Yeah .
Professor E: And , you know , we {disfmarker} we h we 're highly motivated . Uh in fact , the morning group is really motivated cuz they 're working on connected digits , so it 's {disfmarker}
Grad G: Actually that 's something I wanted to ask , is I have a bunch of scripts to help with the transcription of the digits .
Professor E: Yeah .
Grad G: We don't have to hand - transcribe the digits because we 're reading them and I have those .
PhD C: Right .
Professor E: Yeah .
Grad G: And so I have some scripts that let you very quickly extract the sections of each utterance . But I haven't been ru I haven't been doing that . Um , if I did that , is someone gonna be working on it ?
Professor E: Uh , yeah , I {disfmarker} I think definitely s so Absolutely .
Grad G: I mean , is it something of interest ?
Professor E: Yeah , whoever we have working on the acoustics for the Meeting Recorder are gonna start with that .
Grad G: OK . I mean , I I 'm {disfmarker} I 'm interested in it , I just don't have time to do it now .
PhD F: I was {disfmarker} these meetings {disfmarker} I 'm sure someone thought of this , but these {disfmarker} this uh reading of the numbers would be extremely helpful to do um adaptation .
Grad G: So
PhD F: Um .
Grad G: Yep . Yep .
PhD C: Actually I have o
Grad G: I {disfmarker} I would really like someone to do adaptation .
PhD F: Mm - hmm .
Grad G: So if we got someone interested in that , I think it would be great for Meeting Recorder .
Professor E: Well {disfmarker} I mean , one of the things I wanted to do , uh , that I I talked to {disfmarker} to Don about , is one of the possible things he could do or m also , we could have someone else do it , is to do block echo cancellation ,
Grad G: Since it 's the same people over and over .
PhD F: Mm - hmm .
Professor E: to try to get rid of some of the effects of the {disfmarker} the {disfmarker} the far - field effects . Um , I mean we have {disfmarker} the party line has been that echo cancellation is not the right way to handle the situation
PhD F: Mm - hmm .
Professor E: because people move around , and uh , if {disfmarker} if it 's {disfmarker} if it 's uh not a simple echo , like a cross - talk kind of echo , but it 's actually room acoustics , it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} you can't really do inversion ,
PhD F: Mm - hmm .
Professor E: and even echo cancellation is going to uh be something {disfmarker} It may {disfmarker} you {disfmarker} Someone may be moving enough that you are not able to adapt quickly and so the tack that we 've taken is more " lets come up with feature approaches and multi - stream approaches and so forth , that will be robust to it for the recognizer and not try to create a clean signal " .
PhD F: Mm - hmm .
Professor E: Uh , that 's the party line . But it occurred to me a few months ago that uh party lines are always , you know , sort of dangerous . It 's good {disfmarker} {vocalsound} good to sort of test them , actually . And so we haven't had anybody try to do a good serious job on echo cancellation and we should know how well that can do . So that 's something I 'd like somebody to do at some point , just take these digits , take the far - field mike signal , and the close uh mike signal , and apply really good echo cancellation . Um , there was a {disfmarker} have been some nice talks recently by {disfmarker} by Lucent on {disfmarker} on their b
PhD F: Hmm .
Professor E: the block echo cancellation particularly appealed to me , uh you know , trying and change it sample by sample , but you have some reasonable sized blocks . {comment} And um , you know , th
PhD A: W what is the um {disfmarker} the artifact you try to {disfmarker} you 're trying to get rid of when you do that ?
PhD F: Ciao .
Professor E: Uh so it 's {disfmarker} it {disfmarker} you have a {disfmarker} a direct uh {disfmarker} Uh , what 's the difference in {disfmarker} If you were trying to construct a linear filter , that would um {disfmarker}
PhD F: I 'm signing off .
Professor E: Yeah . that would subtract off {comment} the um uh parts of the signal that were the aspects of the signal that were different between the close - talk and the distant . You know , so {disfmarker} so uh um I guess in most echo cancellation {disfmarker} Yeah , so you {disfmarker} Given that um {disfmarker} Yeah , so you 're trying to {disfmarker} So you 'd {disfmarker} There 's a {disfmarker} a distance between the close and the distant mikes so there 's a time delay there , and after the time delay , there 's these various reflections . And if you figure out well what 's the {disfmarker} there 's a {disfmarker} a least squares algorithm that adjusts itself {disfmarker} adjusts the weight so that you try to subtract {disfmarker} essentially to subtract off uh different uh {disfmarker} different reflections . Right ? So let 's take the simple case where you just had {disfmarker} you had some uh some delay in a satellite connection or something and then there 's a {disfmarker} there 's an echo . It comes back . And you want to adjust this filter so that it will maximally reduce the effect of this echo .
PhD A: So that would mean like if you were listening to the data that was recorded on one of those . Uh , just the raw data , you would {disfmarker} you might hear kind of an echo ? And {disfmarker} and then this {disfmarker} noise cancellation would get
Professor E: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm saying {disfmarker} That 's a simplified version of what 's really happening . {comment} What 's really happening is {disfmarker} Well , when I 'm talking to you right now , you 're getting the direct sound from my speech , but you 're also getting , uh , the indirect sound that 's bounced around the room a number of times . OK ? So now , if you um try to r you {disfmarker} To completely remove the effect of that is sort of impractical for a number of technical reasons , but I {disfmarker} but {disfmarker} not to try to completely remove it , that is , invert the {disfmarker} the room response , but just to try to uh uh eliminate some of the {disfmarker} the effect of some of the echos . Um , a number of people have done this so that , say , if you 're talking to a speakerphone , uh it makes it more like it would be , if you were talking right up to it . So this is sort of the st the straight - forward approach . You say I {disfmarker} I {disfmarker} I want to use this uh {disfmarker} this item but I want to subtract off various kinds of echos . So you construct a filter , and you have this {disfmarker} this filtered version uh of the speech um gets uh uh {disfmarker} gets subtracted off from the original speech . Then you try to {disfmarker} you try to minimize the energy in some sense . And so um {disfmarker} uh with some constraints .
PhD A: Kind of a clean up thing , that {disfmarker}
Professor E: It 's a clean up thing . Right .
PhD A: OK .
Professor E: So , echo cancelling is {disfmarker} is , you know , commonly done in telephony , and {disfmarker} and {disfmarker} and it 's sort of the obvious thing to do in this situation if you {disfmarker} if , you know , you 're gonna be talking some distance from a mike .
PhD A: When uh , I would have meetings with the folks in Cambridge when I was at BBN over the phone , they had a um {disfmarker} some kind of a special speaker phone and when they would first connect me , it would come on and we 'd hear all this noise . And then it was uh {disfmarker} And then it would come on and it was very clear ,
Professor E: Yeah .
PhD A: you know .
Professor E: Right . So it 's taking samples , it 's doing adaptation , it 's adjusting weights , and then it 's getting the sum . So um , uh anyway that 's {disfmarker} that 's kind of a reasonable thing that I 'd like to have somebody try {disfmarker} somebody look {disfmarker} And {disfmarker} and the digits would be a reasonable thing to do that with . I think that 'd be enough data {disfmarker} plenty of data to do that with , and i for that sort of task you wouldn't care whether it was uh large vocabulary speech or anything . Uh . {vocalsound} Um
Postdoc B: Is Brian Kingsbury 's work related to that , or is it a different type of reverberation ?
Professor E: Brian 's {comment} Kingsbury 's work is an example of what we did f f from the opposite dogma . Right ? Which is what I was calling the " party line " , which is that uh doing that sort of thing is not really what we want . We want something more flexible , uh i i where people might change their position , and there might be , you know {disfmarker} There 's also um oh yeah , noise . So the echo cancellation does not really allow for noise . It 's if you have a clean situation but you just have some delays , Then we 'll figure out the right {disfmarker} the right set of weights for your taps for your filter in order to produce the effect of those {disfmarker} those echos . But um if there 's noise , then the very signal that it 's looking at is corrupted so that it 's decision about what the right {disfmarker} you know , right {disfmarker} right uh {disfmarker} delays are {disfmarker} is , uh {disfmarker} is {disfmarker} right delayed signal is {disfmarker} is {disfmarker} is {disfmarker} uh is incorrect . And so , in a noisy situation , um , also in a {disfmarker} in a situation that 's very reverberant {disfmarker} {comment} with long reverberation times {comment} and really long delays , it 's {disfmarker} it 's sort of typically impractical . So for those kind of reasons , and also a {disfmarker} a c a complete inversion , if you actually {disfmarker} I mentioned that it 's kind of hard to really do the inversion of the room acoustics . Um , that 's difficult because um often times the {disfmarker} the um {disfmarker} {vocalsound} the system transfer function is such that when it 's inverted you get something that 's unstable , and so , if you {disfmarker} you do your estimate of what the system is , and then you try to invert it , you get a filter that actually uh , you know , rings , and {disfmarker} and uh goes to infinity . So it 's {disfmarker} so there 's {disfmarker} there 's {disfmarker} there 's that sort of technical reason , and the fact that things move , and there 's air currents {disfmarker} I mean there 's all sorts of {disfmarker} all sorts of reasons why it 's not really practical . So for all those kinds of reasons , uh we {disfmarker} we {disfmarker} we sort of um , concluded we didn't want to in do inversion , and we 're even pretty skeptical of echo cancellation , which isn't really inversion , and um we decided to do this approach of taking {disfmarker} uh , just picking uh features , which were {disfmarker} uh will give you more {disfmarker} something that was more stable , in the presence of , or absence of , room reverberation , and that 's what Brian was trying to do . So , um , let me just say a couple things that I was {disfmarker} I was gonna bring up . Uh . Let 's see . I guess you {disfmarker} you actually already said this thing about the uh {disfmarker} about the consent forms , which was that we now don't have to {disfmarker} So this was the human subjects folks who said this , {comment} or that {disfmarker} that {disfmarker} ?
Postdoc B: The a apparently {disfmarker} I mean , we 're gonna do a revised form , of course . Um but once a person has signed it once , then that 's valid for a certain number of meetings . She wanted me to actually estimate how many meetings and put that on the consent form . I told her that would be a little bit difficult to say . So I think from a s practical standpoint , maybe we could have them do it once every ten meetings , or something . It won't be that many people who do it {pause} that often , but um just , you know , so long as they don't forget that they 've done it , I guess .
Professor E: OK . Um , back on the data thing , so there 's this sort of one hour , ten hour , a hundred hour sort of thing that {disfmarker} that we have . We have {disfmarker} we have an hour uh that {disfmarker} that is transcribed , we have {disfmarker} we have twelve hours that 's recorded but not transcribed , and at the rate we 're going , uh by the end of the semester we 'll have , I don't know , forty or fifty or something , if we {disfmarker} if this really uh {disfmarker} Well , do we have that much ?
PhD C: Not really . It 's three to four per week .
Professor E: Let 's see , we have {disfmarker}
PhD C: So that 's what {disfmarker} You know , that {disfmarker}
Professor E: uh eight weeks , uh is {disfmarker}
PhD C: So that 's not a lot of hours .
Professor E: Eight weeks times three hours is twenty - four , so that 's {disfmarker} Yeah , so like thirty {disfmarker} thirty hours ?
PhD A: Three {disfmarker} Three hours .
PhD C: Yeah . I mean , is there {disfmarker} I know this sounds {pause} tough but we 've got the room set up . Um I was starting to think of some projects where you would use well , similar to what we talked about with uh energy detection on the close - talking mikes . There are a number of interesting questions that you can ask about how interactions happen in a meeting , that don't require any transcription . So what are the patterns , the energy patterns over the meeting ? And I 'm really interested in this {vocalsound} but we don't have a whole lot of data . So I was thinking , you know , we 've got the room set up and you can always think of , also for political reasons , if ICSI collected you know , two hundred hours , that looks different than forty hours , even if we don't transcribe it ourselves ,
Professor E: But I don't think we 're gonna stop at the end of this semester .
PhD C: so {disfmarker}
Professor E: Right ? So , I th I think that if we are able to keep that up for a few months , we are gonna have more like a hundred hours .
PhD C: I mean , is there {disfmarker} Are there any other meetings here that we can record , especially meetings that have some kind of conflict in them {comment} or some kind of deci I mean , that are less well {disfmarker} I don't {disfmarker} uh , that have some more emotional aspects to them , or strong {disfmarker}
Grad G: We had some good ones earlier .
PhD C: There 's laughter , um I 'm talking more about strong differences of opinion meetings , maybe with manager types , or {disfmarker}
Grad G: I think it 's hard to record those .
PhD C: To be allowed to record them ?
Postdoc B: It 's also likely that people will cancel out afterwards .
PhD C: OK .
Professor E: Yeah , people will get {disfmarker}
Postdoc B: But I {disfmarker} but I wanted to raise the KPFA idea .
PhD C: OK . Well , if there is , anyway .
Professor E: Yeah , I was gonna mention that .
Grad G: Oh , that 's a good idea . That 's {disfmarker} That would be a good match .
Professor E: Yeah . So {disfmarker} Yeah . So I {disfmarker} I {disfmarker} uh , I {disfmarker} I 'd mentioned to Adam , and {disfmarker} that was another thing I was gonna talk {disfmarker} uh , mention to them before {disfmarker} {comment} that uh there 's uh {disfmarker} It {disfmarker} it oc it occurred to me that we might be able to get some additional data by talking to uh acquaintances in local broadcast media . Because , you know , we had talked before about the problem about using found data , {comment} that {disfmarker} that uh it 's just set up however they have it set up and we don't have any say about it and it 's typically one microphone , in a , uh , uh {disfmarker} or {disfmarker} and {disfmarker} and so it doesn't really give us the {disfmarker} the {disfmarker} the uh characteristics we want . Um and so I do think we 're gonna continue recording here and record what we can . But um , it did occur to me that we could go to friends in broadcast media and say " hey you have this panel show , {pause} or this {disfmarker} you know , this discussion show , and um can you record multi - channel ? " And uh they may be willing to record it uh with {disfmarker}
PhD C: With lapel mikes or something ?
Professor E: Well , they probably already use lapel , but they might be able to have it {disfmarker} it wouldn't be that weird for them to have another mike that was somewhat distant .
PhD C: Right .
Professor E: It wouldn't be exactly this setup , but it would be that sort of thing , and what we were gonna get from UW , you know , assuming they {disfmarker} they {disfmarker} they start recording , isn't {disfmarker} als also is not going to be this exact setup .
PhD C: Right . No , I think that 'd be great , if we can get more data .
Professor E: So , {comment} I {disfmarker} I {disfmarker} I {disfmarker} I was thinking of looking into that . the other thing that occurred to me after we had that discussion , in fact , is that it 's even possible , since of course , many radio shows are not live , {comment} uh that we could invite them to have like some of their {disfmarker} {comment} record some of their shows here .
Postdoc B: Wow !
PhD C: Well {disfmarker} Or {disfmarker} The thing is , they 're not as averse to wearing one of these head - mount I mean , they 're on the radio ,
Grad G: Right , as we are .
PhD C: right ? So . {comment} Um , I think that 'd be fantastic
Professor E: Right .
PhD C: cuz those kinds of panels and {disfmarker} Those have interesting
Professor E: Yeah .
PhD C: Th - that 's an {disfmarker} a side of style {disfmarker} a style that we 're not collecting here , so it 'd be great .
Professor E: And {disfmarker} and the {disfmarker} I mean , the other side to it was the {disfmarker} what {disfmarker} which is where we were coming from {disfmarker} I 'll {disfmarker} I 'll talk to you more about it later {comment} is that {disfmarker} is that there 's {disfmarker} there 's uh the radio stations and television stations already have stuff worked out presumably , uh related to , you know , legal issues and {disfmarker} and permissions and all that . I mean , they already do what they do {disfmarker} do whatever they do . So it 's {disfmarker} uh , it 's {disfmarker} So it 's {disfmarker} so it 's another source . So I think it 's something we should look into , you know , we 'll collect what we collect here hopefully they will collect more at UW also and um {disfmarker} and maybe we have this other source . But yeah I think that it 's not unreasonable to aim at getting , you know , significantly in excess of a hundred hours . I mean , that was sort of our goal . The thing was , I was hoping that we could {disfmarker} @ @ in the {disfmarker} under this controlled situation we could at least collect , you know , thirty to fifty hours . And at the rate we 're going we 'll get pretty close to that I think this semester . And if we continue to collect some next semester , I think we should , uh {disfmarker}
PhD C: Right . Yeah I was mostly trying to think , " OK , if you start a project , within say a month , you know , how much data do you have to work with . And you {disfmarker} you wanna s you wanna sort of fr freeze your {disfmarker} your data for awhile so um right now {disfmarker} and we don't have the transcripts back yet from IBM right ? Do {disfmarker} Oh , do we now ?
Professor E: Well , we don't even have it for this f you know , forty - five minutes , that was {disfmarker}
PhD C: So um , not complaining , I was just trying to think , you know , what kinds of projects can you do now versus uh six months from now
Professor E: Yeah .
PhD C: and they 're pretty different , because
Professor E: Yeah . So I was thinking right now it 's sort of this exploratory stuff where you {disfmarker} you look at the data , you use some primitive measures and get a feeling for what the scatter plots look like ,
Grad G: Right .
PhD C: um {disfmarker} Right . Right , right .
Professor E: and {disfmarker} and {disfmarker} and uh {disfmarker} and meanwhile we collect , and it 's more like yeah , three months from now , or six months from now you can {disfmarker} you can do a lot of other things .
PhD C: Cuz I 'm not actually sure , just logistically that I can spend {disfmarker} you know , I don't wanna charge the time that I have on the project too early , before there 's enough data to make good use of the time . And that 's {disfmarker} and especially with the student
Grad G: Right .
PhD C: uh for instance this guy who seems {disfmarker}
Professor E: Yeah .
PhD C: Uh anyway , I shouldn't say too much , but um if someone came that was great and wanted to do some real work and they have to end by the end of this school year in the spring , how much data will I have to work with , with that person . And so it 's {disfmarker}
Professor E: i Yeah , so I would think , exploratory things now . Uh , three months from now {disfmarker} Um , I mean the transcriptions I think are a bit of an unknown cuz we haven't gotten those back yet as far as the timing , but I think as far as the collection , it doesn't seem to me l like , uh , unreasonable to say that uh in January , you know , ro roughly uh {disfmarker} which is roughly three months from now , we should have at least something like , you know , twenty - five , thirty hours .
PhD C: And we just don't know about the transcription part of that ,
Professor E: So that 's {disfmarker}
Postdoc B: Yeah , we need to {disfmarker} I think that there 's a possibility that the transcript will need to be adjusted afterwards ,
PhD C: so . I mean , it {disfmarker}
Postdoc B: and uh es especially since these people won't be uh used to dealing with multi - channel uh transcriptions .
PhD C: Right .
Professor E: Yeah .
Postdoc B: So I think that we 'll need to adjust some {disfmarker} And also if we wanna add things like um , well , more refined coding of overlaps , then definitely I think we should count on having an extra pass through . I wanted to ask another a a aspect of the data collection . There 'd be no reason why a person couldn't get together several uh , you know , friends , and come and argue about a topic if they wanted to , right ?
Professor E: If they really have something they wanna talk about as opposed to something @ @ {disfmarker} I mean , what we 're trying to stay away from was artificial constructions , but I think if it 's a real {disfmarker} Why not ? Yeah .
PhD C: I mean , I 'm thinking , politically {disfmarker}
Grad G: Stage some political debates .
Postdoc B: You could do this ,
PhD C: Well yeah ,
Postdoc B: you know . You could .
PhD C: or just if you 're {disfmarker} if you ha If there are meetings here that happen that we can record even if we don't {pause} um have them do the digits , {comment} or maybe have them do a shorter {pause} digit thing {comment} like if it was , you know , uh , one string of digits , or something , they 'd probably be willing to do .
Grad G: We don't have to do the digits at all if we don't want to .
PhD C: Then , having the data is very valuable , cuz I think it 's um politically better for us to say we have this many hours of audio data , especially with the ITR , if we put in a proposal on it . It 'll just look like ICSI 's collected a lot more audio data . Um , whether it 's transcribed or not um , is another issue , but there 's {disfmarker} there are research questions you can answer without the transcriptions , or at least that you can start to answer .
Postdoc B: It seems like you could hold some meetings .
Grad G: Yep .
Postdoc B: You know , you and maybe Adam ?
PhD C: So .
Postdoc B: You {disfmarker} you could {disfmarker} you could maybe hold some additional meetings , if you wanted .
PhD A: Would it help at all {disfmarker} I mean , we 're already talking about sort of two levels of detail in meetings . One is uh um without doing the digits {disfmarker} Or , I guess the full - blown one is where you do the digits , and everything , and then talk about doing it without digits , what if we had another level , just to collect data , which is without the headsets and we just did the table - mounted stuff .
PhD C: Need the close - talking mikes .
PhD A: You do , OK .
PhD C: I mean , absolutely ,
Professor E: Yeah . Yeah .
PhD C: yeah . I 'm really scared {disfmarker}
Grad G: It seems like it 's a big part of this corpus is to have the close - talking mikes .
PhD A: I see , OK .
PhD C: Um or at least , like , me personally ? I would {disfmarker} {comment} I {disfmarker} couldn't use that data .
Professor E: Yeah .
Postdoc B: I agree . And Mari also ,
PhD C: Um .
Postdoc B: we had {disfmarker} This came up when she she was here . That 's important .
PhD C: So it 's a great idea ,
Professor E: Yeah , I {disfmarker} I {disfmarker} b By the {disfmarker} by the way , I don't think the transcriptions are actually , in the long run , such a big bottleneck .
PhD C: and if it were true than I would just do that , but it 's not that bad {disfmarker} like the room is not the bottleneck , and we have enough time in the room , it 's getting the people to come in and put on the {disfmarker} and get the setup going .
Professor E: I think the issue is just that we 're {disfmarker} we 're blazing that path . Right ? And {disfmarker} and um {disfmarker} d Do you have any idea when {disfmarker} when uh the {disfmarker} you 'll be able to send uh the ten hours to them ?
Grad G: Well , I 've been burning two C Ds a day , which is about all I can do with the time I have .
Professor E: Yeah . Yeah .
Grad G: So it 'll be early next week .
Professor E: Yeah , OK . So early next week we send it to them , and then {disfmarker} then we check with them to see if they 've got it and we {disfmarker} we start , you know asking about the timing for it .
Grad G: Yep .
Professor E: So I think once they get it sorted out about how they 're gonna do it , which I think they 're pretty well along on , cuz they were able to read the files and so on .
Grad G: Yep .
Professor E: Right ?
Grad G: Yeah , but {disfmarker}
Professor E: Well {disfmarker}
Grad G: Yeah , who knows where they are .
PhD A: Have they ever responded to you ?
Grad G: Nope .
Professor E: Yeah , but {disfmarker} You know , so they {disfmarker} they {disfmarker} they have {disfmarker} you know , they 're volunteering their time and they have a lot of other things to do ,
PhD C: What if {disfmarker}
Grad G: Yeah , you {disfmarker} we can't complain .
Professor E: right ? But they {disfmarker} But at any rate , they 'll {disfmarker} I {disfmarker} I think once they get that sorted out , they 're {disfmarker} they 're making cassettes there , then they 're handing it to someone who they {disfmarker} who 's {disfmarker} who is doing it , and uh I think it 's not going to be {disfmarker} I don't think it 's going to be that much more of a deal for them to do thirty hours then to do one hour , I think . It 's not going to be thirty
Grad G: Yep . I think that 's probably true .
PhD C: Really ? So it 's the amount of {disfmarker}
Professor E: It 's {disfmarker} it 's just getting it going .
Grad G: It 's pipeline , pipeline issues .
PhD C: Right . What about these lunch meetings {disfmarker}
Grad G: Once the pipeline fills .
PhD C: I mean , I don't know , if there 's any way without too much more overhead , even if we don't ship it right away to IBM even if we just collect it here for awhile , {comment} to record you know , two or three more meeting a week , just to have the data , even if they 're um not doing the digits , but they do wear the headphones ?
Professor E: But the lunch meetings are pretty much one person getting up and {disfmarker}
PhD C: No , I meant , um , sorry , the meetings where people eat their lunch downstairs , maybe they don't wanna be recorded , but {disfmarker}
Grad G: Oh , and we 're just chatting ?
PhD C: Just the ch the chatting .
Grad G: Yeah , we have a lot of those .
PhD C: I actually {disfmarker} I actually think that 's {pause} useful {pause} data , um {pause} the chatting ,
Grad G: Yeah , the problem with that is I would {disfmarker} I think I would feel a little constrained to {disfmarker} You know ? Uh , some of the meetings {disfmarker}
PhD C: but {disfmarker} OK . You don't wanna do it , cuz {disfmarker} OK .
Grad G: You know , our " soccer ball " meeting ?
PhD C: Alright .
Grad G: I guess none of you were there for our soccer ball meeting .
PhD C: Alright , {comment} so I 'll just throw it out there , if anyone knows of one more m or two more wee meetings per week that happen at ICSI , um that we could record , I think it would be worth it .
Grad G: That was hilarious .
Professor E: Yeah . Well , we should also check with Mari again , because they {disfmarker} because they were really intending , you know , maybe just didn't happen , but they were really intending to be duplicating this in some level . So then that would double {pause} what we had . Uh . And there 's a lot of different meetings at UW uh {disfmarker} I mean really m a lot more {comment} than we have here right cuz we 're not right on campus ,
Grad G: Right .
Professor E: so .
PhD A: Is the uh , notion of recording any of Chuck 's meetings dead in the water , or is that still a possibility ?
Professor E: Uh , {vocalsound} they seem to have some problems with it . We can {disfmarker} we can talk about that later . Um , but , again , Jerry is {disfmarker} Jerry 's open {disfmarker} So I mean , we have two speech meetings , one uh network meeting , uh Jerry was open to it but I {disfmarker} I s One of the things that I think is a little {disfmarker} a little bit of a limitation , there is a think when the people are not involved uh in our work , we probably can't do it every week . You know ? I {disfmarker} I {disfmarker} I {disfmarker} I think that {disfmarker} that people are gonna feel uh {disfmarker} are gonna feel a little bit constrained . Now , it might get a little better if we don't have them do the digits all the time . And the {disfmarker} then {disfmarker} so then they can just really sort of try to {disfmarker} put the mikes on and then just charge in and {disfmarker}
Grad G: Yep .
PhD C: What if we give people {disfmarker} you know , we cater a lunch in exchange for them having their meeting here or something ?
Postdoc B: Well , you know , I {disfmarker} I do think eating while you 're doing a meeting is going to be increasing the noise .
PhD C: OK .
Postdoc B: But I had another question , which is um , you know , in principle , w um , I know that you don't want artificial topics ,
PhD C: Alright , alright , alright .
Postdoc B: but um it does seem to me that we might be able to get subjects from campus to come down and do something that wouldn't be too artificial . I mean , we could {disfmarker} political discussions , or {disfmarker} or something or other ,
PhD C: No , definitely .
Postdoc B: and i you know , people who are {disfmarker} Because , you know , there 's also this constraint . We d it 's like , you know , the {disfmarker} the {disfmarker} uh goldibears {disfmarker} goldi goldilocks , it 's like you don't want meetings that are too large , but you don't want meetings that are too small . And um {disfmarker} a and it just seems like maybe we could exploit the subj human subject p p pool , in the positive sense of the word .
PhD A: Well , even {disfmarker} I mean , coming down from campus is sort of a big thing , but what about
Postdoc B: We could pay subjects .
PhD A: or what about people in the {disfmarker} in the building ?
PhD C: Yeah , I was thinking , there 's all these other peo
PhD A: I mean , there 's the State of California downstairs , and {disfmarker}
PhD C: Yeah . I mean {disfmarker}
Grad G: I just really doubt that uh any of the State of California meetings would be recordable and then releasable to the general public .
Postdoc B: Yeah .
PhD A: Oh .
PhD C: Mm - hmm .
Grad G: So I {disfmarker} I mean I talked with some people at the Haas Business School who are i who are interested in speech recognition
PhD C: Alright , well .
Grad G: and , they sort of hummed and hawed and said " well maybe we could have meetings down here " , but then I got email from them that said " no , we decided we 're not really interested and we don't wanna come down and hold meetings . " So , I think it 's gonna be a problem to get people regularly .
PhD A: What about Joachim , maybe he can {disfmarker}
Professor E: But {disfmarker} but we c But I think , you know , we get some scattered things from this and that . And I {disfmarker} I d I do think that maybe we can get somewhere with the {disfmarker} with the radio .
PhD C: Mm - hmm .
Professor E: Uh i I have better contacts in radio than in television , but {disfmarker}
PhD A: You could get a lot of lively discussions from those radio ones .
PhD C: Well , and they 're already {disfmarker} they 're {disfmarker} these things are already recorded ,
Grad G: Yep .
Professor E: Yeah .
PhD C: we don't have to ask them to {disfmarker} even {disfmarker} and I 'm not sure wh how they record it , but they must record from individual {disfmarker}
Professor E: n Well {disfmarker} No , I 'm not talking about ones that are already recorded . I 'm talking about new ones
PhD C: Why {disfmarker} why not ?
Professor E: because {disfmarker} because {disfmarker} because we would be asking them to do something different .
PhD C: Well , we can find out . I know for instance Mark Liberman was interested uh in {disfmarker} in LDC getting {pause} data , uh , and {disfmarker}
Professor E: Right , that 's the found data idea .
PhD C: Yeah .
Professor E: But what I 'm saying is uh if I talk to people that I know who do these th who produce these things we could ask them if they could record an extra channel , let 's say , of a distant mike .
PhD C: Mm - hmm .
Professor E: And u I think routinely they would not do this . So , since I 'm interested in the distant mike stuff , I wanna make sure that there is at least that somewhere
PhD C: Right . Great . OK .  
Professor E: and uh {disfmarker} But if we ask them to do that they might be intrigued enough by the idea that they uh might be e e willing to {disfmarker} the {disfmarker} I might be able to talk them into it .
PhD C: Mm - hmm .
Grad G: Um . We 're getting towards the end of our disk space , so we should think about trying to wrap up here .
PhD C: That 's a good way to end a meeting .
Professor E: OK . Well I don't {disfmarker} why don't we {disfmarker} why d u why don't we uh uh turn them {disfmarker} turn
Grad G: OK , leave {disfmarker} leave them on for a moment until I turn this off , cuz that 's when it crashed last time .
Postdoc B: Oh . That 's good to know .
Professor E: Turning off the microphone made it crash . Well {disfmarker}
Postdoc B: That 's good to know .
Professor E: OK .
2022-06-15 06:45:33 | INFO | __main__ | output #1: Efforts by speaker mn005 are in progress to detect overlapping speech. For a single transcribed meeting, speaker mn005 reported approximately 300 cases of overlap. Future work will involve manually deriving time marks from sections of overlapping speech for the same meeting, and then experimenting with different measures, e.g. energy increase, to determine a set of acoustically salient features for identifying speaker overlap. 
2022-06-15 06:45:33 | INFO | __main__ | 
Running tokenizer on train dataset:   0%|          | 0/2 [00:00<?, ?ba/s]Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:09<00:00,  9.42s/ba]Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:09<00:00,  9.42s/ba]
2022-06-15 06:45:42 | INFO | datasets.arrow_writer | Done writing 272 examples in 3878618 bytes .
Running tokenizer on train dataset:  50%|█████     | 1/2 [00:27<00:27, 27.56s/ba]Running tokenizer on train dataset: 100%|██████████| 2/2 [00:35<00:00, 21.54s/ba]Running tokenizer on train dataset: 100%|██████████| 2/2 [00:35<00:00, 17.53s/ba]
Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s][INFO|trainer.py:422] 2022-06-15 06:46:08,993 >> Using amp fp16 backend
[INFO|trainer.py:528] 2022-06-15 06:46:08,993 >> The following columns in the training set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:1248] 2022-06-15 06:46:09,144 >> ***** Running training *****
[INFO|trainer.py:1250] 2022-06-15 06:46:09,144 >>   Num examples = 1257
[INFO|trainer.py:1251] 2022-06-15 06:46:09,144 >>   Num Epochs = 20
[INFO|trainer.py:1257] 2022-06-15 06:46:09,144 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1258] 2022-06-15 06:46:09,144 >>   Total optimization steps = 180
Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:07<00:00,  7.02s/ba]Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:07<00:00,  7.02s/ba]
  0%|          | 0/180 [00:00<?, ?it/s][INFO|trainer.py:1411] 2022-06-15 06:46:16,742 >> Unused parameters:
  1%|          | 1/180 [00:08<25:31,  8.55s/it]2022-06-15 06:46:24 | INFO | root | Reducer buckets have been rebuilt in this iteration.
2022-06-15 06:46:24 | INFO | root | Reducer buckets have been rebuilt in this iteration.
  1%|          | 2/180 [00:15<24:15,  8.18s/it]  2%|▏         | 3/180 [00:23<23:16,  7.89s/it]  2%|▏         | 4/180 [00:30<22:34,  7.70s/it]  3%|▎         | 5/180 [00:37<22:02,  7.56s/it]  3%|▎         | 6/180 [00:44<21:40,  7.47s/it]  4%|▍         | 7/180 [00:52<21:20,  7.40s/it]  4%|▍         | 8/180 [00:59<21:06,  7.37s/it]  5%|▌         | 9/180 [01:06<20:57,  7.35s/it][INFO|trainer.py:528] 2022-06-15 06:47:29,125 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 06:47:29,128 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 06:47:29,128 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 06:47:29,128 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.11it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.86it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.63it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.88it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.40it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.08it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.88it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.73it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.62it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.55it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.51it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.48it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.45it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.44it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.41it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.38it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.39it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.36it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.28it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.24it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.22it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.19it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.24it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.28it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.26it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.29it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.31it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.32it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.35it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.35it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.37it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.39it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.40it/s][A                                               
                                               [A{'eval_loss': 3.593782663345337, 'eval_runtime': 5.9934, 'eval_samples_per_second': 45.383, 'eval_steps_per_second': 11.346, 'epoch': 0.91}
  5%|▌         | 9/180 [01:18<20:57,  7.35s/it]
100%|██████████| 68/68 [00:05<00:00, 11.40it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 06:47:35,322 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-9
[INFO|configuration_utils.py:391] 2022-06-15 06:47:35,380 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-9/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 06:56:15,732 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-9/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 06:56:15,752 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-9/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 06:56:15,755 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-9/special_tokens_map.json
  6%|▌         | 10/180 [26:44<22:01:31, 466.42s/it]                                                    {'padding_count': 0, 'loss': 4.4091, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.1}
  6%|▌         | 10/180 [26:44<22:01:31, 466.42s/it]  6%|▌         | 11/180 [26:51<15:25:41, 328.65s/it]  7%|▋         | 12/180 [26:58<10:50:09, 232.20s/it]  7%|▋         | 13/180 [27:06<7:39:20, 165.03s/it]   8%|▊         | 14/180 [27:14<5:25:33, 117.67s/it]  8%|▊         | 15/180 [27:21<3:52:31, 84.56s/it]   9%|▉         | 16/180 [27:28<2:47:40, 61.35s/it]  9%|▉         | 17/180 [27:35<2:02:31, 45.10s/it] 10%|█         | 18/180 [27:42<1:31:08, 33.75s/it][INFO|trainer.py:528] 2022-06-15 07:14:05,423 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 07:14:05,426 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 07:14:05,426 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 07:14:05,426 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.13it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.90it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.67it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.92it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.43it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.11it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.86it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.73it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.64it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.58it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.54it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.49it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.48it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.46it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.44it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.43it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.45it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.46it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.45it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.45it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.45it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.41it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.40it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.40it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.36it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.39it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.38it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.41it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.43it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.42it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.43it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.44it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.45it/s][A                                                  
                                               [A{'eval_loss': 3.3802216053009033, 'eval_runtime': 5.9567, 'eval_samples_per_second': 45.663, 'eval_steps_per_second': 11.416, 'epoch': 1.91}
 10%|█         | 18/180 [27:55<1:31:08, 33.75s/it]
100%|██████████| 68/68 [00:05<00:00, 11.45it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 07:14:11,403 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-18
[INFO|configuration_utils.py:391] 2022-06-15 07:14:11,502 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-18/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 07:23:03,495 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-18/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 07:23:03,499 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-18/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 07:23:03,500 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-18/special_tokens_map.json
 11%|█         | 19/180 [52:35<21:04:53, 471.39s/it] 11%|█         | 20/180 [52:42<14:45:40, 332.13s/it]                                                    {'padding_count': 0, 'loss': 3.7633, 'learning_rate': 4.938271604938271e-05, 'epoch': 2.2}
 11%|█         | 20/180 [52:42<14:45:40, 332.13s/it] 12%|█▏        | 21/180 [52:49<10:21:48, 234.64s/it] 12%|█▏        | 22/180 [52:57<7:18:12, 166.41s/it]  13%|█▎        | 23/180 [53:04<5:10:26, 118.64s/it] 13%|█▎        | 24/180 [53:11<3:41:33, 85.22s/it]  14%|█▍        | 25/180 [53:18<2:39:41, 61.82s/it] 14%|█▍        | 26/180 [53:25<1:56:38, 45.45s/it] 15%|█▌        | 27/180 [53:33<1:26:40, 33.99s/it][INFO|trainer.py:528] 2022-06-15 07:39:55,663 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 07:39:55,665 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 07:39:55,665 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 07:39:55,665 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.19it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.93it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.68it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.89it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.41it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.06it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.83it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.69it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.59it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.54it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.51it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.48it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.48it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.46it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.41it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.41it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.42it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.43it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.45it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.46it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.45it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.39it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.41it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.42it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.38it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.41it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.43it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.44it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.44it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.45it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.46it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.47it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.28it/s][A                                                  
                                               [A{'eval_loss': 3.2729268074035645, 'eval_runtime': 5.9723, 'eval_samples_per_second': 45.544, 'eval_steps_per_second': 11.386, 'epoch': 2.91}
 15%|█▌        | 27/180 [53:45<1:26:40, 33.99s/it]
100%|██████████| 68/68 [00:05<00:00, 11.28it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 07:40:01,653 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-27
[INFO|configuration_utils.py:391] 2022-06-15 07:40:01,898 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-27/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 07:48:16,917 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-27/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 07:48:16,920 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-27/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 07:48:16,922 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-27/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 08:06:56,122 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-9] due to args.save_total_limit
 16%|█▌        | 28/180 [1:20:47<21:42:24, 514.11s/it] 16%|█▌        | 29/180 [1:20:54<15:11:04, 362.02s/it] 17%|█▋        | 30/180 [1:21:01<10:38:53, 255.55s/it]                                                      {'padding_count': 0, 'loss': 3.4785, 'learning_rate': 4.62962962962963e-05, 'epoch': 3.3}
 17%|█▋        | 30/180 [1:21:01<10:38:53, 255.55s/it] 17%|█▋        | 31/180 [1:21:09<7:29:34, 181.04s/it]  18%|█▊        | 32/180 [1:21:16<5:17:54, 128.88s/it] 18%|█▊        | 33/180 [1:21:23<3:46:19, 92.38s/it]  19%|█▉        | 34/180 [1:21:30<2:42:36, 66.83s/it] 19%|█▉        | 35/180 [1:21:37<1:58:15, 48.94s/it] 20%|██        | 36/180 [1:21:45<1:27:23, 36.42s/it][INFO|trainer.py:528] 2022-06-15 08:08:07,473 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 08:08:07,476 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 08:08:07,476 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 08:08:07,476 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 16.99it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.67it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.54it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.84it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.41it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.08it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.85it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.74it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.64it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.59it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.55it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.50it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.49it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.48it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.45it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.44it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.45it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.46it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.46it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.46it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.44it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.39it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.40it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.41it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.37it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.39it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.39it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.37it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.40it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.42it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.42it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.45it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.45it/s][A                                                    
                                               [A{'eval_loss': 3.22873592376709, 'eval_runtime': 5.9587, 'eval_samples_per_second': 45.648, 'eval_steps_per_second': 11.412, 'epoch': 3.91}
 20%|██        | 36/180 [1:21:57<1:27:23, 36.42s/it]
100%|██████████| 68/68 [00:05<00:00, 11.45it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 08:08:13,436 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-36
[INFO|configuration_utils.py:391] 2022-06-15 08:08:13,452 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-36/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 08:15:04,581 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-36/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 08:15:04,585 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-36/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 08:15:04,586 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-36/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 08:30:56,506 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-18] due to args.save_total_limit
 21%|██        | 37/180 [1:44:48<17:29:34, 440.38s/it] 21%|██        | 38/180 [1:44:55<12:14:37, 310.40s/it] 22%|██▏       | 39/180 [1:45:02<8:35:40, 219.44s/it]  22%|██▏       | 40/180 [1:45:09<6:03:26, 155.76s/it]                                                     {'padding_count': 0, 'loss': 3.2886, 'learning_rate': 4.3209876543209875e-05, 'epoch': 4.41}
 22%|██▏       | 40/180 [1:45:09<6:03:26, 155.76s/it] 23%|██▎       | 41/180 [1:45:16<4:17:33, 111.18s/it] 23%|██▎       | 42/180 [1:45:23<3:03:59, 80.00s/it]  24%|██▍       | 43/180 [1:45:31<2:12:46, 58.15s/it] 24%|██▍       | 44/180 [1:45:38<1:37:09, 42.87s/it] 25%|██▌       | 45/180 [1:45:45<1:12:22, 32.17s/it][INFO|trainer.py:528] 2022-06-15 08:32:07,899 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 08:32:07,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 08:32:07,901 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 08:32:07,901 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.14it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.91it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.68it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.93it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.44it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.10it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.89it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.74it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.60it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.52it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.50it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.49it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.47it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.47it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.45it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.45it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.44it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.46it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.44it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.45it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.45it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.40it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.39it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.40it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.32it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.28it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.26it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.24it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.22it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.21it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.21it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.22it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.28it/s][A                                                    
                                               [A{'eval_loss': 3.1947765350341797, 'eval_runtime': 5.9849, 'eval_samples_per_second': 45.448, 'eval_steps_per_second': 11.362, 'epoch': 4.91}
 25%|██▌       | 45/180 [1:45:57<1:12:22, 32.17s/it]
100%|██████████| 68/68 [00:05<00:00, 11.28it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 08:32:13,888 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-45
[INFO|configuration_utils.py:391] 2022-06-15 08:32:14,215 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-45/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 08:38:35,926 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-45/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 08:38:35,946 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-45/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 08:38:35,958 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-45/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 08:51:53,958 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-27] due to args.save_total_limit
 26%|██▌       | 46/180 [2:05:45<14:14:18, 382.52s/it] 26%|██▌       | 47/180 [2:05:52<9:58:18, 269.91s/it]  27%|██▋       | 48/180 [2:05:59<7:00:23, 191.08s/it] 27%|██▋       | 49/180 [2:06:06<4:56:44, 135.91s/it] 28%|██▊       | 50/180 [2:06:14<3:30:47, 97.29s/it]                                                     {'padding_count': 0, 'loss': 3.1479, 'learning_rate': 4.012345679012346e-05, 'epoch': 5.51}
 28%|██▊       | 50/180 [2:06:14<3:30:47, 97.29s/it] 28%|██▊       | 51/180 [2:06:21<2:31:04, 70.26s/it] 29%|██▉       | 52/180 [2:06:28<1:49:31, 51.34s/it] 29%|██▉       | 53/180 [2:06:35<1:20:38, 38.10s/it] 30%|███       | 54/180 [2:06:42<1:00:33, 28.83s/it][INFO|trainer.py:528] 2022-06-15 08:53:05,376 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 08:53:05,378 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 08:53:05,378 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 08:53:05,378 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.15it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.88it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.67it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.93it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.45it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.13it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.92it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.77it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.64it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.54it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.44it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.44it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.44it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.44it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.44it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.43it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.45it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.47it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.43it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.45it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.45it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.39it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.38it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.39it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.36it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.38it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.41it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.42it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.41it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.43it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.43it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.44it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.45it/s][A                                                    
                                               [A{'eval_loss': 3.176757574081421, 'eval_runtime': 5.9573, 'eval_samples_per_second': 45.658, 'eval_steps_per_second': 11.414, 'epoch': 5.91}
 30%|███       | 54/180 [2:06:55<1:00:33, 28.83s/it]
100%|██████████| 68/68 [00:05<00:00, 11.45it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 08:53:11,372 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-54
[INFO|configuration_utils.py:391] 2022-06-15 08:53:11,449 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-54/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 08:59:21,463 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-54/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 08:59:21,482 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-54/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 08:59:21,496 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-54/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 09:17:15,813 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-36] due to args.save_total_limit
 31%|███       | 55/180 [2:31:07<15:57:19, 459.52s/it] 31%|███       | 56/180 [2:31:14<11:09:11, 323.81s/it] 32%|███▏      | 57/180 [2:31:22<7:49:15, 228.91s/it]  32%|███▏      | 58/180 [2:31:29<5:30:17, 162.44s/it] 33%|███▎      | 59/180 [2:31:36<3:53:39, 115.86s/it] 33%|███▎      | 60/180 [2:31:43<2:46:32, 83.27s/it]                                                     {'padding_count': 0, 'loss': 3.0034, 'learning_rate': 3.7037037037037037e-05, 'epoch': 6.61}
 33%|███▎      | 60/180 [2:31:43<2:46:32, 83.27s/it] 34%|███▍      | 61/180 [2:31:50<1:59:53, 60.45s/it] 34%|███▍      | 62/180 [2:31:58<1:27:28, 44.48s/it] 35%|███▌      | 63/180 [2:32:05<1:04:58, 33.32s/it][INFO|trainer.py:528] 2022-06-15 09:18:27,932 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 09:18:27,934 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 09:18:27,935 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 09:18:27,935 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.25it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.99it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.73it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.96it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.40it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.01it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.76it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.60it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.47it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.45it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.44it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.44it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.42it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.43it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.43it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.44it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.45it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.47it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.47it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.48it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.48it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.43it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.43it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.46it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.40it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.41it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.43it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.43it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.45it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.44it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.43it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.44it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.45it/s][A                                                    
                                               [A{'eval_loss': 3.1807749271392822, 'eval_runtime': 5.961, 'eval_samples_per_second': 45.63, 'eval_steps_per_second': 11.407, 'epoch': 6.91}
 35%|███▌      | 63/180 [2:32:17<1:04:58, 33.32s/it]
100%|██████████| 68/68 [00:05<00:00, 11.45it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 09:18:33,913 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-63
[INFO|configuration_utils.py:391] 2022-06-15 09:18:34,005 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-63/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 09:25:25,560 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-63/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 09:25:25,578 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 09:25:25,579 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-63/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 09:41:33,930 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-45] due to args.save_total_limit
 36%|███▌      | 64/180 [2:55:25<14:17:05, 443.32s/it] 36%|███▌      | 65/180 [2:55:32<9:58:53, 312.47s/it]  37%|███▋      | 66/180 [2:55:39<6:59:39, 220.87s/it] 37%|███▋      | 67/180 [2:55:46<4:55:14, 156.76s/it] 38%|███▊      | 68/180 [2:55:54<3:28:50, 111.88s/it] 38%|███▊      | 69/180 [2:56:01<2:28:54, 80.49s/it]  39%|███▉      | 70/180 [2:56:08<1:47:14, 58.49s/it]                                                    {'padding_count': 0, 'loss': 2.8876, 'learning_rate': 3.395061728395062e-05, 'epoch': 7.71}
 39%|███▉      | 70/180 [2:56:08<1:47:14, 58.49s/it] 39%|███▉      | 71/180 [2:56:15<1:18:18, 43.10s/it] 40%|████      | 72/180 [2:56:22<58:11, 32.33s/it]  [INFO|trainer.py:528] 2022-06-15 09:42:45,318 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 09:42:45,320 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 09:42:45,320 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 09:42:45,320 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.09it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.87it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.64it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.90it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.43it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.12it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.92it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.77it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.62it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.57it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.54it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.53it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.50it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.49it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.47it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.46it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.46it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.47it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.47it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.48it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.47it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.42it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.43it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.43it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.40it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.42it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.44it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.42it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.43it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.44it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.46it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.47it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.48it/s][A                                                  
                                               [A{'eval_loss': 3.190709352493286, 'eval_runtime': 5.9448, 'eval_samples_per_second': 45.754, 'eval_steps_per_second': 11.439, 'epoch': 7.91}
 40%|████      | 72/180 [2:56:34<58:11, 32.33s/it]
100%|██████████| 68/68 [00:05<00:00, 11.48it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 09:42:51,267 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-72
[INFO|configuration_utils.py:391] 2022-06-15 09:42:51,270 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-72/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 09:50:18,180 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-72/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 09:50:18,184 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-72/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 09:50:18,186 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-72/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 10:05:47,354 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-63] due to args.save_total_limit
 41%|████      | 73/180 [3:19:38<13:07:17, 441.47s/it] 41%|████      | 74/180 [3:19:46<9:09:43, 311.17s/it]  42%|████▏     | 75/180 [3:19:53<6:24:56, 219.96s/it] 42%|████▏     | 76/180 [3:20:00<4:30:37, 156.13s/it] 43%|████▎     | 77/180 [3:20:07<3:11:18, 111.44s/it] 43%|████▎     | 78/180 [3:20:14<2:16:16, 80.17s/it]  44%|████▍     | 79/180 [3:20:22<1:38:05, 58.27s/it] 44%|████▍     | 80/180 [3:20:29<1:11:34, 42.94s/it]                                                    {'padding_count': 0, 'loss': 2.8146, 'learning_rate': 3.08641975308642e-05, 'epoch': 8.81}
 44%|████▍     | 80/180 [3:20:29<1:11:34, 42.94s/it] 45%|████▌     | 81/180 [3:20:36<53:09, 32.22s/it]  [INFO|trainer.py:528] 2022-06-15 10:06:58,832 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 10:06:58,834 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 10:06:58,835 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 10:06:58,835 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.21it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.97it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.71it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.95it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.47it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.15it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.91it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.77it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.65it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.58it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.53it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.52it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.51it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.48it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.45it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.45it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.45it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.44it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.43it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.39it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.39it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.36it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.39it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.41it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.37it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.40it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.40it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.41it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.42it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.42it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.44it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.44it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.44it/s][A                                                  
                                               [A{'eval_loss': 3.2054717540740967, 'eval_runtime': 5.9533, 'eval_samples_per_second': 45.689, 'eval_steps_per_second': 11.422, 'epoch': 8.91}
 45%|████▌     | 81/180 [3:20:48<53:09, 32.22s/it]
100%|██████████| 68/68 [00:05<00:00, 11.44it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 10:07:05,042 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-81
[INFO|configuration_utils.py:391] 2022-06-15 10:07:05,074 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-81/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 10:17:47,265 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 10:17:47,272 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 10:17:47,274 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-81/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 10:32:33,109 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-72] due to args.save_total_limit
 46%|████▌     | 82/180 [3:46:24<13:15:34, 487.09s/it] 46%|████▌     | 83/180 [3:46:33<9:15:24, 343.55s/it]  47%|████▋     | 84/180 [3:46:41<6:28:39, 242.91s/it] 47%|████▋     | 85/180 [3:46:48<4:32:38, 172.19s/it] 48%|████▊     | 86/180 [3:46:55<3:12:12, 122.69s/it] 48%|████▊     | 87/180 [3:47:03<2:16:29, 88.06s/it]  49%|████▉     | 88/180 [3:47:10<1:37:49, 63.80s/it] 49%|████▉     | 89/180 [3:47:17<1:11:00, 46.82s/it] 50%|█████     | 90/180 [3:47:24<52:25, 34.95s/it]                                                    {'padding_count': 0, 'loss': 2.7186, 'learning_rate': 2.777777777777778e-05, 'epoch': 9.91}
 50%|█████     | 90/180 [3:47:24<52:25, 34.95s/it][INFO|trainer.py:528] 2022-06-15 10:33:47,269 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 10:33:47,271 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 10:33:47,272 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 10:33:47,272 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.22it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.98it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.71it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.91it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.43it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.10it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.90it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.76it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.65it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.58it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.54it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.50it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.47it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.46it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.45it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.43it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.42it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.44it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.44it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.45it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.45it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.41it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.40it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.41it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.38it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.40it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.43it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.45it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.42it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.43it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.44it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.44it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.45it/s][A                                                  
                                               [A{'eval_loss': 3.223776340484619, 'eval_runtime': 5.9527, 'eval_samples_per_second': 45.693, 'eval_steps_per_second': 11.423, 'epoch': 9.91}
 50%|█████     | 90/180 [3:47:36<52:25, 34.95s/it]
100%|██████████| 68/68 [00:05<00:00, 11.45it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 10:33:53,488 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-90
[INFO|configuration_utils.py:391] 2022-06-15 10:33:53,526 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-90/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 10:40:21,618 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-90/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 10:40:21,621 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-90/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 10:40:21,623 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-90/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 10:58:20,356 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-81] due to args.save_total_limit
 51%|█████     | 91/180 [4:12:12<11:38:06, 470.63s/it] 51%|█████     | 92/180 [4:12:19<8:06:19, 331.58s/it]  52%|█████▏    | 93/180 [4:12:26<5:39:39, 234.25s/it] 52%|█████▏    | 94/180 [4:12:33<3:58:07, 166.13s/it] 53%|█████▎    | 95/180 [4:12:40<2:47:47, 118.44s/it] 53%|█████▎    | 96/180 [4:12:47<1:59:05, 85.07s/it]  54%|█████▍    | 97/180 [4:12:55<1:25:22, 61.72s/it] 54%|█████▍    | 98/180 [4:13:02<1:01:59, 45.36s/it] 55%|█████▌    | 99/180 [4:13:09<45:46, 33.91s/it]  [INFO|trainer.py:528] 2022-06-15 10:59:31,899 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 10:59:31,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 10:59:31,901 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 10:59:31,901 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 16.95it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.79it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.60it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.90it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.44it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.12it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.91it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.75it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.64it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.57it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.51it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.49it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.48it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.46it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.45it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.44it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.46it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.47it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.46it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.46it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.44it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.40it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.40it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.42it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.38it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.41it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.44it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.45it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.45it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.45it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.45it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.45it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.43it/s][A                                                  
                                               [A{'eval_loss': 3.228529214859009, 'eval_runtime': 5.9517, 'eval_samples_per_second': 45.701, 'eval_steps_per_second': 11.425, 'epoch': 10.91}
 55%|█████▌    | 99/180 [4:13:21<45:46, 33.91s/it]
100%|██████████| 68/68 [00:05<00:00, 11.43it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 10:59:37,855 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-99
[INFO|configuration_utils.py:391] 2022-06-15 10:59:37,878 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-99/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 11:05:53,245 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-99/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 11:05:53,249 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-99/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 11:05:53,251 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-99/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 11:24:15,684 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-90] due to args.save_total_limit
 56%|█████▌    | 100/180 [4:38:07<10:30:47, 473.10s/it]                                                       {'padding_count': 0, 'loss': 2.92, 'learning_rate': 2.4691358024691357e-05, 'epoch': 11.1}
 56%|█████▌    | 100/180 [4:38:07<10:30:47, 473.10s/it] 56%|█████▌    | 101/180 [4:38:14<7:18:52, 333.32s/it]  57%|█████▋    | 102/180 [4:38:21<5:06:06, 235.47s/it] 57%|█████▋    | 103/180 [4:38:28<3:34:17, 166.98s/it] 58%|█████▊    | 104/180 [4:38:36<2:30:46, 119.04s/it] 58%|█████▊    | 105/180 [4:38:43<1:46:52, 85.49s/it]  59%|█████▉    | 106/180 [4:38:50<1:16:30, 62.03s/it] 59%|█████▉    | 107/180 [4:38:57<55:27, 45.59s/it]   60%|██████    | 108/180 [4:39:04<40:53, 34.07s/it][INFO|trainer.py:528] 2022-06-15 11:25:27,373 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 11:25:27,375 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 11:25:27,375 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 11:25:27,375 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.19it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.94it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.69it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.94it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.47it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.13it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.91it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.75it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.65it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.56it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.52it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.50it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.48it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.46it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.46it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.46it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.47it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.46it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.44it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.45it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.45it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.39it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.41it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.41it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.35it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.37it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.40it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.41it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.42it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.42it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.42it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.43it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.44it/s][A                                                   
                                               [A{'eval_loss': 3.235318899154663, 'eval_runtime': 5.9546, 'eval_samples_per_second': 45.679, 'eval_steps_per_second': 11.42, 'epoch': 11.91}
 60%|██████    | 108/180 [4:39:17<40:53, 34.07s/it]
100%|██████████| 68/68 [00:05<00:00, 11.44it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 11:25:33,333 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-108
[INFO|configuration_utils.py:391] 2022-06-15 11:25:33,599 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-108/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 11:34:42,997 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-108/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 11:34:43,001 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-108/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 11:34:43,003 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-108/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 11:48:08,651 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-99] due to args.save_total_limit
 61%|██████    | 109/180 [5:02:00<8:36:28, 436.45s/it] 61%|██████    | 110/180 [5:02:07<5:58:56, 307.66s/it]                                                      {'padding_count': 0, 'loss': 2.5983, 'learning_rate': 2.1604938271604937e-05, 'epoch': 12.2}
 61%|██████    | 110/180 [5:02:07<5:58:56, 307.66s/it] 62%|██████▏   | 111/180 [5:02:14<4:10:07, 217.50s/it] 62%|██████▏   | 112/180 [5:02:21<2:54:59, 154.40s/it] 63%|██████▎   | 113/180 [5:02:28<2:03:05, 110.23s/it] 63%|██████▎   | 114/180 [5:02:36<1:27:14, 79.32s/it]  64%|██████▍   | 115/180 [5:02:43<1:02:28, 57.67s/it] 64%|██████▍   | 116/180 [5:02:50<45:21, 42.52s/it]   65%|██████▌   | 117/180 [5:02:57<33:31, 31.93s/it][INFO|trainer.py:528] 2022-06-15 11:49:20,040 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 11:49:20,043 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 11:49:20,043 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 11:49:20,043 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.15it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.92it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.68it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.94it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.47it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.15it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.93it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.77it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.64it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.58it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.54it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.52it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.42it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.41it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.42it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.44it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.43it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.45it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.46it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.47it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.48it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.44it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.42it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.43it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.38it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.41it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.44it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.44it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.44it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.46it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.45it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.45it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.46it/s][A                                                   
                                               [A{'eval_loss': 3.2504374980926514, 'eval_runtime': 5.9479, 'eval_samples_per_second': 45.73, 'eval_steps_per_second': 11.433, 'epoch': 12.91}
 65%|██████▌   | 117/180 [5:03:09<33:31, 31.93s/it]
100%|██████████| 68/68 [00:05<00:00, 11.46it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 11:49:25,993 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-117
[INFO|configuration_utils.py:391] 2022-06-15 11:49:25,996 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-117/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 11:56:05,407 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 11:56:05,411 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 11:56:05,413 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-117/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 12:09:29,924 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-108] due to args.save_total_limit
 66%|██████▌   | 118/180 [5:23:21<6:42:30, 389.53s/it] 66%|██████▌   | 119/180 [5:23:28<4:39:23, 274.82s/it] 67%|██████▋   | 120/180 [5:23:35<3:14:31, 194.52s/it]                                                      {'padding_count': 0, 'loss': 2.526, 'learning_rate': 1.8518518518518518e-05, 'epoch': 13.3}
 67%|██████▋   | 120/180 [5:23:35<3:14:31, 194.52s/it] 67%|██████▋   | 121/180 [5:23:43<2:16:01, 138.32s/it] 68%|██████▊   | 122/180 [5:23:50<1:35:42, 99.01s/it]  68%|██████▊   | 123/180 [5:23:57<1:07:56, 71.52s/it] 69%|██████▉   | 124/180 [5:24:05<48:47, 52.27s/it]   69%|██████▉   | 125/180 [5:24:12<35:31, 38.76s/it] 70%|███████   | 126/180 [5:24:19<26:23, 29.32s/it][INFO|trainer.py:528] 2022-06-15 12:10:42,081 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 12:10:42,083 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 12:10:42,083 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 12:10:42,083 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 16.97it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.70it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.42it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.67it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.26it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 11.99it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.81it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.70it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.61it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.56it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.51it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.47it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.46it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.44it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.43it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.43it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.43it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.41it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.42it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.42it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.42it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.38it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.40it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.41it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.35it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.36it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.40it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.39it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.40it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.42it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.44it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.45it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.44it/s][A                                                   
                                               [A{'eval_loss': 3.269155740737915, 'eval_runtime': 5.9754, 'eval_samples_per_second': 45.52, 'eval_steps_per_second': 11.38, 'epoch': 13.91}
 70%|███████   | 126/180 [5:24:31<26:23, 29.32s/it]
100%|██████████| 68/68 [00:05<00:00, 11.44it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 12:10:48,061 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-126
[INFO|configuration_utils.py:391] 2022-06-15 12:10:48,342 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-126/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 12:18:33,521 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-126/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 12:18:33,524 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 12:18:33,526 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-126/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 12:36:44,807 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-117] due to args.save_total_limit
 71%|███████   | 127/180 [5:50:36<7:15:59, 493.57s/it] 71%|███████   | 128/180 [5:50:43<5:01:17, 347.64s/it] 72%|███████▏  | 129/180 [5:50:50<3:28:40, 245.49s/it] 72%|███████▏  | 130/180 [5:50:57<2:24:59, 174.00s/it]                                                      {'padding_count': 0, 'loss': 2.494, 'learning_rate': 1.54320987654321e-05, 'epoch': 14.41}
 72%|███████▏  | 130/180 [5:50:57<2:24:59, 174.00s/it] 73%|███████▎  | 131/180 [5:51:05<1:41:13, 123.95s/it] 73%|███████▎  | 132/180 [5:51:12<1:11:08, 88.93s/it]  74%|███████▍  | 133/180 [5:51:19<50:27, 64.42s/it]   74%|███████▍  | 134/180 [5:51:26<36:13, 47.25s/it] 75%|███████▌  | 135/180 [5:51:33<26:26, 35.26s/it][INFO|trainer.py:528] 2022-06-15 12:37:56,447 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 12:37:56,450 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 12:37:56,450 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 12:37:56,450 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.20it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.96it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.70it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.93it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.43it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.11it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.90it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.77it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.67it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.61it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.56it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.51it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.49it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.49it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.47it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.47it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.46it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.47it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.48it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.48it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.47it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.42it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.41it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.43it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.39it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.42it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.44it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.46it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.46it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.43it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.44it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.46it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.40it/s][A                                                   
                                               [A{'eval_loss': 3.2799489498138428, 'eval_runtime': 5.9479, 'eval_samples_per_second': 45.73, 'eval_steps_per_second': 11.433, 'epoch': 14.91}
 75%|███████▌  | 135/180 [5:51:46<26:26, 35.26s/it]
100%|██████████| 68/68 [00:05<00:00, 11.40it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 12:38:02,400 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-135
[INFO|configuration_utils.py:391] 2022-06-15 12:38:02,403 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-135/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 12:44:29,480 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-135/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 12:44:29,484 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-135/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 12:44:29,485 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-135/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 13:00:54,163 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-126] due to args.save_total_limit
 76%|███████▌  | 136/180 [6:14:46<5:24:25, 442.39s/it] 76%|███████▌  | 137/180 [6:14:54<3:43:36, 312.02s/it] 77%|███████▋  | 138/180 [6:15:02<2:34:41, 221.00s/it] 77%|███████▋  | 139/180 [6:15:10<1:47:16, 156.99s/it] 78%|███████▊  | 140/180 [6:15:17<1:14:41, 112.05s/it]                                                      {'padding_count': 0, 'loss': 2.4529, 'learning_rate': 1.2345679012345678e-05, 'epoch': 15.51}
 78%|███████▊  | 140/180 [6:15:17<1:14:41, 112.05s/it] 78%|███████▊  | 141/180 [6:15:24<52:23, 80.60s/it]    79%|███████▉  | 142/180 [6:15:32<37:05, 58.57s/it] 79%|███████▉  | 143/180 [6:15:39<26:36, 43.16s/it] 80%|████████  | 144/180 [6:15:46<19:26, 32.40s/it][INFO|trainer.py:528] 2022-06-15 13:02:09,293 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 13:02:09,296 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 13:02:09,296 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 13:02:09,296 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.16it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.92it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.70it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.94it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.45it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.13it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.93it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.78it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.67it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.60it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.56it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.52it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.51it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.49it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.46it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.46it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.42it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.40it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.42it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.45it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.46it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.43it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.45it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.46it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.40it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.43it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.45it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.46it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.47it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.48it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.47it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.49it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.49it/s][A                                                   
                                               [A{'eval_loss': 3.287095069885254, 'eval_runtime': 5.9397, 'eval_samples_per_second': 45.794, 'eval_steps_per_second': 11.448, 'epoch': 15.91}
 80%|████████  | 144/180 [6:15:58<19:26, 32.40s/it]
100%|██████████| 68/68 [00:05<00:00, 11.49it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 13:02:15,237 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-144
[INFO|configuration_utils.py:391] 2022-06-15 13:02:15,241 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-144/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 13:13:54,933 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-144/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 13:13:54,937 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-144/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 13:13:54,938 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-144/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 13:30:24,160 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-135] due to args.save_total_limit
 81%|████████  | 145/180 [6:44:15<5:12:21, 535.47s/it] 81%|████████  | 146/180 [6:44:22<3:33:36, 376.97s/it] 82%|████████▏ | 147/180 [6:44:30<2:26:18, 266.02s/it] 82%|████████▏ | 148/180 [6:44:37<1:40:27, 188.36s/it] 83%|████████▎ | 149/180 [6:44:44<1:09:14, 134.00s/it] 83%|████████▎ | 150/180 [6:44:51<47:58, 95.96s/it]                                                      {'padding_count': 0, 'loss': 2.4002, 'learning_rate': 9.259259259259259e-06, 'epoch': 16.61}
 83%|████████▎ | 150/180 [6:44:51<47:58, 95.96s/it] 84%|████████▍ | 151/180 [6:44:58<33:30, 69.33s/it] 84%|████████▍ | 152/180 [6:45:05<23:39, 50.68s/it] 85%|████████▌ | 153/180 [6:45:13<16:56, 37.63s/it][INFO|trainer.py:528] 2022-06-15 13:31:35,572 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 13:31:35,574 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 13:31:35,574 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 13:31:35,574 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.20it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.96it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.70it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.95it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.46it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.14it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.94it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.79it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.68it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.61it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.57it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.52it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.50it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.48it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.46it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.47it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.47it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.48it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.48it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.47it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.47it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.41it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.42it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.42it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.37it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.40it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.42it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.44it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.45it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.46it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.47it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.47it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.48it/s][A                                                   
                                               [A{'eval_loss': 3.2917678356170654, 'eval_runtime': 5.9408, 'eval_samples_per_second': 45.785, 'eval_steps_per_second': 11.446, 'epoch': 16.91}
 85%|████████▌ | 153/180 [6:45:25<16:56, 37.63s/it]
100%|██████████| 68/68 [00:05<00:00, 11.48it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 13:31:41,517 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-153
[INFO|configuration_utils.py:391] 2022-06-15 13:31:41,793 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-153/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 13:39:08,305 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-153/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 13:39:08,309 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-153/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 13:39:08,310 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-153/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 13:52:01,448 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-144] due to args.save_total_limit
 86%|████████▌ | 154/180 [7:05:53<2:52:37, 398.36s/it] 86%|████████▌ | 155/180 [7:06:00<1:57:04, 281.00s/it] 87%|████████▋ | 156/180 [7:06:07<1:19:32, 198.84s/it] 87%|████████▋ | 157/180 [7:06:14<54:10, 141.34s/it]   88%|████████▊ | 158/180 [7:06:21<37:04, 101.10s/it] 88%|████████▊ | 159/180 [7:06:29<25:31, 72.94s/it]  89%|████████▉ | 160/180 [7:06:37<17:51, 53.57s/it]                                                   {'padding_count': 0, 'loss': 2.3755, 'learning_rate': 6.172839506172839e-06, 'epoch': 17.71}
 89%|████████▉ | 160/180 [7:06:37<17:51, 53.57s/it] 89%|████████▉ | 161/180 [7:06:44<12:34, 39.72s/it] 90%|█████████ | 162/180 [7:06:52<08:59, 29.99s/it][INFO|trainer.py:528] 2022-06-15 13:53:14,604 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 13:53:14,606 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 13:53:14,606 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 13:53:14,606 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.14it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.88it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.64it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.90it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.41it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 12.10it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.88it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.73it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.53it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.39it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.28it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.21it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.16it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.12it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.14it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.15it/s][A
 51%|█████▏    | 35/68 [00:03<00:02, 11.17it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.20it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.19it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.20it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.20it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.17it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.18it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.17it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.15it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.17it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.19it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.19it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.26it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.32it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.37it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.41it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.43it/s][A                                                   
                                               [A{'eval_loss': 3.294116497039795, 'eval_runtime': 6.0383, 'eval_samples_per_second': 45.046, 'eval_steps_per_second': 11.261, 'epoch': 17.91}
 90%|█████████ | 162/180 [7:07:04<08:59, 29.99s/it]
100%|██████████| 68/68 [00:05<00:00, 11.43it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 13:53:20,647 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-162
[INFO|configuration_utils.py:391] 2022-06-15 13:53:20,650 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-162/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 14:00:21,031 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 14:00:21,035 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 14:00:21,037 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-162/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 14:13:33,174 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-153] due to args.save_total_limit
 91%|█████████ | 163/180 [7:27:24<1:50:43, 390.79s/it] 91%|█████████ | 164/180 [7:27:31<1:13:31, 275.70s/it] 92%|█████████▏| 165/180 [7:27:39<48:46, 195.13s/it]   92%|█████████▏| 166/180 [7:27:46<32:22, 138.74s/it] 93%|█████████▎| 167/180 [7:27:53<21:30, 99.27s/it]  93%|█████████▎| 168/180 [7:28:00<14:19, 71.66s/it] 94%|█████████▍| 169/180 [7:28:07<09:35, 52.32s/it] 94%|█████████▍| 170/180 [7:28:15<06:27, 38.78s/it]                                                   {'padding_count': 0, 'loss': 2.3675, 'learning_rate': 3.0864197530864196e-06, 'epoch': 18.81}
 94%|█████████▍| 170/180 [7:28:15<06:27, 38.78s/it] 95%|█████████▌| 171/180 [7:28:22<04:23, 29.31s/it][INFO|trainer.py:528] 2022-06-15 14:14:44,735 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 14:14:44,738 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 14:14:44,738 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 14:14:44,738 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 16.73it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.55it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.34it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.61it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.15it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 11.91it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.78it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.69it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.60it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.54it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.50it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.50it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.50it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.47it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.47it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.47it/s][A
 51%|█████▏    | 35/68 [00:02<00:02, 11.46it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.45it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.45it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.45it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.46it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.42it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.43it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.45it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.38it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.39it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.42it/s][A
 84%|████████▍ | 57/68 [00:04<00:00, 11.41it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.43it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.44it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.44it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.44it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.44it/s][A                                                   
                                               [A{'eval_loss': 3.2997756004333496, 'eval_runtime': 5.971, 'eval_samples_per_second': 45.554, 'eval_steps_per_second': 11.388, 'epoch': 18.91}
 95%|█████████▌| 171/180 [7:28:34<04:23, 29.31s/it]
100%|██████████| 68/68 [00:05<00:00, 11.44it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 14:14:50,711 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-171
[INFO|configuration_utils.py:391] 2022-06-15 14:14:50,979 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-171/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 14:21:23,256 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-171/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 14:21:23,260 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-171/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 14:21:23,261 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-171/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 14:33:55,408 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-162] due to args.save_total_limit
 96%|█████████▌| 172/180 [7:47:47<49:19, 369.95s/it] 96%|█████████▌| 173/180 [7:47:54<30:27, 261.13s/it] 97%|█████████▋| 174/180 [7:48:01<18:29, 184.94s/it] 97%|█████████▋| 175/180 [7:48:08<10:58, 131.62s/it] 98%|█████████▊| 176/180 [7:48:15<06:17, 94.29s/it]  98%|█████████▊| 177/180 [7:48:22<03:24, 68.16s/it] 99%|█████████▉| 178/180 [7:48:30<01:39, 49.87s/it] 99%|█████████▉| 179/180 [7:48:37<00:37, 37.06s/it]100%|██████████| 180/180 [7:48:44<00:00, 28.10s/it]                                                   {'padding_count': 0, 'loss': 2.3431, 'learning_rate': 0.0, 'epoch': 19.91}
100%|██████████| 180/180 [7:48:44<00:00, 28.10s/it][INFO|trainer.py:528] 2022-06-15 14:35:00,823 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 14:35:00,825 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 14:35:00,825 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 14:35:00,825 >>   Batch size = 2

  0%|          | 0/68 [00:00<?, ?it/s][A
  4%|▍         | 3/68 [00:00<00:03, 17.19it/s][A
  7%|▋         | 5/68 [00:00<00:04, 14.95it/s][A
 10%|█         | 7/68 [00:00<00:04, 13.62it/s][A
 13%|█▎        | 9/68 [00:00<00:04, 12.73it/s][A
 16%|█▌        | 11/68 [00:00<00:04, 12.13it/s][A
 19%|█▉        | 13/68 [00:01<00:04, 11.81it/s][A
 22%|██▏       | 15/68 [00:01<00:04, 11.61it/s][A
 25%|██▌       | 17/68 [00:01<00:04, 11.46it/s][A
 28%|██▊       | 19/68 [00:01<00:04, 11.33it/s][A
 31%|███       | 21/68 [00:01<00:04, 11.26it/s][A
 34%|███▍      | 23/68 [00:01<00:03, 11.25it/s][A
 37%|███▋      | 25/68 [00:02<00:03, 11.23it/s][A
 40%|███▉      | 27/68 [00:02<00:03, 11.22it/s][A
 43%|████▎     | 29/68 [00:02<00:03, 11.14it/s][A
 46%|████▌     | 31/68 [00:02<00:03, 11.08it/s][A
 49%|████▊     | 33/68 [00:02<00:03, 11.13it/s][A
 51%|█████▏    | 35/68 [00:03<00:02, 11.13it/s][A
 54%|█████▍    | 37/68 [00:03<00:02, 11.15it/s][A
 57%|█████▋    | 39/68 [00:03<00:02, 11.17it/s][A
 60%|██████    | 41/68 [00:03<00:02, 11.21it/s][A
 63%|██████▎   | 43/68 [00:03<00:02, 11.17it/s][A
 66%|██████▌   | 45/68 [00:03<00:02, 11.15it/s][A
 69%|██████▉   | 47/68 [00:04<00:01, 11.12it/s][A
 72%|███████▏  | 49/68 [00:04<00:01, 11.15it/s][A
 75%|███████▌  | 51/68 [00:04<00:01, 11.10it/s][A
 78%|███████▊  | 53/68 [00:04<00:01, 11.13it/s][A
 81%|████████  | 55/68 [00:04<00:01, 11.13it/s][A
 84%|████████▍ | 57/68 [00:05<00:00, 11.15it/s][A
 87%|████████▋ | 59/68 [00:05<00:00, 11.14it/s][A
 90%|████████▉ | 61/68 [00:05<00:00, 11.19it/s][A
 93%|█████████▎| 63/68 [00:05<00:00, 11.23it/s][A
 96%|█████████▌| 65/68 [00:05<00:00, 11.20it/s][A
 99%|█████████▊| 67/68 [00:05<00:00, 11.21it/s][A                                                   
                                               [A{'eval_loss': 3.2966513633728027, 'eval_runtime': 6.0916, 'eval_samples_per_second': 44.652, 'eval_steps_per_second': 11.163, 'epoch': 19.91}
100%|██████████| 180/180 [7:48:50<00:00, 28.10s/it]
100%|██████████| 68/68 [00:06<00:00, 11.21it/s][A
                                               [A[INFO|trainer.py:2072] 2022-06-15 14:35:06,919 >> Saving model checkpoint to experiments/output/qmsum_led-1024/checkpoint-180
[INFO|configuration_utils.py:391] 2022-06-15 14:35:07,188 >> Configuration saved in experiments/output/qmsum_led-1024/checkpoint-180/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 14:41:14,127 >> Model weights saved in experiments/output/qmsum_led-1024/checkpoint-180/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 14:41:14,131 >> tokenizer config file saved in experiments/output/qmsum_led-1024/checkpoint-180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 14:41:14,133 >> Special tokens file saved in experiments/output/qmsum_led-1024/checkpoint-180/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-15 14:53:10,293 >> Deleting older checkpoint [experiments/output/qmsum_led-1024/checkpoint-171] due to args.save_total_limit
[INFO|trainer.py:1507] 2022-06-15 14:53:11,023 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1515] 2022-06-15 14:53:11,024 >> Loading best model from experiments/output/qmsum_led-1024/checkpoint-54 (score: 3.176757574081421).
                                                   {'train_runtime': 29231.4962, 'train_samples_per_second': 0.86, 'train_steps_per_second': 0.006, 'train_loss': 2.888267845577664, 'epoch': 19.91}
100%|██████████| 180/180 [8:07:04<00:00, 28.10s/it]100%|██████████| 180/180 [8:07:04<00:00, 162.36s/it]
[INFO|trainer.py:2072] 2022-06-15 14:53:20,643 >> Saving model checkpoint to experiments/output/qmsum_led-1024
[INFO|configuration_utils.py:391] 2022-06-15 14:53:20,662 >> Configuration saved in experiments/output/qmsum_led-1024/config.json
[INFO|modeling_utils.py:1001] 2022-06-15 14:59:25,902 >> Model weights saved in experiments/output/qmsum_led-1024/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-15 14:59:25,906 >> tokenizer config file saved in experiments/output/qmsum_led-1024/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-15 14:59:25,908 >> Special tokens file saved in experiments/output/qmsum_led-1024/special_tokens_map.json
***** train metrics *****
  epoch                    =      19.91
  train_loss               =     2.8883
  train_runtime            = 8:07:11.49
  train_samples            =       1257
  train_samples_per_second =       0.86
  train_steps_per_second   =      0.006
2022-06-15 14:59:29 | INFO | __main__ | *** Evaluate ***
[INFO|trainer.py:528] 2022-06-15 14:59:29,094 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-15 14:59:29,100 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-15 14:59:29,101 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-15 14:59:29,101 >>   Batch size = 2
  0%|          | 0/68 [00:00<?, ?it/s]  4%|▍         | 3/68 [00:00<00:04, 15.97it/s]  7%|▋         | 5/68 [00:00<00:04, 14.11it/s] 10%|█         | 7/68 [00:00<00:04, 13.26it/s] 13%|█▎        | 9/68 [00:00<00:04, 12.74it/s] 16%|█▌        | 11/68 [00:00<00:04, 12.40it/s] 19%|█▉        | 13/68 [00:01<00:04, 12.16it/s] 22%|██▏       | 15/68 [00:01<00:04, 11.99it/s] 25%|██▌       | 17/68 [00:01<00:04, 11.88it/s] 28%|██▊       | 19/68 [00:01<00:04, 11.80it/s] 31%|███       | 21/68 [00:01<00:04, 11.57it/s] 34%|███▍      | 23/68 [00:01<00:04, 10.95it/s] 37%|███▋      | 25/68 [00:02<00:04, 10.56it/s] 40%|███▉      | 27/68 [00:02<00:04, 10.23it/s] 43%|████▎     | 29/68 [00:02<00:03,  9.93it/s] 44%|████▍     | 30/68 [00:02<00:03,  9.74it/s] 46%|████▌     | 31/68 [00:02<00:03,  9.59it/s] 47%|████▋     | 32/68 [00:02<00:03,  9.48it/s] 49%|████▊     | 33/68 [00:03<00:03,  9.40it/s] 50%|█████     | 34/68 [00:03<00:03,  9.33it/s] 51%|█████▏    | 35/68 [00:03<00:03,  9.30it/s] 53%|█████▎    | 36/68 [00:03<00:03,  9.28it/s] 54%|█████▍    | 37/68 [00:03<00:03,  9.28it/s] 56%|█████▌    | 38/68 [00:03<00:03,  9.27it/s] 57%|█████▋    | 39/68 [00:03<00:03,  9.28it/s] 59%|█████▉    | 40/68 [00:03<00:03,  9.28it/s] 60%|██████    | 41/68 [00:03<00:02,  9.28it/s] 62%|██████▏   | 42/68 [00:03<00:02,  9.28it/s] 63%|██████▎   | 43/68 [00:04<00:02,  9.27it/s] 65%|██████▍   | 44/68 [00:04<00:02,  9.25it/s] 66%|██████▌   | 45/68 [00:04<00:02,  9.25it/s] 68%|██████▊   | 46/68 [00:04<00:02,  9.25it/s] 69%|██████▉   | 47/68 [00:04<00:02,  9.23it/s] 71%|███████   | 48/68 [00:04<00:02,  9.25it/s] 72%|███████▏  | 49/68 [00:04<00:02,  9.26it/s] 74%|███████▎  | 50/68 [00:04<00:01,  9.24it/s] 75%|███████▌  | 51/68 [00:04<00:01,  9.25it/s] 76%|███████▋  | 52/68 [00:05<00:01,  9.25it/s] 78%|███████▊  | 53/68 [00:05<00:01,  9.25it/s] 79%|███████▉  | 54/68 [00:05<00:01,  9.26it/s] 81%|████████  | 55/68 [00:05<00:01,  9.26it/s] 82%|████████▏ | 56/68 [00:05<00:01,  9.27it/s] 84%|████████▍ | 57/68 [00:05<00:01,  9.24it/s] 85%|████████▌ | 58/68 [00:05<00:01,  9.25it/s] 87%|████████▋ | 59/68 [00:05<00:00,  9.24it/s] 88%|████████▊ | 60/68 [00:05<00:00,  9.24it/s] 90%|████████▉ | 61/68 [00:06<00:00,  9.24it/s] 91%|█████████ | 62/68 [00:06<00:00,  9.26it/s] 93%|█████████▎| 63/68 [00:06<00:00,  9.25it/s] 94%|█████████▍| 64/68 [00:06<00:00,  9.25it/s] 96%|█████████▌| 65/68 [00:06<00:00,  9.26it/s] 97%|█████████▋| 66/68 [00:06<00:00,  9.24it/s] 99%|█████████▊| 67/68 [00:06<00:00,  9.25it/s]100%|██████████| 68/68 [00:06<00:00,  9.26it/s]100%|██████████| 68/68 [00:06<00:00,  9.99it/s]
***** eval metrics *****
  epoch                   =      19.91
  eval_loss               =     3.1768
  eval_runtime            = 0:00:06.92
  eval_samples            =        272
  eval_samples_per_second =     39.277
  eval_steps_per_second   =      9.819
[INFO] 2022-06-15 14:59:38,734 api: [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
[INFO] 2022-06-15 14:59:38,735 api: Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/home/s1970716/miniconda3/envs/scrolls_venv/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
[INFO] 2022-06-15 14:59:38,736 api: Done waiting for other agents. Elapsed: 0.0010395050048828125 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "436520", "role": "default", "hostname": "arnold.inf.ed.ac.uk", "state": "SUCCEEDED", "total_run_time": 29696, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "436521", "role": "default", "hostname": "arnold.inf.ed.ac.uk", "state": "SUCCEEDED", "total_run_time": 29696, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "arnold.inf.ed.ac.uk", "state": "SUCCEEDED", "total_run_time": 29696, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
/disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/allenai-led-base-16384_global_1024_16_5e-05_4096_scrolls_qmsum_supermarket-shame-34
torch/distributed/run.py --nproc_per_node=2 --master_port=41322 src/run.py --dataset_config_name qmsum --dataset_name tau/scrolls --do_eval True --do_train True --drop_duplicates_in_eval False --evaluation_strategy epoch --greater_is_better False --metric_for_best_model loss --model_name_or_path allenai/led-base-16384 --num_train_epochs 20 --predict_with_generate False --save_strategy epoch --adam_epsilon 1e-6 --adam_beta1 0.9 --adam_beta2 0.98 --weight_decay 0.001 --logging_steps 10 --gradient_checkpointing true --save_total_limit 2 --preprocessing_num_workers 1 --group_by_length true --load_best_model_at_end True --lr_scheduler linear --warmup_ratio 0.1 --prediction_loss_only True --attention_window 1024 --max_target_length 1024 --fp16 True --train_max_tokens 4096 --gradient_accumulation_steps 16 --per_device_eval_batch_size 2 --learning_rate 5e-05 --global_attention_first_token True --folder_suffix global_attention_first_token$max_source_length$gradient_accumulation_steps$learning_rate$train_max_tokens --max_source_length 1024 --output_dir /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/allenai-led-base-16384_global_1024_16_5e-05_4096_scrolls_qmsum_supermarket-shame-34 --run_name allenai-led-base-16384_global_1024_16_5e-05_4096_scrolls_qmsum_supermarket-shame-34 --output_dir=experiments/output/qmsum_led-1024
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2022-06-15 14:59:57 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2022-06-15 15:00:03 | WARNING | __main__ | Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
2022-06-15 15:00:03 | INFO | __main__ | Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fp16_padding=False,
gradient_accumulation_steps=1,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=experiments/output/qmsum_led-1024/runs/Jun15_14-59-58_arnold.inf.ed.ac.uk,
logging_first_step=False,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=20.0,
output_dir=experiments/output/qmsum_led-1024,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=8,
per_device_train_max_batch_size=None,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=qmsum_led-1024,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=qmsum_led-1024_scrolls_qmsum_definition-reward-34,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_max_tokens=0,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
2022-06-15 15:00:06 | INFO | datasets.load | Checking experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py for additional imports.
2022-06-15 15:00:06 | INFO | datasets.utils.filelock | Lock 140255911619984 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py.lock
2022-06-15 15:00:06 | INFO | datasets.load | Found main folder for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls
2022-06-15 15:00:06 | INFO | datasets.load | Found specific version folder for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
2022-06-15 15:00:06 | INFO | datasets.load | Found script file from https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py to experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac/scrolls.py
2022-06-15 15:00:06 | INFO | datasets.load | Couldn't find dataset infos file at https://huggingface.co/datasets/tau/scrolls/resolve/main/dataset_infos.json
2022-06-15 15:00:06 | INFO | datasets.load | Found metadata file for dataset https://huggingface.co/datasets/tau/scrolls/resolve/main/scrolls.py at experiments/data/qmsum_led-1024/huggingface/modules/datasets_modules/datasets/scrolls/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac/scrolls.json
2022-06-15 15:00:06 | INFO | datasets.utils.filelock | Lock 140255911619984 released on experiments/data/qmsum_led-1024/huggingface/datasets/downloads/3254106765f01de32675c65e336f073ad7cc1d607327e536f7da10773d0d6d82.bc4e9ef739060959a4906cb55af66d2f51cbcdd35338551f6700c5062844da81.py.lock
2022-06-15 15:00:06 | INFO | datasets.utils.filelock | Lock 140255911618544 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 15:00:06 | INFO | datasets.builder | Overwrite dataset info from restored data version.
2022-06-15 15:00:06 | INFO | datasets.info | Loading Dataset info from experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
2022-06-15 15:00:06 | INFO | datasets.utils.filelock | Lock 140255911618544 released on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 15:00:06 | INFO | datasets.utils.filelock | Lock 140255911619984 acquired on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 15:00:06 | WARNING | datasets.builder | Reusing dataset scrolls (experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)
2022-06-15 15:00:06 | INFO | datasets.info | Loading Dataset info from experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
2022-06-15 15:00:06 | INFO | datasets.utils.filelock | Lock 140255911619984 released on experiments/data/qmsum_led-1024/huggingface/datasets/experiments_data_qmsum_led-1024_huggingface_datasets_scrolls_qmsum_1.0.0_672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac.lock
2022-06-15 15:00:06 | INFO | datasets.builder | Constructing Dataset for split train, validation, test, from experiments/data/qmsum_led-1024/huggingface/datasets/scrolls/qmsum/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 51.44it/s]
[INFO|configuration_utils.py:559] 2022-06-15 15:00:06,644 >> loading configuration file experiments/output/qmsum_led-1024/config.json
[INFO|configuration_utils.py:598] 2022-06-15 15:00:06,646 >> Model config LEDConfig {
  "_name_or_path": "allenai/led-base-16384",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "LEDForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_window": 1024,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "gradient_checkpointing": true,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_decoder_position_embeddings": 1024,
  "max_encoder_position_embeddings": 16384,
  "model_type": "led",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "remove_global_attention": false,
  "torch_dtype": "float32",
  "transformers_version": "4.10.0.dev0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1669] 2022-06-15 15:00:06,657 >> Didn't find file experiments/output/qmsum_led-1024/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1737] 2022-06-15 15:00:06,658 >> loading file experiments/output/qmsum_led-1024/vocab.json
[INFO|tokenization_utils_base.py:1737] 2022-06-15 15:00:06,658 >> loading file experiments/output/qmsum_led-1024/merges.txt
[INFO|tokenization_utils_base.py:1737] 2022-06-15 15:00:06,658 >> loading file experiments/output/qmsum_led-1024/tokenizer.json
[INFO|tokenization_utils_base.py:1737] 2022-06-15 15:00:06,658 >> loading file None
[INFO|tokenization_utils_base.py:1737] 2022-06-15 15:00:06,658 >> loading file experiments/output/qmsum_led-1024/special_tokens_map.json
[INFO|tokenization_utils_base.py:1737] 2022-06-15 15:00:06,658 >> loading file experiments/output/qmsum_led-1024/tokenizer_config.json
[INFO|modeling_utils.py:1277] 2022-06-15 15:00:06,909 >> loading weights file experiments/output/qmsum_led-1024/pytorch_model.bin
[INFO|modeling_utils.py:1524] 2022-06-15 15:00:14,715 >> All model checkpoint weights were used when initializing LEDForConditionalGeneration.

[INFO|modeling_utils.py:1532] 2022-06-15 15:00:14,715 >> All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at experiments/output/qmsum_led-1024.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.
2022-06-15 15:00:14 | INFO | datasets.arrow_writer | Done writing 281 indices in 2248 bytes .
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 31.00ba/s]
2022-06-15 15:00:14 | INFO | datasets.arrow_writer | Done writing 281 examples in 16406660 bytes .
Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:05<00:00,  5.79s/ba]Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:05<00:00,  5.79s/ba]
2022-06-15 15:00:20 | INFO | datasets.arrow_writer | Done writing 281 examples in 3818276 bytes .
[INFO|trainer.py:422] 2022-06-15 15:00:26,473 >> Using amp fp16 backend
2022-06-15 15:00:26 | INFO | __main__ | *** Predict ***
[INFO|trainer.py:528] 2022-06-15 15:00:26,474 >> The following columns in the test set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-15 15:00:26,476 >> ***** Running Prediction *****
[INFO|trainer.py:2326] 2022-06-15 15:00:26,476 >>   Num examples = 281
[INFO|trainer.py:2329] 2022-06-15 15:00:26,476 >>   Batch size = 4
  0%|          | 0/71 [00:00<?, ?it/s]  3%|▎         | 2/71 [00:00<00:22,  3.12it/s]  4%|▍         | 3/71 [00:01<00:32,  2.12it/s]  6%|▌         | 4/71 [00:02<00:43,  1.54it/s]  7%|▋         | 5/71 [00:11<03:28,  3.17s/it]  8%|▊         | 6/71 [00:12<02:38,  2.43s/it] 10%|▉         | 7/71 [00:13<02:12,  2.06s/it] 11%|█▏        | 8/71 [00:14<01:47,  1.71s/it] 13%|█▎        | 9/71 [00:15<01:29,  1.44s/it] 14%|█▍        | 10/71 [00:16<01:21,  1.33s/it] 15%|█▌        | 11/71 [00:17<01:14,  1.23s/it] 17%|█▋        | 12/71 [00:18<01:06,  1.12s/it] 18%|█▊        | 13/71 [00:18<01:00,  1.04s/it] 20%|█▉        | 14/71 [00:20<01:02,  1.09s/it] 21%|██        | 15/71 [00:21<00:57,  1.04s/it] 23%|██▎       | 16/71 [00:30<03:08,  3.43s/it] 24%|██▍       | 17/71 [00:31<02:32,  2.82s/it] 25%|██▌       | 18/71 [00:32<01:54,  2.17s/it] 27%|██▋       | 19/71 [00:41<03:40,  4.24s/it] 28%|██▊       | 20/71 [00:41<02:41,  3.17s/it] 30%|██▉       | 21/71 [00:51<04:07,  4.95s/it] 31%|███       | 22/71 [00:51<02:58,  3.64s/it] 32%|███▏      | 23/71 [00:53<02:22,  2.98s/it] 34%|███▍      | 24/71 [00:54<02:01,  2.59s/it] 35%|███▌      | 25/71 [00:55<01:33,  2.03s/it] 37%|███▋      | 26/71 [00:56<01:17,  1.72s/it] 38%|███▊      | 27/71 [00:57<01:01,  1.40s/it] 39%|███▉      | 28/71 [01:02<01:46,  2.47s/it] 41%|████      | 29/71 [01:02<01:23,  1.99s/it] 42%|████▏     | 30/71 [01:03<01:06,  1.61s/it] 44%|████▎     | 31/71 [01:07<01:34,  2.36s/it] 45%|████▌     | 32/71 [01:16<02:50,  4.38s/it] 46%|████▋     | 33/71 [01:25<03:39,  5.78s/it] 48%|████▊     | 34/71 [01:26<02:35,  4.22s/it] 49%|████▉     | 35/71 [01:30<02:26,  4.08s/it] 51%|█████     | 36/71 [01:34<02:25,  4.15s/it] 52%|█████▏    | 37/71 [01:35<01:49,  3.23s/it] 54%|█████▎    | 38/71 [01:36<01:22,  2.49s/it] 55%|█████▍    | 39/71 [01:37<01:05,  2.06s/it] 56%|█████▋    | 40/71 [01:38<00:51,  1.66s/it] 58%|█████▊    | 41/71 [01:38<00:41,  1.37s/it] 59%|█████▉    | 42/71 [01:43<01:09,  2.39s/it] 61%|██████    | 43/71 [01:44<00:51,  1.86s/it] 62%|██████▏   | 44/71 [01:53<01:49,  4.05s/it] 63%|██████▎   | 45/71 [01:54<01:22,  3.17s/it] 65%|██████▍   | 46/71 [02:03<02:03,  4.93s/it] 66%|██████▌   | 47/71 [02:04<01:30,  3.78s/it] 68%|██████▊   | 48/71 [02:05<01:05,  2.86s/it] 69%|██████▉   | 49/71 [02:06<00:54,  2.47s/it] 70%|███████   | 50/71 [02:07<00:42,  2.03s/it] 72%|███████▏  | 51/71 [02:09<00:37,  1.88s/it] 73%|███████▎  | 52/71 [02:10<00:30,  1.60s/it] 75%|███████▍  | 53/71 [02:11<00:23,  1.33s/it] 76%|███████▌  | 54/71 [02:20<01:01,  3.64s/it] 77%|███████▋  | 55/71 [02:21<00:45,  2.84s/it] 79%|███████▉  | 56/71 [02:21<00:32,  2.18s/it] 80%|████████  | 57/71 [02:22<00:26,  1.88s/it] 82%|████████▏ | 58/71 [02:23<00:19,  1.51s/it] 83%|████████▎ | 59/71 [02:24<00:16,  1.38s/it] 85%|████████▍ | 60/71 [02:33<00:40,  3.68s/it] 86%|████████▌ | 61/71 [02:34<00:28,  2.83s/it] 87%|████████▋ | 62/71 [02:35<00:20,  2.22s/it] 89%|████████▊ | 63/71 [02:36<00:14,  1.75s/it] 90%|█████████ | 64/71 [02:37<00:11,  1.57s/it] 92%|█████████▏| 65/71 [02:37<00:07,  1.31s/it] 93%|█████████▎| 66/71 [02:46<00:18,  3.64s/it] 94%|█████████▍| 67/71 [02:56<00:21,  5.27s/it] 96%|█████████▌| 68/71 [02:57<00:12,  4.10s/it] 97%|█████████▋| 69/71 [03:06<00:11,  5.58s/it] 99%|█████████▊| 70/71 [03:06<00:04,  4.04s/it]100%|██████████| 71/71 [03:10<00:00,  3.93s/it]2022-06-15 15:03:38 | INFO | __main__ | Generated Prediction goes to: experiments/output/qmsum_led-1024/generated_predictions.json
100%|██████████| 71/71 [03:10<00:00,  2.69s/it]
/disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/qmsum_led-1024_scrolls_qmsum_definition-reward-34
src/run.py --dataset_config_name qmsum --dataset_name tau/scrolls --drop_duplicates_in_eval True --greater_is_better False --metric_for_best_model loss --model_name_or_path experiments/output/qmsum_led-1024 --num_train_epochs 20 --logging_steps 10 --preprocessing_num_workers 1 --predict_with_generate True --num_beams 1 --attention_window 1024 --max_target_length 1024 --fp16 True --per_device_eval_batch_size 2 --do_predict True --global_attention_first_token True --max_source_length 1024 --output_dir /disk/nfs/ostrom/s1970716/scrolls_ilcc/baselines/outputs/qmsum_led-1024_scrolls_qmsum_definition-reward-34 --run_name qmsum_led-1024_scrolls_qmsum_definition-reward-34 --output_dir experiments/output/qmsum_led-1024
***** predict metrics *****
  predict_runtime            = 0:03:11.58
  predict_samples            =        281
  predict_samples_per_second =      1.467
  predict_steps_per_second   =      0.371
Adding predictions for qmsum from qmsum_file...
Adding predictions for qasper from qasper_file...
Adding predictions for summ_screen_fd from summ_screen_file...
Adding predictions for quality from quality_file...
Adding predictions for narrative_qa from narrative_qa_file...
Adding predictions for contract_nli from contract_nli_file...
Adding predictions for gov_report from gov_report_file...
Saving the complete predictions file to: ./scrolls_predictions.csv
validating submission file is exactly the same as expected
Your benchmark predictions file is ready. If it contains predictions for the test sets please head over to https://scrolls-benchmark.com/submission to submit to the SCROLLS leaderboard.
