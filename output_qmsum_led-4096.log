{'padding_count': 0, 'loss': 4.4564, 'learning_rate': 6.379585326953748e-08, 'epoch': 0.02}
{'padding_count': 0, 'loss': 4.435, 'learning_rate': 1.3556618819776716e-07, 'epoch': 0.03}
{'padding_count': 0, 'loss': 4.2861, 'learning_rate': 2.1531100478468902e-07, 'epoch': 0.05}
{'padding_count': 0, 'loss': 4.5834, 'learning_rate': 2.9505582137161085e-07, 'epoch': 0.06}
{'padding_count': 0, 'loss': 4.2473, 'learning_rate': 3.748006379585327e-07, 'epoch': 0.08}
{'padding_count': 0, 'loss': 4.235, 'learning_rate': 4.5454545454545457e-07, 'epoch': 0.1}
{'padding_count': 0, 'loss': 4.4335, 'learning_rate': 5.342902711323764e-07, 'epoch': 0.11}
{'padding_count': 0, 'loss': 4.0908, 'learning_rate': 6.140350877192982e-07, 'epoch': 0.13}
{'padding_count': 0, 'loss': 4.0085, 'learning_rate': 6.937799043062202e-07, 'epoch': 0.14}
{'padding_count': 0, 'loss': 3.9248, 'learning_rate': 7.735247208931421e-07, 'epoch': 0.16}
{'padding_count': 0, 'loss': 4.2821, 'learning_rate': 8.532695374800638e-07, 'epoch': 0.18}
{'padding_count': 0, 'loss': 3.8579, 'learning_rate': 9.330143540669858e-07, 'epoch': 0.19}
{'padding_count': 0, 'loss': 3.6571, 'learning_rate': 1.0127591706539077e-06, 'epoch': 0.21}
{'padding_count': 0, 'loss': 3.7368, 'learning_rate': 1.0925039872408293e-06, 'epoch': 0.22}
{'padding_count': 0, 'loss': 3.5748, 'learning_rate': 1.1722488038277514e-06, 'epoch': 0.24}
{'padding_count': 0, 'loss': 3.6277, 'learning_rate': 1.2519936204146732e-06, 'epoch': 0.26}
{'padding_count': 0, 'loss': 3.4905, 'learning_rate': 1.331738437001595e-06, 'epoch': 0.27}
{'padding_count': 0, 'loss': 3.644, 'learning_rate': 1.4114832535885167e-06, 'epoch': 0.29}
{'padding_count': 0, 'loss': 3.7802, 'learning_rate': 1.4912280701754387e-06, 'epoch': 0.3}
{'padding_count': 0, 'loss': 3.3611, 'learning_rate': 1.5709728867623605e-06, 'epoch': 0.32}
{'padding_count': 0, 'loss': 3.7095, 'learning_rate': 1.6507177033492824e-06, 'epoch': 0.33}
{'padding_count': 0, 'loss': 3.7442, 'learning_rate': 1.7304625199362044e-06, 'epoch': 0.35}
{'padding_count': 0, 'loss': 3.3245, 'learning_rate': 1.8102073365231263e-06, 'epoch': 0.37}
{'padding_count': 0, 'loss': 3.3054, 'learning_rate': 1.8899521531100479e-06, 'epoch': 0.38}
{'padding_count': 0, 'loss': 3.6985, 'learning_rate': 1.96969696969697e-06, 'epoch': 0.4}
{'padding_count': 0, 'loss': 3.2296, 'learning_rate': 2.0494417862838915e-06, 'epoch': 0.41}
{'padding_count': 0, 'loss': 3.4962, 'learning_rate': 2.1291866028708136e-06, 'epoch': 0.43}
{'padding_count': 0, 'loss': 3.3901, 'learning_rate': 2.2089314194577356e-06, 'epoch': 0.45}
{'padding_count': 0, 'loss': 3.5842, 'learning_rate': 2.2886762360446573e-06, 'epoch': 0.46}
{'padding_count': 0, 'loss': 3.344, 'learning_rate': 2.368421052631579e-06, 'epoch': 0.48}
{'padding_count': 0, 'loss': 3.4312, 'learning_rate': 2.448165869218501e-06, 'epoch': 0.49}
{'padding_count': 0, 'loss': 3.0991, 'learning_rate': 2.527910685805423e-06, 'epoch': 0.51}
{'padding_count': 0, 'loss': 3.7727, 'learning_rate': 2.6076555023923446e-06, 'epoch': 0.53}
{'padding_count': 0, 'loss': 3.0749, 'learning_rate': 2.687400318979266e-06, 'epoch': 0.54}
{'padding_count': 0, 'loss': 3.4548, 'learning_rate': 2.7671451355661882e-06, 'epoch': 0.56}
{'padding_count': 0, 'loss': 3.3438, 'learning_rate': 2.8468899521531103e-06, 'epoch': 0.57}
{'padding_count': 0, 'loss': 3.4859, 'learning_rate': 2.926634768740032e-06, 'epoch': 0.59}
{'padding_count': 0, 'loss': 3.4224, 'learning_rate': 3.006379585326954e-06, 'epoch': 0.61}
{'padding_count': 0, 'loss': 3.4431, 'learning_rate': 3.086124401913876e-06, 'epoch': 0.62}
{'padding_count': 0, 'loss': 3.2924, 'learning_rate': 3.1658692185007976e-06, 'epoch': 0.64}
{'padding_count': 0, 'loss': 3.2799, 'learning_rate': 3.2456140350877197e-06, 'epoch': 0.65}
{'padding_count': 0, 'loss': 3.3321, 'learning_rate': 3.3253588516746417e-06, 'epoch': 0.67}
{'padding_count': 0, 'loss': 3.0359, 'learning_rate': 3.4051036682615633e-06, 'epoch': 0.69}
{'padding_count': 1, 'loss': 3.2826, 'learning_rate': 3.4848484848484854e-06, 'epoch': 0.7}
{'padding_count': 1, 'loss': 3.4056, 'learning_rate': 3.5645933014354066e-06, 'epoch': 0.72}
{'padding_count': 1, 'loss': 3.3778, 'learning_rate': 3.6443381180223286e-06, 'epoch': 0.73}
{'padding_count': 1, 'loss': 3.2369, 'learning_rate': 3.7240829346092507e-06, 'epoch': 0.75}
{'padding_count': 1, 'loss': 3.2621, 'learning_rate': 3.8038277511961723e-06, 'epoch': 0.77}
{'padding_count': 1, 'loss': 3.422, 'learning_rate': 3.883572567783094e-06, 'epoch': 0.78}
{'padding_count': 1, 'loss': 3.0639, 'learning_rate': 3.963317384370016e-06, 'epoch': 0.8}
{'padding_count': 1, 'loss': 3.1264, 'learning_rate': 4.043062200956938e-06, 'epoch': 0.81}
{'padding_count': 1, 'loss': 3.3089, 'learning_rate': 4.12280701754386e-06, 'epoch': 0.83}
{'padding_count': 1, 'loss': 3.2889, 'learning_rate': 4.202551834130782e-06, 'epoch': 0.85}
{'padding_count': 1, 'loss': 3.492, 'learning_rate': 4.282296650717704e-06, 'epoch': 0.86}
{'padding_count': 1, 'loss': 3.484, 'learning_rate': 4.362041467304626e-06, 'epoch': 0.88}
{'padding_count': 1, 'loss': 3.2077, 'learning_rate': 4.441786283891548e-06, 'epoch': 0.89}
{'padding_count': 2, 'loss': 3.2647, 'learning_rate': 4.521531100478469e-06, 'epoch': 0.91}
{'padding_count': 2, 'loss': 3.1318, 'learning_rate': 4.601275917065391e-06, 'epoch': 0.93}
{'padding_count': 2, 'loss': 3.3545, 'learning_rate': 4.681020733652313e-06, 'epoch': 0.94}
{'padding_count': 2, 'loss': 3.1756, 'learning_rate': 4.760765550239235e-06, 'epoch': 0.96}
{'padding_count': 2, 'loss': 3.4869, 'learning_rate': 4.840510366826156e-06, 'epoch': 0.97}
{'padding_count': 2, 'loss': 3.5868, 'learning_rate': 4.920255183413078e-06, 'epoch': 0.99}
{'eval_loss': 3.2559938430786133, 'eval_runtime': 43.285, 'eval_samples_per_second': 6.284, 'eval_steps_per_second': 1.571, 'epoch': 1.0}
{'padding_count': 2, 'loss': 3.4093, 'learning_rate': 5e-06, 'epoch': 1.0}
{'padding_count': 2, 'loss': 3.1962, 'learning_rate': 5.0797448165869225e-06, 'epoch': 1.02}
{'padding_count': 2, 'loss': 3.2459, 'learning_rate': 5.1594896331738445e-06, 'epoch': 1.04}
{'padding_count': 2, 'loss': 3.4493, 'learning_rate': 5.239234449760766e-06, 'epoch': 1.05}
{'padding_count': 2, 'loss': 3.2206, 'learning_rate': 5.318979266347688e-06, 'epoch': 1.07}
{'padding_count': 2, 'loss': 3.14, 'learning_rate': 5.39872408293461e-06, 'epoch': 1.08}
{'padding_count': 2, 'loss': 3.1732, 'learning_rate': 5.478468899521532e-06, 'epoch': 1.1}
{'padding_count': 2, 'loss': 3.4794, 'learning_rate': 5.558213716108454e-06, 'epoch': 1.12}
{'padding_count': 2, 'loss': 3.294, 'learning_rate': 5.637958532695376e-06, 'epoch': 1.13}
{'padding_count': 2, 'loss': 3.2931, 'learning_rate': 5.717703349282297e-06, 'epoch': 1.15}
{'padding_count': 2, 'loss': 3.3285, 'learning_rate': 5.797448165869219e-06, 'epoch': 1.16}
{'padding_count': 2, 'loss': 3.6052, 'learning_rate': 5.877192982456141e-06, 'epoch': 1.18}
{'padding_count': 2, 'loss': 3.2801, 'learning_rate': 5.956937799043063e-06, 'epoch': 1.2}
{'padding_count': 2, 'loss': 3.3005, 'learning_rate': 6.0366826156299844e-06, 'epoch': 1.21}
{'padding_count': 2, 'loss': 3.1099, 'learning_rate': 6.116427432216906e-06, 'epoch': 1.23}
{'padding_count': 2, 'loss': 3.0382, 'learning_rate': 6.196172248803828e-06, 'epoch': 1.24}
{'padding_count': 2, 'loss': 2.8574, 'learning_rate': 6.27591706539075e-06, 'epoch': 1.26}
{'padding_count': 2, 'loss': 3.181, 'learning_rate': 6.355661881977672e-06, 'epoch': 1.28}
{'padding_count': 2, 'loss': 3.0928, 'learning_rate': 6.435406698564594e-06, 'epoch': 1.29}
{'padding_count': 2, 'loss': 3.2133, 'learning_rate': 6.515151515151516e-06, 'epoch': 1.31}
{'padding_count': 2, 'loss': 3.1104, 'learning_rate': 6.594896331738437e-06, 'epoch': 1.32}
{'padding_count': 2, 'loss': 3.359, 'learning_rate': 6.674641148325359e-06, 'epoch': 1.34}
{'padding_count': 2, 'loss': 3.0459, 'learning_rate': 6.754385964912281e-06, 'epoch': 1.36}
{'padding_count': 2, 'loss': 3.0185, 'learning_rate': 6.834130781499203e-06, 'epoch': 1.37}
{'padding_count': 2, 'loss': 3.0464, 'learning_rate': 6.913875598086125e-06, 'epoch': 1.39}
{'padding_count': 2, 'loss': 3.1048, 'learning_rate': 6.993620414673047e-06, 'epoch': 1.4}
{'padding_count': 2, 'loss': 3.1857, 'learning_rate': 7.0733652312599685e-06, 'epoch': 1.42}
{'padding_count': 2, 'loss': 2.9387, 'learning_rate': 7.1531100478468905e-06, 'epoch': 1.44}
{'padding_count': 2, 'loss': 3.1325, 'learning_rate': 7.2328548644338126e-06, 'epoch': 1.45}
{'padding_count': 2, 'loss': 3.1494, 'learning_rate': 7.312599681020735e-06, 'epoch': 1.47}
{'padding_count': 2, 'loss': 3.2406, 'learning_rate': 7.392344497607657e-06, 'epoch': 1.48}
{'padding_count': 2, 'loss': 2.9297, 'learning_rate': 7.472089314194579e-06, 'epoch': 1.5}
{'padding_count': 2, 'loss': 3.0794, 'learning_rate': 7.5518341307815e-06, 'epoch': 1.52}
{'padding_count': 2, 'loss': 3.1782, 'learning_rate': 7.631578947368423e-06, 'epoch': 1.53}
{'padding_count': 2, 'loss': 3.0501, 'learning_rate': 7.711323763955344e-06, 'epoch': 1.55}
{'padding_count': 2, 'loss': 3.1654, 'learning_rate': 7.791068580542265e-06, 'epoch': 1.56}
{'padding_count': 2, 'loss': 3.1794, 'learning_rate': 7.870813397129188e-06, 'epoch': 1.58}
{'padding_count': 2, 'loss': 3.146, 'learning_rate': 7.95055821371611e-06, 'epoch': 1.59}
{'padding_count': 2, 'loss': 3.1872, 'learning_rate': 8.03030303030303e-06, 'epoch': 1.61}
{'padding_count': 2, 'loss': 3.0813, 'learning_rate': 8.110047846889952e-06, 'epoch': 1.63}
{'padding_count': 2, 'loss': 3.0808, 'learning_rate': 8.189792663476875e-06, 'epoch': 1.64}
{'padding_count': 2, 'loss': 2.9811, 'learning_rate': 8.269537480063796e-06, 'epoch': 1.66}
{'padding_count': 2, 'loss': 2.8336, 'learning_rate': 8.349282296650719e-06, 'epoch': 1.67}
{'padding_count': 2, 'loss': 2.9916, 'learning_rate': 8.42902711323764e-06, 'epoch': 1.69}
{'padding_count': 3, 'loss': 3.1556, 'learning_rate': 8.508771929824563e-06, 'epoch': 1.71}
{'padding_count': 3, 'loss': 3.1295, 'learning_rate': 8.588516746411484e-06, 'epoch': 1.72}
{'padding_count': 3, 'loss': 3.1525, 'learning_rate': 8.668261562998405e-06, 'epoch': 1.74}
{'padding_count': 3, 'loss': 2.9785, 'learning_rate': 8.748006379585328e-06, 'epoch': 1.75}
{'padding_count': 3, 'loss': 3.0309, 'learning_rate': 8.82775119617225e-06, 'epoch': 1.77}
{'padding_count': 3, 'loss': 3.2293, 'learning_rate': 8.907496012759172e-06, 'epoch': 1.79}
{'padding_count': 3, 'loss': 2.785, 'learning_rate': 8.987240829346093e-06, 'epoch': 1.8}
{'padding_count': 3, 'loss': 3.0051, 'learning_rate': 9.066985645933015e-06, 'epoch': 1.82}
{'padding_count': 3, 'loss': 2.9499, 'learning_rate': 9.146730462519937e-06, 'epoch': 1.83}
{'padding_count': 3, 'loss': 3.1225, 'learning_rate': 9.226475279106859e-06, 'epoch': 1.85}
{'padding_count': 3, 'loss': 3.3589, 'learning_rate': 9.306220095693781e-06, 'epoch': 1.87}
{'padding_count': 3, 'loss': 3.0788, 'learning_rate': 9.385964912280703e-06, 'epoch': 1.88}
{'padding_count': 3, 'loss': 3.1323, 'learning_rate': 9.465709728867626e-06, 'epoch': 1.9}
{'padding_count': 4, 'loss': 2.7883, 'learning_rate': 9.545454545454547e-06, 'epoch': 1.91}
{'padding_count': 4, 'loss': 3.0342, 'learning_rate': 9.625199362041468e-06, 'epoch': 1.93}
{'padding_count': 4, 'loss': 2.9505, 'learning_rate': 9.704944178628391e-06, 'epoch': 1.95}
{'padding_count': 4, 'loss': 3.1773, 'learning_rate': 9.784688995215312e-06, 'epoch': 1.96}
{'padding_count': 4, 'loss': 3.2802, 'learning_rate': 9.864433811802233e-06, 'epoch': 1.98}
{'padding_count': 4, 'loss': 3.2776, 'learning_rate': 9.944178628389154e-06, 'epoch': 1.99}
{'eval_loss': 3.1453683376312256, 'eval_runtime': 43.4777, 'eval_samples_per_second': 6.256, 'eval_steps_per_second': 1.564, 'epoch': 2.0}
{'padding_count': 4, 'loss': 3.0547, 'learning_rate': 9.997341839447104e-06, 'epoch': 2.01}
{'padding_count': 4, 'loss': 2.9261, 'learning_rate': 9.988481304270778e-06, 'epoch': 2.03}
{'padding_count': 4, 'loss': 3.1186, 'learning_rate': 9.979620769094454e-06, 'epoch': 2.04}
{'padding_count': 4, 'loss': 3.2882, 'learning_rate': 9.97076023391813e-06, 'epoch': 2.06}
{'padding_count': 4, 'loss': 3.0459, 'learning_rate': 9.961899698741804e-06, 'epoch': 2.07}
{'padding_count': 4, 'loss': 2.789, 'learning_rate': 9.95303916356548e-06, 'epoch': 2.09}
{'padding_count': 4, 'loss': 3.2032, 'learning_rate': 9.944178628389154e-06, 'epoch': 2.11}
{'padding_count': 4, 'loss': 3.1384, 'learning_rate': 9.93531809321283e-06, 'epoch': 2.12}
{'padding_count': 4, 'loss': 3.0647, 'learning_rate': 9.926457558036506e-06, 'epoch': 2.14}
{'padding_count': 4, 'loss': 2.8675, 'learning_rate': 9.91759702286018e-06, 'epoch': 2.15}
{'padding_count': 4, 'loss': 3.3723, 'learning_rate': 9.908736487683857e-06, 'epoch': 2.17}
{'padding_count': 4, 'loss': 3.2577, 'learning_rate': 9.899875952507533e-06, 'epoch': 2.19}
{'padding_count': 4, 'loss': 3.0466, 'learning_rate': 9.891015417331209e-06, 'epoch': 2.2}
{'padding_count': 4, 'loss': 3.0219, 'learning_rate': 9.882154882154883e-06, 'epoch': 2.22}
{'padding_count': 4, 'loss': 2.8105, 'learning_rate': 9.873294346978557e-06, 'epoch': 2.23}
{'padding_count': 4, 'loss': 2.8014, 'learning_rate': 9.864433811802233e-06, 'epoch': 2.25}
{'padding_count': 4, 'loss': 2.7767, 'learning_rate': 9.85557327662591e-06, 'epoch': 2.26}
{'padding_count': 4, 'loss': 2.7968, 'learning_rate': 9.846712741449585e-06, 'epoch': 2.28}
{'padding_count': 4, 'loss': 2.9623, 'learning_rate': 9.83785220627326e-06, 'epoch': 2.3}
{'padding_count': 4, 'loss': 2.9437, 'learning_rate': 9.828991671096934e-06, 'epoch': 2.31}
{'padding_count': 4, 'loss': 3.01, 'learning_rate': 9.820131135920611e-06, 'epoch': 2.33}
{'padding_count': 4, 'loss': 2.9918, 'learning_rate': 9.811270600744286e-06, 'epoch': 2.34}
{'padding_count': 4, 'loss': 2.8084, 'learning_rate': 9.802410065567962e-06, 'epoch': 2.36}
{'padding_count': 4, 'loss': 2.8235, 'learning_rate': 9.793549530391636e-06, 'epoch': 2.38}
{'padding_count': 4, 'loss': 2.8955, 'learning_rate': 9.784688995215312e-06, 'epoch': 2.39}
{'padding_count': 4, 'loss': 2.6851, 'learning_rate': 9.775828460038988e-06, 'epoch': 2.41}
{'padding_count': 4, 'loss': 3.0943, 'learning_rate': 9.766967924862662e-06, 'epoch': 2.42}
{'padding_count': 4, 'loss': 2.6628, 'learning_rate': 9.758107389686338e-06, 'epoch': 2.44}
{'padding_count': 4, 'loss': 2.9137, 'learning_rate': 9.749246854510013e-06, 'epoch': 2.46}
{'padding_count': 4, 'loss': 2.9893, 'learning_rate': 9.740386319333689e-06, 'epoch': 2.47}
{'padding_count': 4, 'loss': 2.919, 'learning_rate': 9.731525784157365e-06, 'epoch': 2.49}
{'padding_count': 4, 'loss': 2.6497, 'learning_rate': 9.722665248981039e-06, 'epoch': 2.5}
{'padding_count': 4, 'loss': 3.0804, 'learning_rate': 9.713804713804715e-06, 'epoch': 2.52}
{'padding_count': 4, 'loss': 2.6765, 'learning_rate': 9.704944178628391e-06, 'epoch': 2.54}
{'padding_count': 4, 'loss': 2.8987, 'learning_rate': 9.696083643452065e-06, 'epoch': 2.55}
{'padding_count': 4, 'loss': 2.9745, 'learning_rate': 9.687223108275741e-06, 'epoch': 2.57}
{'padding_count': 4, 'loss': 3.0084, 'learning_rate': 9.678362573099415e-06, 'epoch': 2.58}
{'padding_count': 4, 'loss': 2.9363, 'learning_rate': 9.669502037923091e-06, 'epoch': 2.6}
{'padding_count': 4, 'loss': 2.8958, 'learning_rate': 9.660641502746767e-06, 'epoch': 2.62}
{'padding_count': 4, 'loss': 2.7895, 'learning_rate': 9.651780967570442e-06, 'epoch': 2.63}
{'padding_count': 4, 'loss': 2.8307, 'learning_rate': 9.642920432394118e-06, 'epoch': 2.65}
{'padding_count': 4, 'loss': 2.904, 'learning_rate': 9.634059897217792e-06, 'epoch': 2.66}
{'padding_count': 4, 'loss': 2.4512, 'learning_rate': 9.625199362041468e-06, 'epoch': 2.68}
{'padding_count': 5, 'loss': 2.7895, 'learning_rate': 9.616338826865144e-06, 'epoch': 2.7}
{'padding_count': 5, 'loss': 2.8064, 'learning_rate': 9.607478291688818e-06, 'epoch': 2.71}
{'padding_count': 5, 'loss': 2.9878, 'learning_rate': 9.598617756512494e-06, 'epoch': 2.73}
{'padding_count': 5, 'loss': 2.7961, 'learning_rate': 9.58975722133617e-06, 'epoch': 2.74}
{'padding_count': 5, 'loss': 2.7732, 'learning_rate': 9.580896686159845e-06, 'epoch': 2.76}
{'padding_count': 5, 'loss': 2.8952, 'learning_rate': 9.57203615098352e-06, 'epoch': 2.78}
{'padding_count': 5, 'loss': 2.8263, 'learning_rate': 9.563175615807195e-06, 'epoch': 2.79}
{'padding_count': 5, 'loss': 2.7458, 'learning_rate': 9.55431508063087e-06, 'epoch': 2.81}
{'padding_count': 5, 'loss': 2.6765, 'learning_rate': 9.545454545454547e-06, 'epoch': 2.82}
{'padding_count': 5, 'loss': 2.7816, 'learning_rate': 9.536594010278221e-06, 'epoch': 2.84}
{'padding_count': 5, 'loss': 3.035, 'learning_rate': 9.527733475101897e-06, 'epoch': 2.85}
{'padding_count': 5, 'loss': 3.009, 'learning_rate': 9.518872939925571e-06, 'epoch': 2.87}
{'padding_count': 5, 'loss': 2.6873, 'learning_rate': 9.510012404749247e-06, 'epoch': 2.89}
{'padding_count': 6, 'loss': 2.8665, 'learning_rate': 9.501151869572923e-06, 'epoch': 2.9}
{'padding_count': 6, 'loss': 2.7482, 'learning_rate': 9.492291334396598e-06, 'epoch': 2.92}
{'padding_count': 6, 'loss': 2.6734, 'learning_rate': 9.483430799220274e-06, 'epoch': 2.93}
{'padding_count': 6, 'loss': 2.7287, 'learning_rate': 9.474570264043948e-06, 'epoch': 2.95}
{'padding_count': 6, 'loss': 2.9665, 'learning_rate': 9.465709728867626e-06, 'epoch': 2.97}
{'padding_count': 6, 'loss': 3.0001, 'learning_rate': 9.4568491936913e-06, 'epoch': 2.98}
{'padding_count': 6, 'loss': 3.103, 'learning_rate': 9.447988658514974e-06, 'epoch': 3.0}
{'eval_loss': 3.094468593597412, 'eval_runtime': 43.4849, 'eval_samples_per_second': 6.255, 'eval_steps_per_second': 1.564, 'epoch': 3.0}
{'padding_count': 6, 'loss': 2.7234, 'learning_rate': 9.43912812333865e-06, 'epoch': 3.01}
{'padding_count': 6, 'loss': 2.7968, 'learning_rate': 9.430267588162326e-06, 'epoch': 3.03}
{'padding_count': 6, 'loss': 2.8608, 'learning_rate': 9.421407052986002e-06, 'epoch': 3.05}
{'padding_count': 6, 'loss': 3.0855, 'learning_rate': 9.412546517809676e-06, 'epoch': 3.06}
{'padding_count': 6, 'loss': 2.5766, 'learning_rate': 9.40368598263335e-06, 'epoch': 3.08}
{'padding_count': 6, 'loss': 2.5305, 'learning_rate': 9.394825447457027e-06, 'epoch': 3.09}
{'padding_count': 6, 'loss': 3.0778, 'learning_rate': 9.385964912280703e-06, 'epoch': 3.11}
{'padding_count': 6, 'loss': 2.7506, 'learning_rate': 9.377104377104379e-06, 'epoch': 3.13}
{'padding_count': 6, 'loss': 2.8501, 'learning_rate': 9.368243841928053e-06, 'epoch': 3.14}
{'padding_count': 6, 'loss': 2.5699, 'learning_rate': 9.359383306751729e-06, 'epoch': 3.16}
{'padding_count': 6, 'loss': 3.2996, 'learning_rate': 9.350522771575405e-06, 'epoch': 3.17}
{'padding_count': 6, 'loss': 2.9577, 'learning_rate': 9.34166223639908e-06, 'epoch': 3.19}
{'padding_count': 6, 'loss': 2.692, 'learning_rate': 9.332801701222755e-06, 'epoch': 3.21}
{'padding_count': 6, 'loss': 2.6366, 'learning_rate': 9.32394116604643e-06, 'epoch': 3.22}
{'padding_count': 6, 'loss': 2.7643, 'learning_rate': 9.315080630870106e-06, 'epoch': 3.24}
{'padding_count': 6, 'loss': 2.494, 'learning_rate': 9.306220095693781e-06, 'epoch': 3.25}
{'padding_count': 6, 'loss': 2.5975, 'learning_rate': 9.297359560517456e-06, 'epoch': 3.27}
{'padding_count': 6, 'loss': 2.5761, 'learning_rate': 9.288499025341132e-06, 'epoch': 3.29}
{'padding_count': 6, 'loss': 2.8127, 'learning_rate': 9.279638490164806e-06, 'epoch': 3.3}
{'padding_count': 6, 'loss': 2.5746, 'learning_rate': 9.270777954988482e-06, 'epoch': 3.32}
{'padding_count': 6, 'loss': 2.809, 'learning_rate': 9.261917419812158e-06, 'epoch': 3.33}
{'padding_count': 6, 'loss': 2.7677, 'learning_rate': 9.253056884635832e-06, 'epoch': 3.35}
{'padding_count': 6, 'loss': 2.5145, 'learning_rate': 9.244196349459508e-06, 'epoch': 3.37}
{'padding_count': 6, 'loss': 2.4993, 'learning_rate': 9.235335814283184e-06, 'epoch': 3.38}
{'padding_count': 6, 'loss': 2.7928, 'learning_rate': 9.226475279106859e-06, 'epoch': 3.4}
{'padding_count': 6, 'loss': 2.5296, 'learning_rate': 9.217614743930535e-06, 'epoch': 3.41}
{'padding_count': 6, 'loss': 2.5877, 'learning_rate': 9.208754208754209e-06, 'epoch': 3.43}
{'padding_count': 6, 'loss': 2.4868, 'learning_rate': 9.199893673577885e-06, 'epoch': 3.44}
{'padding_count': 6, 'loss': 2.739, 'learning_rate': 9.191033138401561e-06, 'epoch': 3.46}
{'padding_count': 6, 'loss': 2.7036, 'learning_rate': 9.182172603225235e-06, 'epoch': 3.48}
{'padding_count': 6, 'loss': 2.5756, 'learning_rate': 9.173312068048911e-06, 'epoch': 3.49}
{'padding_count': 6, 'loss': 2.5508, 'learning_rate': 9.164451532872585e-06, 'epoch': 3.51}
{'padding_count': 6, 'loss': 2.8864, 'learning_rate': 9.155590997696261e-06, 'epoch': 3.52}
{'padding_count': 6, 'loss': 2.4033, 'learning_rate': 9.146730462519937e-06, 'epoch': 3.54}
{'padding_count': 6, 'loss': 2.6935, 'learning_rate': 9.137869927343612e-06, 'epoch': 3.56}
{'padding_count': 6, 'loss': 2.8027, 'learning_rate': 9.129009392167288e-06, 'epoch': 3.57}
{'padding_count': 6, 'loss': 2.6379, 'learning_rate': 9.120148856990964e-06, 'epoch': 3.59}
{'padding_count': 6, 'loss': 2.9541, 'learning_rate': 9.111288321814638e-06, 'epoch': 3.6}
{'padding_count': 6, 'loss': 2.6955, 'learning_rate': 9.102427786638314e-06, 'epoch': 3.62}
{'padding_count': 6, 'loss': 2.4875, 'learning_rate': 9.093567251461988e-06, 'epoch': 3.64}
{'padding_count': 6, 'loss': 2.5351, 'learning_rate': 9.084706716285664e-06, 'epoch': 3.65}
{'padding_count': 6, 'loss': 2.6086, 'learning_rate': 9.07584618110934e-06, 'epoch': 3.67}
{'padding_count': 6, 'loss': 2.299, 'learning_rate': 9.066985645933015e-06, 'epoch': 3.68}
{'padding_count': 7, 'loss': 2.6037, 'learning_rate': 9.05812511075669e-06, 'epoch': 3.7}
{'padding_count': 7, 'loss': 2.628, 'learning_rate': 9.049264575580365e-06, 'epoch': 3.72}
{'padding_count': 7, 'loss': 2.7571, 'learning_rate': 9.040404040404042e-06, 'epoch': 3.73}
{'padding_count': 7, 'loss': 2.6271, 'learning_rate': 9.031543505227717e-06, 'epoch': 3.75}
{'padding_count': 7, 'loss': 2.7119, 'learning_rate': 9.022682970051391e-06, 'epoch': 3.76}
{'padding_count': 7, 'loss': 2.7398, 'learning_rate': 9.013822434875067e-06, 'epoch': 3.78}
{'padding_count': 7, 'loss': 2.4688, 'learning_rate': 9.004961899698741e-06, 'epoch': 3.8}
{'padding_count': 7, 'loss': 2.7459, 'learning_rate': 8.996101364522419e-06, 'epoch': 3.81}
{'padding_count': 7, 'loss': 2.4431, 'learning_rate': 8.987240829346093e-06, 'epoch': 3.83}
{'padding_count': 7, 'loss': 2.6751, 'learning_rate': 8.978380294169768e-06, 'epoch': 3.84}
{'padding_count': 7, 'loss': 2.7825, 'learning_rate': 8.969519758993444e-06, 'epoch': 3.86}
{'padding_count': 7, 'loss': 2.8882, 'learning_rate': 8.96065922381712e-06, 'epoch': 3.88}
{'padding_count': 7, 'loss': 2.5984, 'learning_rate': 8.951798688640796e-06, 'epoch': 3.89}
{'padding_count': 8, 'loss': 2.4746, 'learning_rate': 8.94293815346447e-06, 'epoch': 3.91}
{'padding_count': 8, 'loss': 2.36, 'learning_rate': 8.934077618288146e-06, 'epoch': 3.92}
{'padding_count': 8, 'loss': 2.6284, 'learning_rate': 8.925217083111822e-06, 'epoch': 3.94}
{'padding_count': 8, 'loss': 2.4866, 'learning_rate': 8.916356547935496e-06, 'epoch': 3.96}
{'padding_count': 8, 'loss': 2.7449, 'learning_rate': 8.907496012759172e-06, 'epoch': 3.97}
{'padding_count': 8, 'loss': 2.8125, 'learning_rate': 8.898635477582846e-06, 'epoch': 3.99}
{'eval_loss': 3.067258596420288, 'eval_runtime': 43.6331, 'eval_samples_per_second': 6.234, 'eval_steps_per_second': 1.558, 'epoch': 4.0}
{'padding_count': 8, 'loss': 2.7026, 'learning_rate': 8.889774942406522e-06, 'epoch': 4.0}
{'padding_count': 8, 'loss': 2.6601, 'learning_rate': 8.880914407230198e-06, 'epoch': 4.02}
{'padding_count': 8, 'loss': 2.623, 'learning_rate': 8.872053872053873e-06, 'epoch': 4.04}
{'padding_count': 8, 'loss': 2.8011, 'learning_rate': 8.863193336877549e-06, 'epoch': 4.05}
{'padding_count': 8, 'loss': 2.6494, 'learning_rate': 8.854332801701223e-06, 'epoch': 4.07}
{'padding_count': 8, 'loss': 2.3506, 'learning_rate': 8.845472266524899e-06, 'epoch': 4.08}
{'padding_count': 8, 'loss': 2.4018, 'learning_rate': 8.836611731348575e-06, 'epoch': 4.1}
{'padding_count': 8, 'loss': 2.8893, 'learning_rate': 8.82775119617225e-06, 'epoch': 4.11}
{'padding_count': 8, 'loss': 2.6363, 'learning_rate': 8.818890660995925e-06, 'epoch': 4.13}
{'padding_count': 8, 'loss': 2.5712, 'learning_rate': 8.8100301258196e-06, 'epoch': 4.15}
{'padding_count': 8, 'loss': 2.5366, 'learning_rate': 8.801169590643275e-06, 'epoch': 4.16}
{'padding_count': 8, 'loss': 3.0082, 'learning_rate': 8.792309055466951e-06, 'epoch': 4.18}
{'padding_count': 8, 'loss': 2.5585, 'learning_rate': 8.783448520290626e-06, 'epoch': 4.19}
{'padding_count': 8, 'loss': 2.6332, 'learning_rate': 8.774587985114302e-06, 'epoch': 4.21}
{'padding_count': 8, 'loss': 2.5404, 'learning_rate': 8.765727449937978e-06, 'epoch': 4.23}
{'padding_count': 8, 'loss': 2.4196, 'learning_rate': 8.756866914761652e-06, 'epoch': 4.24}
{'padding_count': 8, 'loss': 2.1425, 'learning_rate': 8.748006379585328e-06, 'epoch': 4.26}
{'padding_count': 8, 'loss': 2.674, 'learning_rate': 8.739145844409002e-06, 'epoch': 4.27}
{'padding_count': 8, 'loss': 2.2842, 'learning_rate': 8.730285309232678e-06, 'epoch': 4.29}
{'padding_count': 8, 'loss': 2.6512, 'learning_rate': 8.721424774056354e-06, 'epoch': 4.31}
{'padding_count': 8, 'loss': 2.3862, 'learning_rate': 8.712564238880029e-06, 'epoch': 4.32}
{'padding_count': 8, 'loss': 2.7141, 'learning_rate': 8.703703703703705e-06, 'epoch': 4.34}
{'padding_count': 8, 'loss': 2.4626, 'learning_rate': 8.694843168527379e-06, 'epoch': 4.35}
{'padding_count': 8, 'loss': 2.5013, 'learning_rate': 8.685982633351055e-06, 'epoch': 4.37}
{'padding_count': 8, 'loss': 2.3019, 'learning_rate': 8.67712209817473e-06, 'epoch': 4.39}
{'padding_count': 8, 'loss': 2.4917, 'learning_rate': 8.668261562998405e-06, 'epoch': 4.4}
{'padding_count': 8, 'loss': 2.354, 'learning_rate': 8.659401027822081e-06, 'epoch': 4.42}
{'padding_count': 8, 'loss': 2.3053, 'learning_rate': 8.650540492645757e-06, 'epoch': 4.43}
{'padding_count': 8, 'loss': 2.4201, 'learning_rate': 8.641679957469431e-06, 'epoch': 4.45}
{'padding_count': 8, 'loss': 2.6231, 'learning_rate': 8.632819422293107e-06, 'epoch': 4.47}
{'padding_count': 8, 'loss': 2.3869, 'learning_rate': 8.623958887116782e-06, 'epoch': 4.48}
{'padding_count': 8, 'loss': 2.4028, 'learning_rate': 8.615098351940458e-06, 'epoch': 4.5}
{'padding_count': 8, 'loss': 2.3778, 'learning_rate': 8.606237816764134e-06, 'epoch': 4.51}
{'padding_count': 8, 'loss': 2.6072, 'learning_rate': 8.597377281587808e-06, 'epoch': 4.53}
{'padding_count': 8, 'loss': 2.3476, 'learning_rate': 8.588516746411484e-06, 'epoch': 4.55}
{'padding_count': 8, 'loss': 2.5467, 'learning_rate': 8.579656211235158e-06, 'epoch': 4.56}
{'padding_count': 8, 'loss': 2.4701, 'learning_rate': 8.570795676058836e-06, 'epoch': 4.58}
{'padding_count': 8, 'loss': 2.7097, 'learning_rate': 8.56193514088251e-06, 'epoch': 4.59}
{'padding_count': 8, 'loss': 2.5531, 'learning_rate': 8.553074605706184e-06, 'epoch': 4.61}
{'padding_count': 8, 'loss': 2.4658, 'learning_rate': 8.54421407052986e-06, 'epoch': 4.63}
{'padding_count': 8, 'loss': 2.395, 'learning_rate': 8.535353535353535e-06, 'epoch': 4.64}
{'padding_count': 8, 'loss': 2.3285, 'learning_rate': 8.526493000177212e-06, 'epoch': 4.66}
{'padding_count': 8, 'loss': 2.183, 'learning_rate': 8.517632465000887e-06, 'epoch': 4.67}
{'padding_count': 8, 'loss': 2.3171, 'learning_rate': 8.508771929824563e-06, 'epoch': 4.69}
{'padding_count': 9, 'loss': 2.3742, 'learning_rate': 8.499911394648237e-06, 'epoch': 4.7}
{'padding_count': 9, 'loss': 2.5284, 'learning_rate': 8.491050859471913e-06, 'epoch': 4.72}
{'padding_count': 9, 'loss': 2.6469, 'learning_rate': 8.482190324295589e-06, 'epoch': 4.74}
{'padding_count': 9, 'loss': 2.3025, 'learning_rate': 8.473329789119263e-06, 'epoch': 4.75}
{'padding_count': 9, 'loss': 2.5418, 'learning_rate': 8.46446925394294e-06, 'epoch': 4.77}
{'padding_count': 9, 'loss': 2.7037, 'learning_rate': 8.455608718766615e-06, 'epoch': 4.78}
{'padding_count': 9, 'loss': 2.1414, 'learning_rate': 8.44674818359029e-06, 'epoch': 4.8}
{'padding_count': 9, 'loss': 2.5172, 'learning_rate': 8.437887648413966e-06, 'epoch': 4.82}
{'padding_count': 9, 'loss': 2.382, 'learning_rate': 8.42902711323764e-06, 'epoch': 4.83}
{'padding_count': 9, 'loss': 2.466, 'learning_rate': 8.420166578061316e-06, 'epoch': 4.85}
{'padding_count': 9, 'loss': 2.6573, 'learning_rate': 8.411306042884992e-06, 'epoch': 4.86}
{'padding_count': 9, 'loss': 2.6678, 'learning_rate': 8.402445507708666e-06, 'epoch': 4.88}
{'padding_count': 9, 'loss': 2.4339, 'learning_rate': 8.393584972532342e-06, 'epoch': 4.9}
{'padding_count': 10, 'loss': 2.2432, 'learning_rate': 8.384724437356016e-06, 'epoch': 4.91}
{'padding_count': 10, 'loss': 2.2961, 'learning_rate': 8.375863902179692e-06, 'epoch': 4.93}
{'padding_count': 10, 'loss': 2.4523, 'learning_rate': 8.367003367003368e-06, 'epoch': 4.94}
{'padding_count': 10, 'loss': 2.4495, 'learning_rate': 8.358142831827043e-06, 'epoch': 4.96}
{'padding_count': 10, 'loss': 2.5812, 'learning_rate': 8.349282296650719e-06, 'epoch': 4.98}
{'padding_count': 10, 'loss': 2.7551, 'learning_rate': 8.340421761474393e-06, 'epoch': 4.99}
{'eval_loss': 3.0876052379608154, 'eval_runtime': 43.4626, 'eval_samples_per_second': 6.258, 'eval_steps_per_second': 1.565, 'epoch': 5.0}
{'padding_count': 10, 'loss': 2.4659, 'learning_rate': 8.331561226298069e-06, 'epoch': 5.01}
{'padding_count': 10, 'loss': 2.3929, 'learning_rate': 8.322700691121745e-06, 'epoch': 5.02}
{'padding_count': 10, 'loss': 2.5376, 'learning_rate': 8.31384015594542e-06, 'epoch': 5.04}
{'padding_count': 10, 'loss': 2.6965, 'learning_rate': 8.304979620769095e-06, 'epoch': 5.06}
{'padding_count': 10, 'loss': 2.4713, 'learning_rate': 8.296119085592771e-06, 'epoch': 5.07}
{'padding_count': 10, 'loss': 2.0595, 'learning_rate': 8.287258550416445e-06, 'epoch': 5.09}
{'padding_count': 10, 'loss': 2.4281, 'learning_rate': 8.278398015240121e-06, 'epoch': 5.1}
{'padding_count': 10, 'loss': 2.6362, 'learning_rate': 8.269537480063796e-06, 'epoch': 5.12}
{'padding_count': 10, 'loss': 2.4859, 'learning_rate': 8.260676944887472e-06, 'epoch': 5.14}
{'padding_count': 10, 'loss': 2.1971, 'learning_rate': 8.251816409711148e-06, 'epoch': 5.15}
{'padding_count': 10, 'loss': 2.7381, 'learning_rate': 8.242955874534822e-06, 'epoch': 5.17}
{'padding_count': 10, 'loss': 2.699, 'learning_rate': 8.234095339358498e-06, 'epoch': 5.18}
{'padding_count': 10, 'loss': 2.4094, 'learning_rate': 8.225234804182172e-06, 'epoch': 5.2}
{'padding_count': 10, 'loss': 2.4374, 'learning_rate': 8.216374269005848e-06, 'epoch': 5.22}
{'padding_count': 10, 'loss': 2.2727, 'learning_rate': 8.207513733829524e-06, 'epoch': 5.23}
{'padding_count': 10, 'loss': 2.306, 'learning_rate': 8.198653198653199e-06, 'epoch': 5.25}
{'padding_count': 10, 'loss': 1.9991, 'learning_rate': 8.189792663476875e-06, 'epoch': 5.26}
{'padding_count': 10, 'loss': 2.3335, 'learning_rate': 8.18093212830055e-06, 'epoch': 5.28}
{'padding_count': 10, 'loss': 2.2722, 'learning_rate': 8.172071593124225e-06, 'epoch': 5.3}
{'padding_count': 10, 'loss': 2.4332, 'learning_rate': 8.1632110579479e-06, 'epoch': 5.31}
{'padding_count': 10, 'loss': 2.3249, 'learning_rate': 8.154350522771575e-06, 'epoch': 5.33}
{'padding_count': 10, 'loss': 2.3327, 'learning_rate': 8.145489987595251e-06, 'epoch': 5.34}
{'padding_count': 10, 'loss': 2.363, 'learning_rate': 8.136629452418927e-06, 'epoch': 5.36}
{'padding_count': 10, 'loss': 2.1922, 'learning_rate': 8.127768917242601e-06, 'epoch': 5.37}
{'padding_count': 10, 'loss': 2.2448, 'learning_rate': 8.118908382066277e-06, 'epoch': 5.39}
{'padding_count': 10, 'loss': 2.2189, 'learning_rate': 8.110047846889952e-06, 'epoch': 5.41}
{'padding_count': 10, 'loss': 2.2704, 'learning_rate': 8.10118731171363e-06, 'epoch': 5.42}
{'padding_count': 10, 'loss': 2.1117, 'learning_rate': 8.092326776537304e-06, 'epoch': 5.44}
{'padding_count': 10, 'loss': 2.2762, 'learning_rate': 8.08346624136098e-06, 'epoch': 5.45}
{'padding_count': 10, 'loss': 2.3933, 'learning_rate': 8.074605706184654e-06, 'epoch': 5.47}
{'padding_count': 10, 'loss': 2.3332, 'learning_rate': 8.06574517100833e-06, 'epoch': 5.49}
{'padding_count': 10, 'loss': 2.1558, 'learning_rate': 8.056884635832006e-06, 'epoch': 5.5}
{'padding_count': 10, 'loss': 2.4565, 'learning_rate': 8.04802410065568e-06, 'epoch': 5.52}
{'padding_count': 10, 'loss': 2.1494, 'learning_rate': 8.039163565479356e-06, 'epoch': 5.53}
{'padding_count': 10, 'loss': 2.441, 'learning_rate': 8.03030303030303e-06, 'epoch': 5.55}
{'padding_count': 10, 'loss': 2.3985, 'learning_rate': 8.021442495126706e-06, 'epoch': 5.57}
{'padding_count': 10, 'loss': 2.5282, 'learning_rate': 8.012581959950382e-06, 'epoch': 5.58}
{'padding_count': 10, 'loss': 2.443, 'learning_rate': 8.003721424774057e-06, 'epoch': 5.6}
{'padding_count': 10, 'loss': 2.4142, 'learning_rate': 7.994860889597733e-06, 'epoch': 5.61}
{'padding_count': 10, 'loss': 2.0775, 'learning_rate': 7.986000354421409e-06, 'epoch': 5.63}
{'padding_count': 10, 'loss': 2.2279, 'learning_rate': 7.977139819245083e-06, 'epoch': 5.65}
{'padding_count': 10, 'loss': 2.2311, 'learning_rate': 7.968279284068759e-06, 'epoch': 5.66}
{'padding_count': 10, 'loss': 1.9591, 'learning_rate': 7.959418748892433e-06, 'epoch': 5.68}
{'padding_count': 11, 'loss': 2.314, 'learning_rate': 7.95055821371611e-06, 'epoch': 5.69}
{'padding_count': 11, 'loss': 2.2995, 'learning_rate': 7.941697678539785e-06, 'epoch': 5.71}
{'padding_count': 11, 'loss': 2.4159, 'learning_rate': 7.93283714336346e-06, 'epoch': 5.73}
{'padding_count': 11, 'loss': 2.463, 'learning_rate': 7.923976608187136e-06, 'epoch': 5.74}
{'padding_count': 11, 'loss': 2.1873, 'learning_rate': 7.91511607301081e-06, 'epoch': 5.76}
{'padding_count': 11, 'loss': 2.3508, 'learning_rate': 7.906255537834486e-06, 'epoch': 5.77}
{'padding_count': 11, 'loss': 2.3987, 'learning_rate': 7.897395002658162e-06, 'epoch': 5.79}
{'padding_count': 11, 'loss': 2.1836, 'learning_rate': 7.888534467481836e-06, 'epoch': 5.81}
{'padding_count': 11, 'loss': 2.1889, 'learning_rate': 7.879673932305512e-06, 'epoch': 5.82}
{'padding_count': 11, 'loss': 2.3089, 'learning_rate': 7.870813397129188e-06, 'epoch': 5.84}
{'padding_count': 11, 'loss': 2.4719, 'learning_rate': 7.861952861952862e-06, 'epoch': 5.85}
{'padding_count': 11, 'loss': 2.4942, 'learning_rate': 7.853092326776538e-06, 'epoch': 5.87}
{'padding_count': 11, 'loss': 2.221, 'learning_rate': 7.844231791600213e-06, 'epoch': 5.89}
{'padding_count': 12, 'loss': 2.1996, 'learning_rate': 7.835371256423889e-06, 'epoch': 5.9}
{'padding_count': 12, 'loss': 2.2307, 'learning_rate': 7.826510721247565e-06, 'epoch': 5.92}
{'padding_count': 12, 'loss': 2.1392, 'learning_rate': 7.817650186071239e-06, 'epoch': 5.93}
{'padding_count': 12, 'loss': 2.2583, 'learning_rate': 7.808789650894915e-06, 'epoch': 5.95}
{'padding_count': 12, 'loss': 2.3707, 'learning_rate': 7.79992911571859e-06, 'epoch': 5.96}
{'padding_count': 12, 'loss': 2.4534, 'learning_rate': 7.791068580542265e-06, 'epoch': 5.98}
{'padding_count': 12, 'loss': 2.4827, 'learning_rate': 7.782208045365941e-06, 'epoch': 6.0}
{'eval_loss': 3.122800827026367, 'eval_runtime': 43.9028, 'eval_samples_per_second': 6.195, 'eval_steps_per_second': 1.549, 'epoch': 6.0}
{'padding_count': 12, 'loss': 2.2636, 'learning_rate': 7.773347510189615e-06, 'epoch': 6.01}
{'padding_count': 12, 'loss': 2.2819, 'learning_rate': 7.764486975013291e-06, 'epoch': 6.03}
{'padding_count': 12, 'loss': 2.4669, 'learning_rate': 7.755626439836966e-06, 'epoch': 6.04}
{'padding_count': 12, 'loss': 2.6402, 'learning_rate': 7.746765904660642e-06, 'epoch': 6.06}
{'padding_count': 12, 'loss': 2.0729, 'learning_rate': 7.737905369484318e-06, 'epoch': 6.08}
{'padding_count': 12, 'loss': 1.8563, 'learning_rate': 7.729044834307992e-06, 'epoch': 6.09}
{'padding_count': 12, 'loss': 2.5867, 'learning_rate': 7.720184299131668e-06, 'epoch': 6.11}
{'padding_count': 12, 'loss': 2.2808, 'learning_rate': 7.711323763955344e-06, 'epoch': 6.12}
{'padding_count': 12, 'loss': 2.3468, 'learning_rate': 7.702463228779018e-06, 'epoch': 6.14}
{'padding_count': 12, 'loss': 1.9596, 'learning_rate': 7.693602693602694e-06, 'epoch': 6.16}
{'padding_count': 12, 'loss': 2.68, 'learning_rate': 7.684742158426369e-06, 'epoch': 6.17}
{'padding_count': 12, 'loss': 2.5929, 'learning_rate': 7.675881623250045e-06, 'epoch': 6.19}
{'padding_count': 12, 'loss': 2.2, 'learning_rate': 7.66702108807372e-06, 'epoch': 6.2}
{'padding_count': 12, 'loss': 2.0753, 'learning_rate': 7.658160552897397e-06, 'epoch': 6.22}
{'padding_count': 12, 'loss': 2.307, 'learning_rate': 7.64930001772107e-06, 'epoch': 6.24}
{'padding_count': 12, 'loss': 1.9152, 'learning_rate': 7.640439482544745e-06, 'epoch': 6.25}
{'padding_count': 12, 'loss': 2.0065, 'learning_rate': 7.631578947368423e-06, 'epoch': 6.27}
{'padding_count': 12, 'loss': 1.9906, 'learning_rate': 7.622718412192097e-06, 'epoch': 6.28}
{'padding_count': 12, 'loss': 2.333, 'learning_rate': 7.613857877015772e-06, 'epoch': 6.3}
{'padding_count': 12, 'loss': 2.1844, 'learning_rate': 7.604997341839447e-06, 'epoch': 6.32}
{'padding_count': 12, 'loss': 2.2051, 'learning_rate': 7.596136806663123e-06, 'epoch': 6.33}
{'padding_count': 12, 'loss': 2.1725, 'learning_rate': 7.5872762714867985e-06, 'epoch': 6.35}
{'padding_count': 12, 'loss': 2.1328, 'learning_rate': 7.578415736310474e-06, 'epoch': 6.36}
{'padding_count': 12, 'loss': 2.1596, 'learning_rate': 7.569555201134149e-06, 'epoch': 6.38}
{'padding_count': 12, 'loss': 2.1003, 'learning_rate': 7.560694665957824e-06, 'epoch': 6.4}
{'padding_count': 12, 'loss': 2.0817, 'learning_rate': 7.5518341307815e-06, 'epoch': 6.41}
{'padding_count': 12, 'loss': 1.9965, 'learning_rate': 7.542973595605175e-06, 'epoch': 6.43}
{'padding_count': 12, 'loss': 2.084, 'learning_rate': 7.53411306042885e-06, 'epoch': 6.44}
{'padding_count': 12, 'loss': 2.2749, 'learning_rate': 7.525252525252525e-06, 'epoch': 6.46}
{'padding_count': 12, 'loss': 2.2702, 'learning_rate': 7.516391990076201e-06, 'epoch': 6.48}
{'padding_count': 12, 'loss': 1.9993, 'learning_rate': 7.508417508417509e-06, 'epoch': 6.49}
{'padding_count': 12, 'loss': 2.0002, 'learning_rate': 7.499556973241184e-06, 'epoch': 6.51}
{'padding_count': 12, 'loss': 2.4365, 'learning_rate': 7.490696438064859e-06, 'epoch': 6.52}
{'padding_count': 12, 'loss': 1.9344, 'learning_rate': 7.481835902888534e-06, 'epoch': 6.54}
{'padding_count': 12, 'loss': 2.275, 'learning_rate': 7.472975367712211e-06, 'epoch': 6.56}
{'padding_count': 12, 'loss': 2.3097, 'learning_rate': 7.464114832535886e-06, 'epoch': 6.57}
{'padding_count': 12, 'loss': 2.2076, 'learning_rate': 7.455254297359561e-06, 'epoch': 6.59}
{'padding_count': 12, 'loss': 2.4603, 'learning_rate': 7.446393762183236e-06, 'epoch': 6.6}
{'padding_count': 12, 'loss': 2.231, 'learning_rate': 7.4375332270069125e-06, 'epoch': 6.62}
{'padding_count': 12, 'loss': 2.0205, 'learning_rate': 7.428672691830588e-06, 'epoch': 6.63}
{'padding_count': 12, 'loss': 2.0982, 'learning_rate': 7.419812156654263e-06, 'epoch': 6.65}
{'padding_count': 12, 'loss': 1.9597, 'learning_rate': 7.410951621477938e-06, 'epoch': 6.67}
{'padding_count': 12, 'loss': 1.9945, 'learning_rate': 7.402091086301614e-06, 'epoch': 6.68}
{'padding_count': 13, 'loss': 2.0285, 'learning_rate': 7.393230551125289e-06, 'epoch': 6.7}
{'padding_count': 13, 'loss': 2.2534, 'learning_rate': 7.384370015948964e-06, 'epoch': 6.71}
{'padding_count': 13, 'loss': 2.2868, 'learning_rate': 7.375509480772639e-06, 'epoch': 6.73}
{'padding_count': 13, 'loss': 2.2192, 'learning_rate': 7.3666489455963145e-06, 'epoch': 6.75}
{'padding_count': 13, 'loss': 2.1358, 'learning_rate': 7.3577884104199905e-06, 'epoch': 6.76}
{'padding_count': 13, 'loss': 2.281, 'learning_rate': 7.348927875243666e-06, 'epoch': 6.78}
{'padding_count': 13, 'loss': 2.1694, 'learning_rate': 7.340067340067341e-06, 'epoch': 6.79}
{'padding_count': 13, 'loss': 2.1734, 'learning_rate': 7.331206804891016e-06, 'epoch': 6.81}
{'padding_count': 13, 'loss': 2.0533, 'learning_rate': 7.322346269714692e-06, 'epoch': 6.83}
{'padding_count': 13, 'loss': 2.0933, 'learning_rate': 7.313485734538367e-06, 'epoch': 6.84}
{'padding_count': 13, 'loss': 2.3978, 'learning_rate': 7.304625199362042e-06, 'epoch': 6.86}
{'padding_count': 13, 'loss': 2.4174, 'learning_rate': 7.295764664185717e-06, 'epoch': 6.87}
{'padding_count': 13, 'loss': 2.1353, 'learning_rate': 7.2869041290093924e-06, 'epoch': 6.89}
{'padding_count': 14, 'loss': 2.1224, 'learning_rate': 7.278043593833068e-06, 'epoch': 6.91}
{'padding_count': 14, 'loss': 1.917, 'learning_rate': 7.2691830586567436e-06, 'epoch': 6.92}
{'padding_count': 14, 'loss': 2.1384, 'learning_rate': 7.260322523480419e-06, 'epoch': 6.94}
{'padding_count': 14, 'loss': 2.1043, 'learning_rate': 7.251461988304094e-06, 'epoch': 6.95}
{'padding_count': 14, 'loss': 2.2941, 'learning_rate': 7.24260145312777e-06, 'epoch': 6.97}
{'padding_count': 14, 'loss': 2.2022, 'learning_rate': 7.233740917951445e-06, 'epoch': 6.99}
{'eval_loss': 3.14852237701416, 'eval_runtime': 43.4684, 'eval_samples_per_second': 6.257, 'eval_steps_per_second': 1.564, 'epoch': 7.0}
{'padding_count': 14, 'loss': 2.2988, 'learning_rate': 7.22488038277512e-06, 'epoch': 7.0}
{'padding_count': 14, 'loss': 2.2719, 'learning_rate': 7.216019847598795e-06, 'epoch': 7.02}
{'padding_count': 14, 'loss': 2.2533, 'learning_rate': 7.20715931242247e-06, 'epoch': 7.03}
{'padding_count': 14, 'loss': 2.32, 'learning_rate': 7.198298777246146e-06, 'epoch': 7.05}
{'padding_count': 14, 'loss': 2.23, 'learning_rate': 7.1894382420698215e-06, 'epoch': 7.07}
{'padding_count': 14, 'loss': 1.9639, 'learning_rate': 7.180577706893497e-06, 'epoch': 7.08}
{'padding_count': 14, 'loss': 1.8235, 'learning_rate': 7.171717171717172e-06, 'epoch': 7.1}
{'padding_count': 14, 'loss': 2.4405, 'learning_rate': 7.162856636540848e-06, 'epoch': 7.11}
{'padding_count': 14, 'loss': 2.2594, 'learning_rate': 7.153996101364523e-06, 'epoch': 7.13}
{'padding_count': 14, 'loss': 2.1398, 'learning_rate': 7.145135566188198e-06, 'epoch': 7.15}
{'padding_count': 14, 'loss': 1.9137, 'learning_rate': 7.136275031011873e-06, 'epoch': 7.16}
{'padding_count': 14, 'loss': 2.6634, 'learning_rate': 7.127414495835549e-06, 'epoch': 7.18}
{'padding_count': 14, 'loss': 2.2737, 'learning_rate': 7.118553960659224e-06, 'epoch': 7.19}
{'padding_count': 14, 'loss': 1.9937, 'learning_rate': 7.1096934254828995e-06, 'epoch': 7.21}
{'padding_count': 14, 'loss': 2.1374, 'learning_rate': 7.100832890306575e-06, 'epoch': 7.22}
{'padding_count': 14, 'loss': 2.1947, 'learning_rate': 7.09197235513025e-06, 'epoch': 7.24}
{'padding_count': 14, 'loss': 1.587, 'learning_rate': 7.083111819953926e-06, 'epoch': 7.26}
{'padding_count': 14, 'loss': 2.1721, 'learning_rate': 7.074251284777601e-06, 'epoch': 7.27}
{'padding_count': 14, 'loss': 1.8763, 'learning_rate': 7.065390749601276e-06, 'epoch': 7.29}
{'padding_count': 14, 'loss': 2.2182, 'learning_rate': 7.056530214424951e-06, 'epoch': 7.3}
{'padding_count': 14, 'loss': 1.9281, 'learning_rate': 7.047669679248628e-06, 'epoch': 7.32}
{'padding_count': 14, 'loss': 2.1508, 'learning_rate': 7.038809144072303e-06, 'epoch': 7.34}
{'padding_count': 14, 'loss': 2.1358, 'learning_rate': 7.029948608895978e-06, 'epoch': 7.35}
{'padding_count': 14, 'loss': 2.041, 'learning_rate': 7.0210880737196526e-06, 'epoch': 7.37}
{'padding_count': 14, 'loss': 1.7939, 'learning_rate': 7.012227538543328e-06, 'epoch': 7.38}
{'padding_count': 14, 'loss': 2.2022, 'learning_rate': 7.0033670033670045e-06, 'epoch': 7.4}
{'padding_count': 14, 'loss': 1.8194, 'learning_rate': 6.99450646819068e-06, 'epoch': 7.42}
{'padding_count': 14, 'loss': 1.6839, 'learning_rate': 6.985645933014355e-06, 'epoch': 7.43}
{'padding_count': 14, 'loss': 2.1448, 'learning_rate': 6.97678539783803e-06, 'epoch': 7.45}
{'padding_count': 14, 'loss': 2.1622, 'learning_rate': 6.967924862661706e-06, 'epoch': 7.46}
{'padding_count': 14, 'loss': 1.8731, 'learning_rate': 6.959064327485381e-06, 'epoch': 7.48}
{'padding_count': 14, 'loss': 1.9572, 'learning_rate': 6.950203792309056e-06, 'epoch': 7.5}
{'padding_count': 14, 'loss': 1.999, 'learning_rate': 6.941343257132731e-06, 'epoch': 7.51}
{'padding_count': 14, 'loss': 2.2239, 'learning_rate': 6.932482721956407e-06, 'epoch': 7.53}
{'padding_count': 14, 'loss': 1.8824, 'learning_rate': 6.9236221867800825e-06, 'epoch': 7.54}
{'padding_count': 14, 'loss': 2.182, 'learning_rate': 6.914761651603758e-06, 'epoch': 7.56}
{'padding_count': 14, 'loss': 2.129, 'learning_rate': 6.905901116427433e-06, 'epoch': 7.58}
{'padding_count': 14, 'loss': 2.2214, 'learning_rate': 6.897040581251108e-06, 'epoch': 7.59}
{'padding_count': 14, 'loss': 2.1704, 'learning_rate': 6.888180046074784e-06, 'epoch': 7.61}
{'padding_count': 14, 'loss': 2.0895, 'learning_rate': 6.879319510898459e-06, 'epoch': 7.62}
{'padding_count': 14, 'loss': 1.9314, 'learning_rate': 6.870458975722134e-06, 'epoch': 7.64}
{'padding_count': 14, 'loss': 1.918, 'learning_rate': 6.861598440545809e-06, 'epoch': 7.66}
{'padding_count': 14, 'loss': 1.7735, 'learning_rate': 6.852737905369485e-06, 'epoch': 7.67}
{'padding_count': 14, 'loss': 2.0035, 'learning_rate': 6.8438773701931605e-06, 'epoch': 7.69}
{'padding_count': 15, 'loss': 1.8523, 'learning_rate': 6.835016835016836e-06, 'epoch': 7.7}
{'padding_count': 15, 'loss': 2.1524, 'learning_rate': 6.826156299840511e-06, 'epoch': 7.72}
{'padding_count': 15, 'loss': 2.2196, 'learning_rate': 6.817295764664186e-06, 'epoch': 7.74}
{'padding_count': 15, 'loss': 1.9094, 'learning_rate': 6.808435229487862e-06, 'epoch': 7.75}
{'padding_count': 15, 'loss': 2.0995, 'learning_rate': 6.799574694311537e-06, 'epoch': 7.77}
{'padding_count': 15, 'loss': 2.3567, 'learning_rate': 6.790714159135212e-06, 'epoch': 7.78}
{'padding_count': 15, 'loss': 1.8213, 'learning_rate': 6.781853623958887e-06, 'epoch': 7.8}
{'padding_count': 15, 'loss': 2.1225, 'learning_rate': 6.772993088782563e-06, 'epoch': 7.81}
{'padding_count': 15, 'loss': 2.0252, 'learning_rate': 6.764132553606238e-06, 'epoch': 7.83}
{'padding_count': 15, 'loss': 2.0538, 'learning_rate': 6.7552720184299136e-06, 'epoch': 7.85}
{'padding_count': 15, 'loss': 2.2601, 'learning_rate': 6.746411483253589e-06, 'epoch': 7.86}
{'padding_count': 15, 'loss': 2.3165, 'learning_rate': 6.737550948077265e-06, 'epoch': 7.88}
{'padding_count': 15, 'loss': 1.917, 'learning_rate': 6.72869041290094e-06, 'epoch': 7.89}
{'padding_count': 16, 'loss': 1.9023, 'learning_rate': 6.719829877724615e-06, 'epoch': 7.91}
{'padding_count': 16, 'loss': 1.9781, 'learning_rate': 6.71096934254829e-06, 'epoch': 7.93}
{'padding_count': 16, 'loss': 1.9296, 'learning_rate': 6.702108807371965e-06, 'epoch': 7.94}
{'padding_count': 16, 'loss': 1.9043, 'learning_rate': 6.693248272195641e-06, 'epoch': 7.96}
{'padding_count': 16, 'loss': 2.1027, 'learning_rate': 6.684387737019316e-06, 'epoch': 7.97}
{'padding_count': 16, 'loss': 2.3061, 'learning_rate': 6.6755272018429915e-06, 'epoch': 7.99}
{'eval_loss': 3.206176280975342, 'eval_runtime': 43.6242, 'eval_samples_per_second': 6.235, 'eval_steps_per_second': 1.559, 'epoch': 8.0}
{'padding_count': 16, 'loss': 2.1061, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.01}
{'padding_count': 16, 'loss': 2.0752, 'learning_rate': 6.657806131490343e-06, 'epoch': 8.02}
{'padding_count': 16, 'loss': 2.1674, 'learning_rate': 6.648945596314018e-06, 'epoch': 8.04}
{'padding_count': 16, 'loss': 2.2155, 'learning_rate': 6.640085061137693e-06, 'epoch': 8.05}
{'padding_count': 16, 'loss': 2.1261, 'learning_rate': 6.631224525961368e-06, 'epoch': 8.07}
{'padding_count': 16, 'loss': 1.6341, 'learning_rate': 6.622363990785043e-06, 'epoch': 8.09}
{'padding_count': 16, 'loss': 1.951, 'learning_rate': 6.61350345560872e-06, 'epoch': 8.1}
{'padding_count': 16, 'loss': 2.3503, 'learning_rate': 6.604642920432394e-06, 'epoch': 8.12}
{'padding_count': 16, 'loss': 2.0847, 'learning_rate': 6.5957823852560695e-06, 'epoch': 8.13}
{'padding_count': 16, 'loss': 1.9706, 'learning_rate': 6.586921850079745e-06, 'epoch': 8.15}
{'padding_count': 16, 'loss': 2.0305, 'learning_rate': 6.5780613149034214e-06, 'epoch': 8.17}
{'padding_count': 16, 'loss': 2.4141, 'learning_rate': 6.569200779727097e-06, 'epoch': 8.18}
{'padding_count': 16, 'loss': 1.9896, 'learning_rate': 6.560340244550772e-06, 'epoch': 8.2}
{'padding_count': 16, 'loss': 2.0598, 'learning_rate': 6.551479709374447e-06, 'epoch': 8.21}
{'padding_count': 16, 'loss': 1.9522, 'learning_rate': 6.542619174198123e-06, 'epoch': 8.23}
{'padding_count': 16, 'loss': 1.9359, 'learning_rate': 6.533758639021798e-06, 'epoch': 8.25}
{'padding_count': 16, 'loss': 1.6069, 'learning_rate': 6.524898103845473e-06, 'epoch': 8.26}
{'padding_count': 16, 'loss': 1.9904, 'learning_rate': 6.516037568669148e-06, 'epoch': 8.28}
{'padding_count': 16, 'loss': 1.771, 'learning_rate': 6.507177033492823e-06, 'epoch': 8.29}
{'padding_count': 16, 'loss': 2.1096, 'learning_rate': 6.498316498316499e-06, 'epoch': 8.31}
{'padding_count': 16, 'loss': 1.998, 'learning_rate': 6.4894559631401745e-06, 'epoch': 8.33}
{'padding_count': 16, 'loss': 1.8854, 'learning_rate': 6.48059542796385e-06, 'epoch': 8.34}
{'padding_count': 16, 'loss': 1.9925, 'learning_rate': 6.471734892787525e-06, 'epoch': 8.36}
{'padding_count': 16, 'loss': 2.0173, 'learning_rate': 6.462874357611201e-06, 'epoch': 8.37}
{'padding_count': 16, 'loss': 1.7046, 'learning_rate': 6.454013822434876e-06, 'epoch': 8.39}
{'padding_count': 16, 'loss': 1.8681, 'learning_rate': 6.445153287258551e-06, 'epoch': 8.41}
{'padding_count': 16, 'loss': 1.8747, 'learning_rate': 6.436292752082226e-06, 'epoch': 8.42}
{'padding_count': 16, 'loss': 1.685, 'learning_rate': 6.427432216905901e-06, 'epoch': 8.44}
{'padding_count': 16, 'loss': 1.9351, 'learning_rate': 6.418571681729577e-06, 'epoch': 8.45}
{'padding_count': 16, 'loss': 1.9395, 'learning_rate': 6.4097111465532525e-06, 'epoch': 8.47}
{'padding_count': 16, 'loss': 1.9541, 'learning_rate': 6.400850611376928e-06, 'epoch': 8.48}
{'padding_count': 16, 'loss': 1.734, 'learning_rate': 6.391990076200603e-06, 'epoch': 8.5}
{'padding_count': 16, 'loss': 2.0528, 'learning_rate': 6.383129541024279e-06, 'epoch': 8.52}
{'padding_count': 16, 'loss': 1.8193, 'learning_rate': 6.374269005847954e-06, 'epoch': 8.53}
{'padding_count': 16, 'loss': 2.0158, 'learning_rate': 6.365408470671629e-06, 'epoch': 8.55}
{'padding_count': 16, 'loss': 2.0065, 'learning_rate': 6.356547935495304e-06, 'epoch': 8.56}
{'padding_count': 16, 'loss': 2.0868, 'learning_rate': 6.347687400318979e-06, 'epoch': 8.58}
{'padding_count': 16, 'loss': 2.1036, 'learning_rate': 6.338826865142655e-06, 'epoch': 8.6}
{'padding_count': 16, 'loss': 2.1999, 'learning_rate': 6.3299663299663304e-06, 'epoch': 8.61}
{'padding_count': 16, 'loss': 1.7517, 'learning_rate': 6.321105794790006e-06, 'epoch': 8.63}
{'padding_count': 16, 'loss': 1.9864, 'learning_rate': 6.312245259613681e-06, 'epoch': 8.64}
{'padding_count': 16, 'loss': 1.7139, 'learning_rate': 6.303384724437357e-06, 'epoch': 8.66}
{'padding_count': 16, 'loss': 1.5375, 'learning_rate': 6.294524189261032e-06, 'epoch': 8.68}
{'padding_count': 17, 'loss': 2.0428, 'learning_rate': 6.285663654084707e-06, 'epoch': 8.69}
{'padding_count': 17, 'loss': 1.8658, 'learning_rate': 6.276803118908382e-06, 'epoch': 8.71}
{'padding_count': 17, 'loss': 2.0753, 'learning_rate': 6.267942583732058e-06, 'epoch': 8.72}
{'padding_count': 17, 'loss': 2.1608, 'learning_rate': 6.259082048555733e-06, 'epoch': 8.74}
{'padding_count': 17, 'loss': 1.6947, 'learning_rate': 6.250221513379408e-06, 'epoch': 8.76}
{'padding_count': 17, 'loss': 1.9863, 'learning_rate': 6.2413609782030835e-06, 'epoch': 8.77}
{'padding_count': 17, 'loss': 2.1222, 'learning_rate': 6.232500443026759e-06, 'epoch': 8.79}
{'padding_count': 17, 'loss': 1.7407, 'learning_rate': 6.223639907850435e-06, 'epoch': 8.8}
{'padding_count': 17, 'loss': 2.0744, 'learning_rate': 6.21477937267411e-06, 'epoch': 8.82}
{'padding_count': 17, 'loss': 1.6633, 'learning_rate': 6.205918837497785e-06, 'epoch': 8.84}
{'padding_count': 17, 'loss': 2.0591, 'learning_rate': 6.19705830232146e-06, 'epoch': 8.85}
{'padding_count': 17, 'loss': 2.2963, 'learning_rate': 6.188197767145137e-06, 'epoch': 8.87}
{'padding_count': 17, 'loss': 1.9276, 'learning_rate': 6.179337231968811e-06, 'epoch': 8.88}
{'padding_count': 18, 'loss': 1.8907, 'learning_rate': 6.170476696792486e-06, 'epoch': 8.9}
{'padding_count': 18, 'loss': 1.7703, 'learning_rate': 6.1616161616161615e-06, 'epoch': 8.92}
{'padding_count': 18, 'loss': 1.7101, 'learning_rate': 6.152755626439837e-06, 'epoch': 8.93}
{'padding_count': 18, 'loss': 1.9391, 'learning_rate': 6.1438950912635135e-06, 'epoch': 8.95}
{'padding_count': 18, 'loss': 1.9473, 'learning_rate': 6.135034556087189e-06, 'epoch': 8.96}
{'padding_count': 18, 'loss': 2.0109, 'learning_rate': 6.126174020910864e-06, 'epoch': 8.98}
{'padding_count': 18, 'loss': 2.1129, 'learning_rate': 6.117313485734538e-06, 'epoch': 9.0}
{'eval_loss': 3.257282018661499, 'eval_runtime': 44.0184, 'eval_samples_per_second': 6.179, 'eval_steps_per_second': 1.545, 'epoch': 9.0}
{'padding_count': 18, 'loss': 1.992, 'learning_rate': 6.108452950558215e-06, 'epoch': 9.01}
{'padding_count': 18, 'loss': 1.8968, 'learning_rate': 6.09959241538189e-06, 'epoch': 9.03}
{'padding_count': 18, 'loss': 2.1418, 'learning_rate': 6.090731880205565e-06, 'epoch': 9.04}
{'padding_count': 18, 'loss': 2.1524, 'learning_rate': 6.08187134502924e-06, 'epoch': 9.06}
{'padding_count': 18, 'loss': 1.8735, 'learning_rate': 6.073010809852916e-06, 'epoch': 9.07}
{'padding_count': 18, 'loss': 1.577, 'learning_rate': 6.0641502746765914e-06, 'epoch': 9.09}
{'padding_count': 18, 'loss': 2.0209, 'learning_rate': 6.0552897395002666e-06, 'epoch': 9.11}
{'padding_count': 18, 'loss': 2.0872, 'learning_rate': 6.046429204323942e-06, 'epoch': 9.12}
{'padding_count': 18, 'loss': 1.9528, 'learning_rate': 6.037568669147617e-06, 'epoch': 9.14}
{'padding_count': 18, 'loss': 1.6673, 'learning_rate': 6.028708133971293e-06, 'epoch': 9.15}
{'padding_count': 18, 'loss': 2.2221, 'learning_rate': 6.019847598794968e-06, 'epoch': 9.17}
{'padding_count': 18, 'loss': 2.2785, 'learning_rate': 6.010987063618643e-06, 'epoch': 9.19}
{'padding_count': 18, 'loss': 1.8222, 'learning_rate': 6.002126528442318e-06, 'epoch': 9.2}
{'padding_count': 18, 'loss': 1.784, 'learning_rate': 5.993265993265994e-06, 'epoch': 9.22}
{'padding_count': 18, 'loss': 1.9169, 'learning_rate': 5.984405458089669e-06, 'epoch': 9.23}
{'padding_count': 18, 'loss': 1.7384, 'learning_rate': 5.9755449229133445e-06, 'epoch': 9.25}
{'padding_count': 18, 'loss': 1.6174, 'learning_rate': 5.96668438773702e-06, 'epoch': 9.27}
{'padding_count': 18, 'loss': 1.6731, 'learning_rate': 5.957823852560695e-06, 'epoch': 9.28}
{'padding_count': 18, 'loss': 1.9101, 'learning_rate': 5.948963317384371e-06, 'epoch': 9.3}
{'padding_count': 18, 'loss': 1.7597, 'learning_rate': 5.940102782208046e-06, 'epoch': 9.31}
{'padding_count': 18, 'loss': 2.0104, 'learning_rate': 5.931242247031721e-06, 'epoch': 9.33}
{'padding_count': 18, 'loss': 1.7943, 'learning_rate': 5.922381711855396e-06, 'epoch': 9.35}
{'padding_count': 18, 'loss': 1.9282, 'learning_rate': 5.913521176679072e-06, 'epoch': 9.36}
{'padding_count': 18, 'loss': 1.7585, 'learning_rate': 5.904660641502747e-06, 'epoch': 9.38}
{'padding_count': 18, 'loss': 1.7691, 'learning_rate': 5.8958001063264225e-06, 'epoch': 9.39}
{'padding_count': 18, 'loss': 1.6938, 'learning_rate': 5.886939571150098e-06, 'epoch': 9.41}
{'padding_count': 18, 'loss': 1.5854, 'learning_rate': 5.878079035973774e-06, 'epoch': 9.43}
{'padding_count': 18, 'loss': 1.6921, 'learning_rate': 5.869218500797449e-06, 'epoch': 9.44}
{'padding_count': 18, 'loss': 2.0377, 'learning_rate': 5.860357965621124e-06, 'epoch': 9.46}
{'padding_count': 18, 'loss': 1.8772, 'learning_rate': 5.851497430444799e-06, 'epoch': 9.47}
{'padding_count': 18, 'loss': 1.6753, 'learning_rate': 5.842636895268474e-06, 'epoch': 9.49}
{'padding_count': 18, 'loss': 1.7263, 'learning_rate': 5.83377636009215e-06, 'epoch': 9.51}
{'padding_count': 18, 'loss': 2.0464, 'learning_rate': 5.824915824915825e-06, 'epoch': 9.52}
{'padding_count': 18, 'loss': 1.6737, 'learning_rate': 5.8160552897395004e-06, 'epoch': 9.54}
{'padding_count': 18, 'loss': 1.9127, 'learning_rate': 5.8071947545631756e-06, 'epoch': 9.55}
{'padding_count': 18, 'loss': 1.9628, 'learning_rate': 5.7983342193868516e-06, 'epoch': 9.57}
{'padding_count': 18, 'loss': 1.9716, 'learning_rate': 5.789473684210527e-06, 'epoch': 9.59}
{'padding_count': 18, 'loss': 2.0248, 'learning_rate': 5.780613149034202e-06, 'epoch': 9.6}
{'padding_count': 18, 'loss': 2.019, 'learning_rate': 5.771752613857877e-06, 'epoch': 9.62}
{'padding_count': 18, 'loss': 1.5535, 'learning_rate': 5.762892078681552e-06, 'epoch': 9.63}
{'padding_count': 18, 'loss': 1.8526, 'learning_rate': 5.754031543505228e-06, 'epoch': 9.65}
{'padding_count': 18, 'loss': 1.6639, 'learning_rate': 5.745171008328903e-06, 'epoch': 9.67}
{'padding_count': 18, 'loss': 1.5023, 'learning_rate': 5.736310473152578e-06, 'epoch': 9.68}
{'padding_count': 19, 'loss': 1.8583, 'learning_rate': 5.7274499379762535e-06, 'epoch': 9.7}
{'padding_count': 19, 'loss': 1.7894, 'learning_rate': 5.71858940279993e-06, 'epoch': 9.71}
{'padding_count': 19, 'loss': 2.0384, 'learning_rate': 5.7097288676236055e-06, 'epoch': 9.73}
{'padding_count': 19, 'loss': 1.9702, 'learning_rate': 5.701754385964913e-06, 'epoch': 9.74}
{'padding_count': 19, 'loss': 1.6703, 'learning_rate': 5.692893850788588e-06, 'epoch': 9.76}
{'padding_count': 19, 'loss': 2.0016, 'learning_rate': 5.684033315612263e-06, 'epoch': 9.78}
{'padding_count': 19, 'loss': 1.9825, 'learning_rate': 5.675172780435939e-06, 'epoch': 9.79}
{'padding_count': 19, 'loss': 1.7661, 'learning_rate': 5.6663122452596145e-06, 'epoch': 9.81}
{'padding_count': 19, 'loss': 1.7318, 'learning_rate': 5.65745171008329e-06, 'epoch': 9.82}
{'padding_count': 19, 'loss': 1.8179, 'learning_rate': 5.648591174906965e-06, 'epoch': 9.84}
{'padding_count': 19, 'loss': 2.095, 'learning_rate': 5.639730639730641e-06, 'epoch': 9.86}
{'padding_count': 19, 'loss': 2.0747, 'learning_rate': 5.630870104554316e-06, 'epoch': 9.87}
{'padding_count': 19, 'loss': 1.688, 'learning_rate': 5.622009569377991e-06, 'epoch': 9.89}
{'padding_count': 20, 'loss': 1.8582, 'learning_rate': 5.613149034201666e-06, 'epoch': 9.9}
{'padding_count': 20, 'loss': 1.6585, 'learning_rate': 5.604288499025342e-06, 'epoch': 9.92}
{'padding_count': 20, 'loss': 1.6999, 'learning_rate': 5.595427963849017e-06, 'epoch': 9.94}
{'padding_count': 20, 'loss': 1.8433, 'learning_rate': 5.586567428672692e-06, 'epoch': 9.95}
{'padding_count': 20, 'loss': 1.8325, 'learning_rate': 5.5777068934963675e-06, 'epoch': 9.97}
{'padding_count': 20, 'loss': 1.8118, 'learning_rate': 5.568846358320043e-06, 'epoch': 9.98}
{'padding_count': 20, 'loss': 2.0615, 'learning_rate': 5.559985823143719e-06, 'epoch': 10.0}
{'eval_loss': 3.304837465286255, 'eval_runtime': 43.9704, 'eval_samples_per_second': 6.186, 'eval_steps_per_second': 1.546, 'epoch': 10.0}
{'padding_count': 20, 'loss': 1.9997, 'learning_rate': 5.551125287967394e-06, 'epoch': 10.02}
{'padding_count': 20, 'loss': 1.9743, 'learning_rate': 5.542264752791069e-06, 'epoch': 10.03}
{'padding_count': 20, 'loss': 1.8695, 'learning_rate': 5.533404217614744e-06, 'epoch': 10.05}
{'padding_count': 20, 'loss': 2.0417, 'learning_rate': 5.52454368243842e-06, 'epoch': 10.06}
{'padding_count': 20, 'loss': 1.6012, 'learning_rate': 5.515683147262095e-06, 'epoch': 10.08}
{'padding_count': 20, 'loss': 1.4584, 'learning_rate': 5.50682261208577e-06, 'epoch': 10.1}
{'padding_count': 20, 'loss': 2.1446, 'learning_rate': 5.4979620769094455e-06, 'epoch': 10.11}
{'padding_count': 20, 'loss': 1.9003, 'learning_rate': 5.489101541733121e-06, 'epoch': 10.13}
{'padding_count': 20, 'loss': 1.797, 'learning_rate': 5.480241006556797e-06, 'epoch': 10.14}
{'padding_count': 20, 'loss': 1.6888, 'learning_rate': 5.471380471380472e-06, 'epoch': 10.16}
{'padding_count': 20, 'loss': 2.1254, 'learning_rate': 5.462519936204147e-06, 'epoch': 10.18}
{'padding_count': 20, 'loss': 2.0467, 'learning_rate': 5.453659401027822e-06, 'epoch': 10.19}
{'padding_count': 20, 'loss': 1.6599, 'learning_rate': 5.444798865851498e-06, 'epoch': 10.21}
{'padding_count': 20, 'loss': 1.6727, 'learning_rate': 5.435938330675173e-06, 'epoch': 10.22}
{'padding_count': 20, 'loss': 1.8586, 'learning_rate': 5.427077795498848e-06, 'epoch': 10.24}
{'padding_count': 20, 'loss': 1.4077, 'learning_rate': 5.4182172603225235e-06, 'epoch': 10.26}
{'padding_count': 20, 'loss': 1.8141, 'learning_rate': 5.4093567251461994e-06, 'epoch': 10.27}
{'padding_count': 20, 'loss': 1.6302, 'learning_rate': 5.400496189969875e-06, 'epoch': 10.29}
{'padding_count': 20, 'loss': 1.9019, 'learning_rate': 5.39163565479355e-06, 'epoch': 10.3}
{'padding_count': 20, 'loss': 1.5539, 'learning_rate': 5.382775119617225e-06, 'epoch': 10.32}
{'padding_count': 20, 'loss': 1.8244, 'learning_rate': 5.3739145844409e-06, 'epoch': 10.33}
{'padding_count': 20, 'loss': 1.9056, 'learning_rate': 5.365054049264576e-06, 'epoch': 10.35}
{'padding_count': 20, 'loss': 1.769, 'learning_rate': 5.356193514088251e-06, 'epoch': 10.37}
{'padding_count': 20, 'loss': 1.5039, 'learning_rate': 5.347332978911926e-06, 'epoch': 10.38}
{'padding_count': 20, 'loss': 1.7825, 'learning_rate': 5.338472443735601e-06, 'epoch': 10.4}
{'padding_count': 20, 'loss': 1.6606, 'learning_rate': 5.329611908559278e-06, 'epoch': 10.41}
{'padding_count': 20, 'loss': 1.3035, 'learning_rate': 5.320751373382953e-06, 'epoch': 10.43}
{'padding_count': 20, 'loss': 1.8343, 'learning_rate': 5.311890838206628e-06, 'epoch': 10.45}
{'padding_count': 20, 'loss': 1.8091, 'learning_rate': 5.303030303030303e-06, 'epoch': 10.46}
{'padding_count': 20, 'loss': 1.5367, 'learning_rate': 5.294169767853978e-06, 'epoch': 10.48}
{'padding_count': 20, 'loss': 1.6967, 'learning_rate': 5.285309232677655e-06, 'epoch': 10.49}
{'padding_count': 20, 'loss': 1.5541, 'learning_rate': 5.27644869750133e-06, 'epoch': 10.51}
{'padding_count': 20, 'loss': 2.03, 'learning_rate': 5.267588162325005e-06, 'epoch': 10.53}
{'padding_count': 20, 'loss': 1.5225, 'learning_rate': 5.258727627148679e-06, 'epoch': 10.54}
{'padding_count': 20, 'loss': 1.9217, 'learning_rate': 5.249867091972356e-06, 'epoch': 10.56}
{'padding_count': 20, 'loss': 1.787, 'learning_rate': 5.241006556796031e-06, 'epoch': 10.57}
{'padding_count': 20, 'loss': 1.9129, 'learning_rate': 5.2321460216197065e-06, 'epoch': 10.59}
{'padding_count': 20, 'loss': 2.0038, 'learning_rate': 5.223285486443382e-06, 'epoch': 10.61}
{'padding_count': 20, 'loss': 1.7187, 'learning_rate': 5.214424951267058e-06, 'epoch': 10.62}
{'padding_count': 20, 'loss': 1.6406, 'learning_rate': 5.205564416090733e-06, 'epoch': 10.64}
{'padding_count': 20, 'loss': 1.5431, 'learning_rate': 5.196703880914408e-06, 'epoch': 10.65}
{'padding_count': 20, 'loss': 1.6391, 'learning_rate': 5.187843345738083e-06, 'epoch': 10.67}
{'padding_count': 20, 'loss': 1.5848, 'learning_rate': 5.178982810561758e-06, 'epoch': 10.69}
{'padding_count': 21, 'loss': 1.6065, 'learning_rate': 5.170122275385434e-06, 'epoch': 10.7}
{'padding_count': 21, 'loss': 1.8159, 'learning_rate': 5.161261740209109e-06, 'epoch': 10.72}
{'padding_count': 21, 'loss': 1.9006, 'learning_rate': 5.1524012050327844e-06, 'epoch': 10.73}
{'padding_count': 21, 'loss': 1.7667, 'learning_rate': 5.14354066985646e-06, 'epoch': 10.75}
{'padding_count': 21, 'loss': 1.7239, 'learning_rate': 5.1346801346801356e-06, 'epoch': 10.77}
{'padding_count': 21, 'loss': 2.0817, 'learning_rate': 5.125819599503811e-06, 'epoch': 10.78}
{'padding_count': 21, 'loss': 1.5507, 'learning_rate': 5.116959064327486e-06, 'epoch': 10.8}
{'padding_count': 21, 'loss': 1.7484, 'learning_rate': 5.108098529151161e-06, 'epoch': 10.81}
{'padding_count': 21, 'loss': 1.7504, 'learning_rate': 5.099237993974836e-06, 'epoch': 10.83}
{'padding_count': 21, 'loss': 1.7613, 'learning_rate': 5.090377458798512e-06, 'epoch': 10.85}
{'padding_count': 21, 'loss': 1.9428, 'learning_rate': 5.081516923622187e-06, 'epoch': 10.86}
{'padding_count': 21, 'loss': 2.0521, 'learning_rate': 5.072656388445862e-06, 'epoch': 10.88}
{'padding_count': 21, 'loss': 1.7161, 'learning_rate': 5.0637958532695375e-06, 'epoch': 10.89}
{'padding_count': 22, 'loss': 1.4725, 'learning_rate': 5.0549353180932135e-06, 'epoch': 10.91}
{'padding_count': 22, 'loss': 1.6688, 'learning_rate': 5.046074782916889e-06, 'epoch': 10.93}
{'padding_count': 22, 'loss': 1.7283, 'learning_rate': 5.037214247740564e-06, 'epoch': 10.94}
{'padding_count': 22, 'loss': 1.6309, 'learning_rate': 5.028353712564239e-06, 'epoch': 10.96}
{'padding_count': 22, 'loss': 1.7122, 'learning_rate': 5.019493177387915e-06, 'epoch': 10.97}
{'padding_count': 22, 'loss': 1.9497, 'learning_rate': 5.01063264221159e-06, 'epoch': 10.99}
{'eval_loss': 3.338934898376465, 'eval_runtime': 43.0432, 'eval_samples_per_second': 6.319, 'eval_steps_per_second': 1.58, 'epoch': 11.0}
{'padding_count': 22, 'loss': 1.8555, 'learning_rate': 5.001772107035265e-06, 'epoch': 11.0}
{'padding_count': 22, 'loss': 1.8983, 'learning_rate': 4.99291157185894e-06, 'epoch': 11.02}
{'padding_count': 22, 'loss': 1.7661, 'learning_rate': 4.984051036682616e-06, 'epoch': 11.04}
{'padding_count': 22, 'loss': 1.9546, 'learning_rate': 4.9751905015062915e-06, 'epoch': 11.05}
{'padding_count': 22, 'loss': 1.8611, 'learning_rate': 4.966329966329967e-06, 'epoch': 11.07}
{'padding_count': 22, 'loss': 1.3706, 'learning_rate': 4.957469431153642e-06, 'epoch': 11.08}
{'padding_count': 22, 'loss': 1.665, 'learning_rate': 4.948608895977318e-06, 'epoch': 11.1}
{'padding_count': 22, 'loss': 1.846, 'learning_rate': 4.939748360800993e-06, 'epoch': 11.12}
{'padding_count': 22, 'loss': 1.8967, 'learning_rate': 4.930887825624668e-06, 'epoch': 11.13}
{'padding_count': 22, 'loss': 1.8219, 'learning_rate': 4.922027290448343e-06, 'epoch': 11.15}
{'padding_count': 22, 'loss': 1.5542, 'learning_rate': 4.913166755272018e-06, 'epoch': 11.16}
{'padding_count': 22, 'loss': 2.1226, 'learning_rate': 4.904306220095694e-06, 'epoch': 11.18}
{'padding_count': 22, 'loss': 1.7446, 'learning_rate': 4.8954456849193694e-06, 'epoch': 11.2}
{'padding_count': 22, 'loss': 1.7791, 'learning_rate': 4.8865851497430446e-06, 'epoch': 11.21}
{'padding_count': 22, 'loss': 1.6211, 'learning_rate': 4.87772461456672e-06, 'epoch': 11.23}
{'padding_count': 22, 'loss': 1.7086, 'learning_rate': 4.868864079390396e-06, 'epoch': 11.24}
{'padding_count': 22, 'loss': 1.3179, 'learning_rate': 4.860003544214071e-06, 'epoch': 11.26}
{'padding_count': 22, 'loss': 1.8071, 'learning_rate': 4.851143009037747e-06, 'epoch': 11.28}
{'padding_count': 22, 'loss': 1.4908, 'learning_rate': 4.842282473861422e-06, 'epoch': 11.29}
{'padding_count': 22, 'loss': 1.8089, 'learning_rate': 4.833421938685097e-06, 'epoch': 11.31}
{'padding_count': 22, 'loss': 1.5783, 'learning_rate': 4.824561403508772e-06, 'epoch': 11.32}
{'padding_count': 22, 'loss': 1.775, 'learning_rate': 4.815700868332447e-06, 'epoch': 11.34}
{'padding_count': 22, 'loss': 1.662, 'learning_rate': 4.806840333156123e-06, 'epoch': 11.36}
{'padding_count': 22, 'loss': 1.6794, 'learning_rate': 4.7979797979797985e-06, 'epoch': 11.37}
{'padding_count': 22, 'loss': 1.4576, 'learning_rate': 4.789119262803474e-06, 'epoch': 11.39}
{'padding_count': 22, 'loss': 1.715, 'learning_rate': 4.780258727627149e-06, 'epoch': 11.4}
{'padding_count': 22, 'loss': 1.5025, 'learning_rate': 4.771398192450825e-06, 'epoch': 11.42}
{'padding_count': 22, 'loss': 1.3164, 'learning_rate': 4.7625376572745e-06, 'epoch': 11.44}
{'padding_count': 22, 'loss': 1.7184, 'learning_rate': 4.753677122098175e-06, 'epoch': 11.45}
{'padding_count': 22, 'loss': 1.708, 'learning_rate': 4.74481658692185e-06, 'epoch': 11.47}
{'padding_count': 22, 'loss': 1.6223, 'learning_rate': 4.735956051745526e-06, 'epoch': 11.48}
{'padding_count': 22, 'loss': 1.4685, 'learning_rate': 4.727095516569201e-06, 'epoch': 11.5}
{'padding_count': 22, 'loss': 1.6743, 'learning_rate': 4.7182349813928765e-06, 'epoch': 11.52}
{'padding_count': 22, 'loss': 1.5897, 'learning_rate': 4.709374446216552e-06, 'epoch': 11.53}
{'padding_count': 22, 'loss': 1.7568, 'learning_rate': 4.700513911040227e-06, 'epoch': 11.55}
{'padding_count': 22, 'loss': 1.7761, 'learning_rate': 4.691653375863903e-06, 'epoch': 11.56}
{'padding_count': 22, 'loss': 1.7884, 'learning_rate': 4.682792840687578e-06, 'epoch': 11.58}
{'padding_count': 22, 'loss': 1.8163, 'learning_rate': 4.673932305511253e-06, 'epoch': 11.59}
{'padding_count': 22, 'loss': 1.9344, 'learning_rate': 4.665071770334928e-06, 'epoch': 11.61}
{'padding_count': 22, 'loss': 1.4232, 'learning_rate': 4.656211235158604e-06, 'epoch': 11.63}
{'padding_count': 22, 'loss': 1.6689, 'learning_rate': 4.647350699982279e-06, 'epoch': 11.64}
{'padding_count': 22, 'loss': 1.4496, 'learning_rate': 4.638490164805955e-06, 'epoch': 11.66}
{'padding_count': 22, 'loss': 1.3338, 'learning_rate': 4.62962962962963e-06, 'epoch': 11.67}
{'padding_count': 22, 'loss': 1.7302, 'learning_rate': 4.620769094453305e-06, 'epoch': 11.69}
{'padding_count': 23, 'loss': 1.4387, 'learning_rate': 4.611908559276981e-06, 'epoch': 11.71}
{'padding_count': 23, 'loss': 1.8294, 'learning_rate': 4.603048024100656e-06, 'epoch': 11.72}
{'padding_count': 23, 'loss': 1.9777, 'learning_rate': 4.594187488924332e-06, 'epoch': 11.74}
{'padding_count': 23, 'loss': 1.4143, 'learning_rate': 4.585326953748007e-06, 'epoch': 11.75}
{'padding_count': 23, 'loss': 1.7336, 'learning_rate': 4.576466418571682e-06, 'epoch': 11.77}
{'padding_count': 23, 'loss': 1.9324, 'learning_rate': 4.567605883395357e-06, 'epoch': 11.79}
{'padding_count': 23, 'loss': 1.3837, 'learning_rate': 4.558745348219033e-06, 'epoch': 11.8}
{'padding_count': 23, 'loss': 1.8181, 'learning_rate': 4.549884813042708e-06, 'epoch': 11.82}
{'padding_count': 23, 'loss': 1.4454, 'learning_rate': 4.5410242778663835e-06, 'epoch': 11.83}
{'padding_count': 23, 'loss': 1.8201, 'learning_rate': 4.532163742690059e-06, 'epoch': 11.85}
{'padding_count': 23, 'loss': 1.8821, 'learning_rate': 4.523303207513734e-06, 'epoch': 11.87}
{'padding_count': 23, 'loss': 1.8861, 'learning_rate': 4.51444267233741e-06, 'epoch': 11.88}
{'padding_count': 23, 'loss': 1.6414, 'learning_rate': 4.505582137161085e-06, 'epoch': 11.9}
{'padding_count': 24, 'loss': 1.4852, 'learning_rate': 4.49672160198476e-06, 'epoch': 11.91}
{'padding_count': 24, 'loss': 1.4504, 'learning_rate': 4.487861066808435e-06, 'epoch': 11.93}
{'padding_count': 24, 'loss': 1.6493, 'learning_rate': 4.479000531632111e-06, 'epoch': 11.95}
{'padding_count': 24, 'loss': 1.5288, 'learning_rate': 4.470139996455786e-06, 'epoch': 11.96}
{'padding_count': 24, 'loss': 1.8406, 'learning_rate': 4.4612794612794615e-06, 'epoch': 11.98}
{'padding_count': 24, 'loss': 1.7528, 'learning_rate': 4.452418926103137e-06, 'epoch': 11.99}
{'eval_loss': 3.3908252716064453, 'eval_runtime': 43.4943, 'eval_samples_per_second': 6.254, 'eval_steps_per_second': 1.563, 'epoch': 12.0}
{'padding_count': 24, 'loss': 1.7906, 'learning_rate': 4.443558390926812e-06, 'epoch': 12.01}
{'padding_count': 24, 'loss': 1.6808, 'learning_rate': 4.434697855750488e-06, 'epoch': 12.03}
{'padding_count': 24, 'loss': 1.6963, 'learning_rate': 4.425837320574163e-06, 'epoch': 12.04}
{'padding_count': 24, 'loss': 2.0039, 'learning_rate': 4.416976785397839e-06, 'epoch': 12.06}
{'padding_count': 24, 'loss': 1.6087, 'learning_rate': 4.408116250221513e-06, 'epoch': 12.07}
{'padding_count': 24, 'loss': 1.2754, 'learning_rate': 4.399255715045189e-06, 'epoch': 12.09}
{'padding_count': 24, 'loss': 1.7737, 'learning_rate': 4.390395179868864e-06, 'epoch': 12.11}
{'padding_count': 24, 'loss': 1.8336, 'learning_rate': 4.38153464469254e-06, 'epoch': 12.12}
{'padding_count': 24, 'loss': 1.7742, 'learning_rate': 4.372674109516215e-06, 'epoch': 12.14}
{'padding_count': 24, 'loss': 1.4196, 'learning_rate': 4.3638135743398906e-06, 'epoch': 12.15}
{'padding_count': 24, 'loss': 1.848, 'learning_rate': 4.354953039163566e-06, 'epoch': 12.17}
{'padding_count': 24, 'loss': 2.0037, 'learning_rate': 4.346092503987241e-06, 'epoch': 12.19}
{'padding_count': 24, 'loss': 1.6103, 'learning_rate': 4.337231968810917e-06, 'epoch': 12.2}
{'padding_count': 24, 'loss': 1.6822, 'learning_rate': 4.328371433634592e-06, 'epoch': 12.22}
{'padding_count': 24, 'loss': 1.5444, 'learning_rate': 4.319510898458267e-06, 'epoch': 12.23}
{'padding_count': 24, 'loss': 1.605, 'learning_rate': 4.310650363281942e-06, 'epoch': 12.25}
{'padding_count': 24, 'loss': 1.3421, 'learning_rate': 4.301789828105618e-06, 'epoch': 12.26}
{'padding_count': 24, 'loss': 1.482, 'learning_rate': 4.292929292929293e-06, 'epoch': 12.28}
{'padding_count': 24, 'loss': 1.6476, 'learning_rate': 4.2840687577529685e-06, 'epoch': 12.3}
{'padding_count': 24, 'loss': 1.6822, 'learning_rate': 4.275208222576644e-06, 'epoch': 12.31}
{'padding_count': 24, 'loss': 1.4515, 'learning_rate': 4.26634768740032e-06, 'epoch': 12.33}
{'padding_count': 24, 'loss': 1.6658, 'learning_rate': 4.257487152223995e-06, 'epoch': 12.34}
{'padding_count': 24, 'loss': 1.6781, 'learning_rate': 4.24862661704767e-06, 'epoch': 12.36}
{'padding_count': 24, 'loss': 1.5451, 'learning_rate': 4.239766081871345e-06, 'epoch': 12.38}
{'padding_count': 24, 'loss': 1.4139, 'learning_rate': 4.23090554669502e-06, 'epoch': 12.39}
{'padding_count': 24, 'loss': 1.536, 'learning_rate': 4.222045011518696e-06, 'epoch': 12.41}
{'padding_count': 24, 'loss': 1.3696, 'learning_rate': 4.213184476342371e-06, 'epoch': 12.42}
{'padding_count': 24, 'loss': 1.4535, 'learning_rate': 4.204323941166047e-06, 'epoch': 12.44}
{'padding_count': 24, 'loss': 1.7333, 'learning_rate': 4.195463405989722e-06, 'epoch': 12.46}
{'padding_count': 24, 'loss': 1.5425, 'learning_rate': 4.186602870813398e-06, 'epoch': 12.47}
{'padding_count': 24, 'loss': 1.5357, 'learning_rate': 4.177742335637073e-06, 'epoch': 12.49}
{'padding_count': 24, 'loss': 1.4909, 'learning_rate': 4.168881800460749e-06, 'epoch': 12.5}
{'padding_count': 24, 'loss': 1.7295, 'learning_rate': 4.160021265284424e-06, 'epoch': 12.52}
{'padding_count': 24, 'loss': 1.3998, 'learning_rate': 4.151160730108099e-06, 'epoch': 12.54}
{'padding_count': 24, 'loss': 1.7774, 'learning_rate': 4.142300194931774e-06, 'epoch': 12.55}
{'padding_count': 24, 'loss': 1.6977, 'learning_rate': 4.133439659755449e-06, 'epoch': 12.57}
{'padding_count': 24, 'loss': 1.8163, 'learning_rate': 4.124579124579125e-06, 'epoch': 12.58}
{'padding_count': 24, 'loss': 1.7372, 'learning_rate': 4.1157185894028e-06, 'epoch': 12.6}
{'padding_count': 24, 'loss': 1.7106, 'learning_rate': 4.1068580542264755e-06, 'epoch': 12.62}
{'padding_count': 24, 'loss': 1.3066, 'learning_rate': 4.097997519050151e-06, 'epoch': 12.63}
{'padding_count': 24, 'loss': 1.46, 'learning_rate': 4.089136983873827e-06, 'epoch': 12.65}
{'padding_count': 24, 'loss': 1.5853, 'learning_rate': 4.080276448697502e-06, 'epoch': 12.66}
{'padding_count': 24, 'loss': 1.2223, 'learning_rate': 4.071415913521177e-06, 'epoch': 12.68}
{'padding_count': 25, 'loss': 1.5547, 'learning_rate': 4.062555378344852e-06, 'epoch': 12.7}
{'padding_count': 25, 'loss': 1.6183, 'learning_rate': 4.053694843168527e-06, 'epoch': 12.71}
{'padding_count': 25, 'loss': 1.7649, 'learning_rate': 4.044834307992203e-06, 'epoch': 12.73}
{'padding_count': 25, 'loss': 1.8055, 'learning_rate': 4.035973772815878e-06, 'epoch': 12.74}
{'padding_count': 25, 'loss': 1.4768, 'learning_rate': 4.0271132376395535e-06, 'epoch': 12.76}
{'padding_count': 25, 'loss': 1.6279, 'learning_rate': 4.018252702463229e-06, 'epoch': 12.78}
{'padding_count': 25, 'loss': 1.7438, 'learning_rate': 4.009392167286905e-06, 'epoch': 12.79}
{'padding_count': 25, 'loss': 1.5636, 'learning_rate': 4.00053163211058e-06, 'epoch': 12.81}
{'padding_count': 25, 'loss': 1.5927, 'learning_rate': 3.991671096934256e-06, 'epoch': 12.82}
{'padding_count': 25, 'loss': 1.4695, 'learning_rate': 3.98281056175793e-06, 'epoch': 12.84}
{'padding_count': 25, 'loss': 1.8889, 'learning_rate': 3.973950026581606e-06, 'epoch': 12.85}
{'padding_count': 25, 'loss': 1.8101, 'learning_rate': 3.965089491405281e-06, 'epoch': 12.87}
{'padding_count': 25, 'loss': 1.5398, 'learning_rate': 3.956228956228956e-06, 'epoch': 12.89}
{'padding_count': 26, 'loss': 1.5683, 'learning_rate': 3.947368421052632e-06, 'epoch': 12.9}
{'padding_count': 26, 'loss': 1.5025, 'learning_rate': 3.9385078858763074e-06, 'epoch': 12.92}
{'padding_count': 26, 'loss': 1.2913, 'learning_rate': 3.929647350699983e-06, 'epoch': 12.93}
{'padding_count': 26, 'loss': 1.6872, 'learning_rate': 3.920786815523658e-06, 'epoch': 12.95}
{'padding_count': 26, 'loss': 1.5047, 'learning_rate': 3.911926280347334e-06, 'epoch': 12.97}
{'padding_count': 26, 'loss': 1.701, 'learning_rate': 3.903065745171009e-06, 'epoch': 12.98}
{'padding_count': 26, 'loss': 1.8267, 'learning_rate': 3.894205209994684e-06, 'epoch': 13.0}
{'eval_loss': 3.4226131439208984, 'eval_runtime': 43.7103, 'eval_samples_per_second': 6.223, 'eval_steps_per_second': 1.556, 'epoch': 13.0}
{'padding_count': 26, 'loss': 1.6898, 'learning_rate': 3.885344674818359e-06, 'epoch': 13.01}
{'padding_count': 26, 'loss': 1.6721, 'learning_rate': 3.876484139642035e-06, 'epoch': 13.03}
{'padding_count': 26, 'loss': 1.599, 'learning_rate': 3.86762360446571e-06, 'epoch': 13.05}
{'padding_count': 26, 'loss': 2.0379, 'learning_rate': 3.858763069289385e-06, 'epoch': 13.06}
{'padding_count': 26, 'loss': 1.2913, 'learning_rate': 3.850788587630693e-06, 'epoch': 13.08}
{'padding_count': 26, 'loss': 1.3292, 'learning_rate': 3.841928052454368e-06, 'epoch': 13.09}
{'padding_count': 26, 'loss': 1.8156, 'learning_rate': 3.833067517278044e-06, 'epoch': 13.11}
{'padding_count': 26, 'loss': 1.7064, 'learning_rate': 3.824206982101719e-06, 'epoch': 13.13}
{'padding_count': 26, 'loss': 1.6238, 'learning_rate': 3.815346446925395e-06, 'epoch': 13.14}
{'padding_count': 26, 'loss': 1.3687, 'learning_rate': 3.80648591174907e-06, 'epoch': 13.16}
{'padding_count': 26, 'loss': 1.9349, 'learning_rate': 3.7976253765727455e-06, 'epoch': 13.17}
{'padding_count': 26, 'loss': 1.8855, 'learning_rate': 3.7887648413964206e-06, 'epoch': 13.19}
{'padding_count': 26, 'loss': 1.5399, 'learning_rate': 3.7799043062200958e-06, 'epoch': 13.21}
{'padding_count': 26, 'loss': 1.434, 'learning_rate': 3.7710437710437713e-06, 'epoch': 13.22}
{'padding_count': 26, 'loss': 1.7069, 'learning_rate': 3.7621832358674465e-06, 'epoch': 13.24}
{'padding_count': 26, 'loss': 1.2645, 'learning_rate': 3.753322700691122e-06, 'epoch': 13.25}
{'padding_count': 26, 'loss': 1.5109, 'learning_rate': 3.744462165514797e-06, 'epoch': 13.27}
{'padding_count': 26, 'loss': 1.3592, 'learning_rate': 3.7356016303384727e-06, 'epoch': 13.29}
{'padding_count': 26, 'loss': 1.5986, 'learning_rate': 3.726741095162148e-06, 'epoch': 13.3}
{'padding_count': 26, 'loss': 1.4735, 'learning_rate': 3.717880559985824e-06, 'epoch': 13.32}
{'padding_count': 26, 'loss': 1.4599, 'learning_rate': 3.7090200248094986e-06, 'epoch': 13.33}
{'padding_count': 26, 'loss': 1.6234, 'learning_rate': 3.7001594896331746e-06, 'epoch': 13.35}
{'padding_count': 26, 'loss': 1.5948, 'learning_rate': 3.6912989544568497e-06, 'epoch': 13.37}
{'padding_count': 26, 'loss': 1.3604, 'learning_rate': 3.6824384192805244e-06, 'epoch': 13.38}
{'padding_count': 26, 'loss': 1.5002, 'learning_rate': 3.6735778841042004e-06, 'epoch': 13.4}
{'padding_count': 26, 'loss': 1.5534, 'learning_rate': 3.6647173489278755e-06, 'epoch': 13.41}
{'padding_count': 26, 'loss': 1.1515, 'learning_rate': 3.655856813751551e-06, 'epoch': 13.43}
{'padding_count': 26, 'loss': 1.4954, 'learning_rate': 3.6469962785752262e-06, 'epoch': 13.44}
{'padding_count': 26, 'loss': 1.6027, 'learning_rate': 3.638135743398902e-06, 'epoch': 13.46}
{'padding_count': 26, 'loss': 1.5045, 'learning_rate': 3.629275208222577e-06, 'epoch': 13.48}
{'padding_count': 26, 'loss': 1.3914, 'learning_rate': 3.6204146730462525e-06, 'epoch': 13.49}
{'padding_count': 26, 'loss': 1.5244, 'learning_rate': 3.6115541378699277e-06, 'epoch': 13.51}
{'padding_count': 26, 'loss': 1.7293, 'learning_rate': 3.6026936026936032e-06, 'epoch': 13.52}
{'padding_count': 26, 'loss': 1.3616, 'learning_rate': 3.5938330675172784e-06, 'epoch': 13.54}
{'padding_count': 26, 'loss': 1.7161, 'learning_rate': 3.5849725323409535e-06, 'epoch': 13.56}
{'padding_count': 26, 'loss': 1.6245, 'learning_rate': 3.576111997164629e-06, 'epoch': 13.57}
{'padding_count': 26, 'loss': 1.5683, 'learning_rate': 3.567251461988304e-06, 'epoch': 13.59}
{'padding_count': 26, 'loss': 1.8771, 'learning_rate': 3.5583909268119798e-06, 'epoch': 13.6}
{'padding_count': 26, 'loss': 1.589, 'learning_rate': 3.549530391635655e-06, 'epoch': 13.62}
{'padding_count': 26, 'loss': 1.3038, 'learning_rate': 3.5406698564593305e-06, 'epoch': 13.64}
{'padding_count': 26, 'loss': 1.4309, 'learning_rate': 3.5318093212830056e-06, 'epoch': 13.65}
{'padding_count': 26, 'loss': 1.4087, 'learning_rate': 3.522948786106681e-06, 'epoch': 13.67}
{'padding_count': 26, 'loss': 1.1981, 'learning_rate': 3.5140882509303563e-06, 'epoch': 13.68}
{'padding_count': 27, 'loss': 1.4047, 'learning_rate': 3.5052277157540323e-06, 'epoch': 13.7}
{'padding_count': 27, 'loss': 1.5925, 'learning_rate': 3.496367180577707e-06, 'epoch': 13.72}
{'padding_count': 27, 'loss': 1.7181, 'learning_rate': 3.487506645401382e-06, 'epoch': 13.73}
{'padding_count': 27, 'loss': 1.6209, 'learning_rate': 3.478646110225058e-06, 'epoch': 13.75}
{'padding_count': 27, 'loss': 1.531, 'learning_rate': 3.469785575048733e-06, 'epoch': 13.76}
{'padding_count': 27, 'loss': 1.7638, 'learning_rate': 3.460925039872409e-06, 'epoch': 13.78}
{'padding_count': 27, 'loss': 1.5102, 'learning_rate': 3.452064504696084e-06, 'epoch': 13.8}
{'padding_count': 27, 'loss': 1.6749, 'learning_rate': 3.4432039695197596e-06, 'epoch': 13.81}
{'padding_count': 27, 'loss': 1.4338, 'learning_rate': 3.4343434343434347e-06, 'epoch': 13.83}
{'padding_count': 27, 'loss': 1.5776, 'learning_rate': 3.4254828991671103e-06, 'epoch': 13.84}
{'padding_count': 27, 'loss': 1.7009, 'learning_rate': 3.4166223639907854e-06, 'epoch': 13.86}
{'padding_count': 27, 'loss': 1.8571, 'learning_rate': 3.407761828814461e-06, 'epoch': 13.88}
{'padding_count': 27, 'loss': 1.5551, 'learning_rate': 3.398901293638136e-06, 'epoch': 13.89}
{'padding_count': 28, 'loss': 1.3832, 'learning_rate': 3.3900407584618112e-06, 'epoch': 13.91}
{'padding_count': 28, 'loss': 1.3588, 'learning_rate': 3.381180223285487e-06, 'epoch': 13.92}
{'padding_count': 28, 'loss': 1.4753, 'learning_rate': 3.372319688109162e-06, 'epoch': 13.94}
{'padding_count': 28, 'loss': 1.4342, 'learning_rate': 3.3634591529328375e-06, 'epoch': 13.96}
{'padding_count': 28, 'loss': 1.5228, 'learning_rate': 3.3545986177565126e-06, 'epoch': 13.97}
{'padding_count': 28, 'loss': 1.6375, 'learning_rate': 3.3457380825801882e-06, 'epoch': 13.99}
{'eval_loss': 3.444420337677002, 'eval_runtime': 43.8474, 'eval_samples_per_second': 6.203, 'eval_steps_per_second': 1.551, 'epoch': 14.0}
{'padding_count': 28, 'loss': 1.6736, 'learning_rate': 3.3368775474038634e-06, 'epoch': 14.0}
{'padding_count': 28, 'loss': 1.7126, 'learning_rate': 3.328017012227539e-06, 'epoch': 14.02}
{'padding_count': 28, 'loss': 1.6514, 'learning_rate': 3.319156477051214e-06, 'epoch': 14.04}
{'padding_count': 28, 'loss': 1.6401, 'learning_rate': 3.3102959418748896e-06, 'epoch': 14.05}
{'padding_count': 28, 'loss': 1.7049, 'learning_rate': 3.3014354066985648e-06, 'epoch': 14.07}
{'padding_count': 28, 'loss': 1.1537, 'learning_rate': 3.29257487152224e-06, 'epoch': 14.08}
{'padding_count': 28, 'loss': 1.3911, 'learning_rate': 3.2837143363459155e-06, 'epoch': 14.1}
{'padding_count': 28, 'loss': 1.8677, 'learning_rate': 3.2748538011695906e-06, 'epoch': 14.11}
{'padding_count': 28, 'loss': 1.5487, 'learning_rate': 3.2659932659932666e-06, 'epoch': 14.13}
{'padding_count': 28, 'loss': 1.5486, 'learning_rate': 3.2571327308169413e-06, 'epoch': 14.15}
{'padding_count': 28, 'loss': 1.3332, 'learning_rate': 3.2482721956406173e-06, 'epoch': 14.16}
{'padding_count': 28, 'loss': 1.896, 'learning_rate': 3.2394116604642924e-06, 'epoch': 14.18}
{'padding_count': 28, 'loss': 1.6189, 'learning_rate': 3.230551125287968e-06, 'epoch': 14.19}
{'padding_count': 28, 'loss': 1.5281, 'learning_rate': 3.221690590111643e-06, 'epoch': 14.21}
{'padding_count': 28, 'loss': 1.4393, 'learning_rate': 3.2128300549353187e-06, 'epoch': 14.23}
{'padding_count': 28, 'loss': 1.4179, 'learning_rate': 3.203969519758994e-06, 'epoch': 14.24}
{'padding_count': 28, 'loss': 1.0909, 'learning_rate': 3.195108984582669e-06, 'epoch': 14.26}
{'padding_count': 28, 'loss': 1.6677, 'learning_rate': 3.1862484494063445e-06, 'epoch': 14.27}
{'padding_count': 28, 'loss': 1.2397, 'learning_rate': 3.1773879142300197e-06, 'epoch': 14.29}
{'padding_count': 28, 'loss': 1.6197, 'learning_rate': 3.1685273790536953e-06, 'epoch': 14.31}
{'padding_count': 28, 'loss': 1.2903, 'learning_rate': 3.1596668438773704e-06, 'epoch': 14.32}
{'padding_count': 28, 'loss': 1.6454, 'learning_rate': 3.150806308701046e-06, 'epoch': 14.34}
{'padding_count': 28, 'loss': 1.4369, 'learning_rate': 3.141945773524721e-06, 'epoch': 14.35}
{'padding_count': 28, 'loss': 1.5383, 'learning_rate': 3.1330852383483967e-06, 'epoch': 14.37}
{'padding_count': 28, 'loss': 1.1554, 'learning_rate': 3.124224703172072e-06, 'epoch': 14.39}
{'padding_count': 28, 'loss': 1.5867, 'learning_rate': 3.115364167995747e-06, 'epoch': 14.4}
{'padding_count': 28, 'loss': 1.2683, 'learning_rate': 3.1065036328194225e-06, 'epoch': 14.42}
{'padding_count': 28, 'loss': 1.1854, 'learning_rate': 3.0976430976430976e-06, 'epoch': 14.43}
{'padding_count': 28, 'loss': 1.5549, 'learning_rate': 3.088782562466773e-06, 'epoch': 14.45}
{'padding_count': 28, 'loss': 1.6639, 'learning_rate': 3.0799220272904483e-06, 'epoch': 14.47}
{'padding_count': 28, 'loss': 1.3382, 'learning_rate': 3.071061492114124e-06, 'epoch': 14.48}
{'padding_count': 28, 'loss': 1.3907, 'learning_rate': 3.062200956937799e-06, 'epoch': 14.5}
{'padding_count': 28, 'loss': 1.4989, 'learning_rate': 3.053340421761475e-06, 'epoch': 14.51}
{'padding_count': 28, 'loss': 1.4654, 'learning_rate': 3.0444798865851498e-06, 'epoch': 14.53}
{'padding_count': 28, 'loss': 1.531, 'learning_rate': 3.0356193514088257e-06, 'epoch': 14.55}
{'padding_count': 28, 'loss': 1.6286, 'learning_rate': 3.026758816232501e-06, 'epoch': 14.56}
{'padding_count': 28, 'loss': 1.414, 'learning_rate': 3.0178982810561756e-06, 'epoch': 14.58}
{'padding_count': 28, 'loss': 1.7246, 'learning_rate': 3.0090377458798516e-06, 'epoch': 14.59}
{'padding_count': 28, 'loss': 1.6858, 'learning_rate': 3.0001772107035267e-06, 'epoch': 14.61}
{'padding_count': 28, 'loss': 1.4343, 'learning_rate': 2.9913166755272023e-06, 'epoch': 14.63}
{'padding_count': 28, 'loss': 1.3423, 'learning_rate': 2.9824561403508774e-06, 'epoch': 14.64}
{'padding_count': 28, 'loss': 1.339, 'learning_rate': 2.973595605174553e-06, 'epoch': 14.66}
{'padding_count': 28, 'loss': 1.1271, 'learning_rate': 2.964735069998228e-06, 'epoch': 14.67}
{'padding_count': 28, 'loss': 1.4542, 'learning_rate': 2.9558745348219037e-06, 'epoch': 14.69}
{'padding_count': 29, 'loss': 1.2446, 'learning_rate': 2.947013999645579e-06, 'epoch': 14.7}
{'padding_count': 29, 'loss': 1.6502, 'learning_rate': 2.9381534644692544e-06, 'epoch': 14.72}
{'padding_count': 29, 'loss': 1.9101, 'learning_rate': 2.9292929292929295e-06, 'epoch': 14.74}
{'padding_count': 29, 'loss': 1.2355, 'learning_rate': 2.9204323941166047e-06, 'epoch': 14.75}
{'padding_count': 29, 'loss': 1.5937, 'learning_rate': 2.9115718589402802e-06, 'epoch': 14.77}
{'padding_count': 29, 'loss': 1.7551, 'learning_rate': 2.9027113237639554e-06, 'epoch': 14.78}
{'padding_count': 29, 'loss': 1.318, 'learning_rate': 2.893850788587631e-06, 'epoch': 14.8}
{'padding_count': 29, 'loss': 1.5878, 'learning_rate': 2.884990253411306e-06, 'epoch': 14.82}
{'padding_count': 29, 'loss': 1.4129, 'learning_rate': 2.8761297182349817e-06, 'epoch': 14.83}
{'padding_count': 29, 'loss': 1.5588, 'learning_rate': 2.867269183058657e-06, 'epoch': 14.85}
{'padding_count': 29, 'loss': 1.65, 'learning_rate': 2.8584086478823324e-06, 'epoch': 14.86}
{'padding_count': 29, 'loss': 1.7443, 'learning_rate': 2.8495481127060075e-06, 'epoch': 14.88}
{'padding_count': 29, 'loss': 1.3654, 'learning_rate': 2.840687577529683e-06, 'epoch': 14.9}
{'padding_count': 30, 'loss': 1.3404, 'learning_rate': 2.831827042353358e-06, 'epoch': 14.91}
{'padding_count': 30, 'loss': 1.2969, 'learning_rate': 2.8229665071770333e-06, 'epoch': 14.93}
{'padding_count': 30, 'loss': 1.456, 'learning_rate': 2.8141059720007093e-06, 'epoch': 14.94}
{'padding_count': 30, 'loss': 1.3766, 'learning_rate': 2.805245436824384e-06, 'epoch': 14.96}
{'padding_count': 30, 'loss': 1.5527, 'learning_rate': 2.79638490164806e-06, 'epoch': 14.98}
{'padding_count': 30, 'loss': 1.6629, 'learning_rate': 2.787524366471735e-06, 'epoch': 14.99}
{'eval_loss': 3.4702861309051514, 'eval_runtime': 43.3945, 'eval_samples_per_second': 6.268, 'eval_steps_per_second': 1.567, 'epoch': 15.0}
{'padding_count': 30, 'loss': 1.5238, 'learning_rate': 2.7786638312954107e-06, 'epoch': 15.01}
{'padding_count': 30, 'loss': 1.6438, 'learning_rate': 2.769803296119086e-06, 'epoch': 15.02}
{'padding_count': 30, 'loss': 1.5238, 'learning_rate': 2.7609427609427614e-06, 'epoch': 15.04}
{'padding_count': 30, 'loss': 1.7521, 'learning_rate': 2.7520822257664366e-06, 'epoch': 15.06}
{'padding_count': 30, 'loss': 1.5336, 'learning_rate': 2.743221690590112e-06, 'epoch': 15.07}
{'padding_count': 30, 'loss': 1.0957, 'learning_rate': 2.7343611554137873e-06, 'epoch': 15.09}
{'padding_count': 30, 'loss': 1.4471, 'learning_rate': 2.7255006202374624e-06, 'epoch': 15.1}
{'padding_count': 30, 'loss': 1.7135, 'learning_rate': 2.716640085061138e-06, 'epoch': 15.12}
{'padding_count': 30, 'loss': 1.6097, 'learning_rate': 2.707779549884813e-06, 'epoch': 15.14}
{'padding_count': 30, 'loss': 1.2874, 'learning_rate': 2.6989190147084887e-06, 'epoch': 15.15}
{'padding_count': 30, 'loss': 1.5911, 'learning_rate': 2.690058479532164e-06, 'epoch': 15.17}
{'padding_count': 30, 'loss': 1.7792, 'learning_rate': 2.6811979443558394e-06, 'epoch': 15.18}
{'padding_count': 30, 'loss': 1.461, 'learning_rate': 2.6723374091795145e-06, 'epoch': 15.2}
{'padding_count': 30, 'loss': 1.5087, 'learning_rate': 2.66347687400319e-06, 'epoch': 15.22}
{'padding_count': 30, 'loss': 1.3258, 'learning_rate': 2.6546163388268652e-06, 'epoch': 15.23}
{'padding_count': 30, 'loss': 1.5291, 'learning_rate': 2.645755803650541e-06, 'epoch': 15.25}
{'padding_count': 30, 'loss': 1.0781, 'learning_rate': 2.636895268474216e-06, 'epoch': 15.26}
{'padding_count': 30, 'loss': 1.4066, 'learning_rate': 2.628034733297891e-06, 'epoch': 15.28}
{'padding_count': 30, 'loss': 1.3558, 'learning_rate': 2.6191741981215666e-06, 'epoch': 15.3}
{'padding_count': 30, 'loss': 1.5466, 'learning_rate': 2.6103136629452418e-06, 'epoch': 15.31}
{'padding_count': 30, 'loss': 1.2827, 'learning_rate': 2.6014531277689178e-06, 'epoch': 15.33}
{'padding_count': 30, 'loss': 1.4145, 'learning_rate': 2.5925925925925925e-06, 'epoch': 15.34}
{'padding_count': 30, 'loss': 1.5633, 'learning_rate': 2.5837320574162685e-06, 'epoch': 15.36}
{'padding_count': 30, 'loss': 1.2528, 'learning_rate': 2.5748715222399436e-06, 'epoch': 15.37}
{'padding_count': 30, 'loss': 1.2789, 'learning_rate': 2.566010987063619e-06, 'epoch': 15.39}
{'padding_count': 30, 'loss': 1.4547, 'learning_rate': 2.5571504518872943e-06, 'epoch': 15.41}
{'padding_count': 30, 'loss': 1.1343, 'learning_rate': 2.54828991671097e-06, 'epoch': 15.42}
{'padding_count': 30, 'loss': 1.3098, 'learning_rate': 2.539429381534645e-06, 'epoch': 15.44}
{'padding_count': 30, 'loss': 1.4732, 'learning_rate': 2.53056884635832e-06, 'epoch': 15.45}
{'padding_count': 30, 'loss': 1.4294, 'learning_rate': 2.5217083111819957e-06, 'epoch': 15.47}
{'padding_count': 30, 'loss': 1.4095, 'learning_rate': 2.512847776005671e-06, 'epoch': 15.49}
{'padding_count': 30, 'loss': 1.3138, 'learning_rate': 2.5039872408293464e-06, 'epoch': 15.5}
{'padding_count': 30, 'loss': 1.5307, 'learning_rate': 2.4951267056530216e-06, 'epoch': 15.52}
{'padding_count': 30, 'loss': 1.2249, 'learning_rate': 2.4862661704766967e-06, 'epoch': 15.53}
{'padding_count': 30, 'loss': 1.5922, 'learning_rate': 2.4774056353003723e-06, 'epoch': 15.55}
{'padding_count': 30, 'loss': 1.5043, 'learning_rate': 2.468545100124048e-06, 'epoch': 15.57}
{'padding_count': 30, 'loss': 1.6246, 'learning_rate': 2.459684564947723e-06, 'epoch': 15.58}
{'padding_count': 30, 'loss': 1.6566, 'learning_rate': 2.4508240297713985e-06, 'epoch': 15.6}
{'padding_count': 30, 'loss': 1.612, 'learning_rate': 2.4419634945950737e-06, 'epoch': 15.61}
{'padding_count': 30, 'loss': 1.1624, 'learning_rate': 2.4331029594187492e-06, 'epoch': 15.63}
{'padding_count': 30, 'loss': 1.4536, 'learning_rate': 2.4242424242424244e-06, 'epoch': 15.65}
{'padding_count': 30, 'loss': 1.2416, 'learning_rate': 2.4153818890661e-06, 'epoch': 15.66}
{'padding_count': 30, 'loss': 1.1346, 'learning_rate': 2.406521353889775e-06, 'epoch': 15.68}
{'padding_count': 31, 'loss': 1.4891, 'learning_rate': 2.3976608187134502e-06, 'epoch': 15.69}
{'padding_count': 31, 'loss': 1.4162, 'learning_rate': 2.388800283537126e-06, 'epoch': 15.71}
{'padding_count': 31, 'loss': 1.558, 'learning_rate': 2.379939748360801e-06, 'epoch': 15.73}
{'padding_count': 31, 'loss': 1.782, 'learning_rate': 2.3710792131844765e-06, 'epoch': 15.74}
{'padding_count': 31, 'loss': 1.2502, 'learning_rate': 2.362218678008152e-06, 'epoch': 15.76}
{'padding_count': 31, 'loss': 1.4818, 'learning_rate': 2.353358142831827e-06, 'epoch': 15.77}
{'padding_count': 31, 'loss': 1.6275, 'learning_rate': 2.3444976076555028e-06, 'epoch': 15.79}
{'padding_count': 31, 'loss': 1.3763, 'learning_rate': 2.335637072479178e-06, 'epoch': 15.81}
{'padding_count': 31, 'loss': 1.4003, 'learning_rate': 2.3267765373028535e-06, 'epoch': 15.82}
{'padding_count': 31, 'loss': 1.4332, 'learning_rate': 2.3179160021265286e-06, 'epoch': 15.84}
{'padding_count': 31, 'loss': 1.6556, 'learning_rate': 2.309055466950204e-06, 'epoch': 15.85}
{'padding_count': 31, 'loss': 1.6653, 'learning_rate': 2.3001949317738793e-06, 'epoch': 15.87}
{'padding_count': 31, 'loss': 1.4384, 'learning_rate': 2.2913343965975545e-06, 'epoch': 15.89}
{'padding_count': 32, 'loss': 1.3328, 'learning_rate': 2.28247386142123e-06, 'epoch': 15.9}
{'padding_count': 32, 'loss': 1.4281, 'learning_rate': 2.273613326244905e-06, 'epoch': 15.92}
{'padding_count': 32, 'loss': 1.2437, 'learning_rate': 2.2647527910685807e-06, 'epoch': 15.93}
{'padding_count': 32, 'loss': 1.4715, 'learning_rate': 2.2558922558922563e-06, 'epoch': 15.95}
{'padding_count': 32, 'loss': 1.3749, 'learning_rate': 2.2470317207159314e-06, 'epoch': 15.96}
{'padding_count': 32, 'loss': 1.5584, 'learning_rate': 2.238171185539607e-06, 'epoch': 15.98}
{'padding_count': 32, 'loss': 1.5664, 'learning_rate': 2.229310650363282e-06, 'epoch': 16.0}
{'eval_loss': 3.4968230724334717, 'eval_runtime': 43.5045, 'eval_samples_per_second': 6.252, 'eval_steps_per_second': 1.563, 'epoch': 16.0}
{'padding_count': 32, 'loss': 1.5162, 'learning_rate': 2.2204501151869577e-06, 'epoch': 16.01}
{'padding_count': 32, 'loss': 1.4673, 'learning_rate': 2.211589580010633e-06, 'epoch': 16.03}
{'padding_count': 32, 'loss': 1.5906, 'learning_rate': 2.202729044834308e-06, 'epoch': 16.04}
{'padding_count': 32, 'loss': 1.8633, 'learning_rate': 2.1938685096579835e-06, 'epoch': 16.06}
{'padding_count': 32, 'loss': 1.2183, 'learning_rate': 2.1850079744816587e-06, 'epoch': 16.08}
{'padding_count': 32, 'loss': 1.1231, 'learning_rate': 2.1761474393053342e-06, 'epoch': 16.09}
{'padding_count': 32, 'loss': 1.6434, 'learning_rate': 2.1672869041290094e-06, 'epoch': 16.11}
{'padding_count': 32, 'loss': 1.473, 'learning_rate': 2.158426368952685e-06, 'epoch': 16.12}
{'padding_count': 32, 'loss': 1.5532, 'learning_rate': 2.1495658337763605e-06, 'epoch': 16.14}
{'padding_count': 32, 'loss': 1.1572, 'learning_rate': 2.1407052986000357e-06, 'epoch': 16.16}
{'padding_count': 32, 'loss': 1.6999, 'learning_rate': 2.1318447634237112e-06, 'epoch': 16.17}
{'padding_count': 32, 'loss': 1.7516, 'learning_rate': 2.1229842282473864e-06, 'epoch': 16.19}
{'padding_count': 32, 'loss': 1.3846, 'learning_rate': 2.114123693071062e-06, 'epoch': 16.2}
{'padding_count': 32, 'loss': 1.2352, 'learning_rate': 2.105263157894737e-06, 'epoch': 16.22}
{'padding_count': 32, 'loss': 1.5746, 'learning_rate': 2.096402622718412e-06, 'epoch': 16.24}
{'padding_count': 32, 'loss': 1.16, 'learning_rate': 2.0875420875420878e-06, 'epoch': 16.25}
{'padding_count': 32, 'loss': 1.3004, 'learning_rate': 2.078681552365763e-06, 'epoch': 16.27}
{'padding_count': 32, 'loss': 1.2082, 'learning_rate': 2.0698210171894385e-06, 'epoch': 16.28}
{'padding_count': 32, 'loss': 1.4915, 'learning_rate': 2.0609604820131136e-06, 'epoch': 16.3}
{'padding_count': 32, 'loss': 1.4085, 'learning_rate': 2.052099946836789e-06, 'epoch': 16.32}
{'padding_count': 32, 'loss': 1.2374, 'learning_rate': 2.0432394116604643e-06, 'epoch': 16.33}
{'padding_count': 32, 'loss': 1.4767, 'learning_rate': 2.03437887648414e-06, 'epoch': 16.35}
{'padding_count': 32, 'loss': 1.4226, 'learning_rate': 2.0255183413078154e-06, 'epoch': 16.36}
{'padding_count': 32, 'loss': 1.4082, 'learning_rate': 2.017543859649123e-06, 'epoch': 16.38}
{'padding_count': 32, 'loss': 1.1565, 'learning_rate': 2.008683324472798e-06, 'epoch': 16.4}
{'padding_count': 32, 'loss': 1.3393, 'learning_rate': 1.9998227892964737e-06, 'epoch': 16.41}
{'padding_count': 32, 'loss': 1.0689, 'learning_rate': 1.990962254120149e-06, 'epoch': 16.43}
{'padding_count': 32, 'loss': 1.4136, 'learning_rate': 1.9821017189438244e-06, 'epoch': 16.44}
{'padding_count': 32, 'loss': 1.498, 'learning_rate': 1.9732411837674995e-06, 'epoch': 16.46}
{'padding_count': 32, 'loss': 1.3891, 'learning_rate': 1.964380648591175e-06, 'epoch': 16.48}
{'padding_count': 32, 'loss': 1.1883, 'learning_rate': 1.9555201134148507e-06, 'epoch': 16.49}
{'padding_count': 32, 'loss': 1.2821, 'learning_rate': 1.946659578238526e-06, 'epoch': 16.51}
{'padding_count': 32, 'loss': 1.625, 'learning_rate': 1.9377990430622014e-06, 'epoch': 16.52}
{'padding_count': 32, 'loss': 1.1581, 'learning_rate': 1.9289385078858765e-06, 'epoch': 16.54}
{'padding_count': 32, 'loss': 1.5744, 'learning_rate': 1.9200779727095516e-06, 'epoch': 16.56}
{'padding_count': 32, 'loss': 1.4788, 'learning_rate': 1.911217437533227e-06, 'epoch': 16.57}
{'padding_count': 32, 'loss': 1.4434, 'learning_rate': 1.9023569023569026e-06, 'epoch': 16.59}
{'padding_count': 32, 'loss': 1.6478, 'learning_rate': 1.893496367180578e-06, 'epoch': 16.6}
{'padding_count': 32, 'loss': 1.5055, 'learning_rate': 1.8846358320042533e-06, 'epoch': 16.62}
{'padding_count': 32, 'loss': 1.1872, 'learning_rate': 1.8757752968279286e-06, 'epoch': 16.63}
{'padding_count': 32, 'loss': 1.3157, 'learning_rate': 1.866914761651604e-06, 'epoch': 16.65}
{'padding_count': 32, 'loss': 1.1333, 'learning_rate': 1.8580542264752793e-06, 'epoch': 16.67}
{'padding_count': 32, 'loss': 1.233, 'learning_rate': 1.8491936912989547e-06, 'epoch': 16.68}
{'padding_count': 33, 'loss': 1.2706, 'learning_rate': 1.84033315612263e-06, 'epoch': 16.7}
{'padding_count': 33, 'loss': 1.4996, 'learning_rate': 1.8314726209463052e-06, 'epoch': 16.71}
{'padding_count': 33, 'loss': 1.5329, 'learning_rate': 1.8226120857699805e-06, 'epoch': 16.73}
{'padding_count': 33, 'loss': 1.6073, 'learning_rate': 1.8137515505936559e-06, 'epoch': 16.75}
{'padding_count': 33, 'loss': 1.3059, 'learning_rate': 1.8048910154173312e-06, 'epoch': 16.76}
{'padding_count': 33, 'loss': 1.5696, 'learning_rate': 1.7960304802410068e-06, 'epoch': 16.78}
{'padding_count': 33, 'loss': 1.5573, 'learning_rate': 1.7871699450646821e-06, 'epoch': 16.79}
{'padding_count': 33, 'loss': 1.3272, 'learning_rate': 1.7783094098883575e-06, 'epoch': 16.81}
{'padding_count': 33, 'loss': 1.4164, 'learning_rate': 1.7694488747120328e-06, 'epoch': 16.83}
{'padding_count': 33, 'loss': 1.3028, 'learning_rate': 1.7605883395357082e-06, 'epoch': 16.84}
{'padding_count': 33, 'loss': 1.7065, 'learning_rate': 1.7517278043593835e-06, 'epoch': 16.86}
{'padding_count': 33, 'loss': 1.6665, 'learning_rate': 1.7428672691830589e-06, 'epoch': 16.87}
{'padding_count': 33, 'loss': 1.3956, 'learning_rate': 1.734006734006734e-06, 'epoch': 16.89}
{'padding_count': 34, 'loss': 1.3436, 'learning_rate': 1.7251461988304094e-06, 'epoch': 16.91}
{'padding_count': 34, 'loss': 1.1844, 'learning_rate': 1.7162856636540847e-06, 'epoch': 16.92}
{'padding_count': 34, 'loss': 1.3852, 'learning_rate': 1.70742512847776e-06, 'epoch': 16.94}
{'padding_count': 34, 'loss': 1.3907, 'learning_rate': 1.6985645933014354e-06, 'epoch': 16.95}
{'padding_count': 34, 'loss': 1.3894, 'learning_rate': 1.689704058125111e-06, 'epoch': 16.97}
{'padding_count': 34, 'loss': 1.3425, 'learning_rate': 1.6808435229487864e-06, 'epoch': 16.99}
{'eval_loss': 3.5108704566955566, 'eval_runtime': 43.827, 'eval_samples_per_second': 6.206, 'eval_steps_per_second': 1.552, 'epoch': 17.0}
{'padding_count': 34, 'loss': 1.5867, 'learning_rate': 1.6719829877724617e-06, 'epoch': 17.0}
{'padding_count': 34, 'loss': 1.6342, 'learning_rate': 1.663122452596137e-06, 'epoch': 17.02}
{'padding_count': 34, 'loss': 1.4869, 'learning_rate': 1.6542619174198124e-06, 'epoch': 17.03}
{'padding_count': 34, 'loss': 1.4821, 'learning_rate': 1.6454013822434878e-06, 'epoch': 17.05}
{'padding_count': 34, 'loss': 1.613, 'learning_rate': 1.636540847067163e-06, 'epoch': 17.07}
{'padding_count': 34, 'loss': 1.1961, 'learning_rate': 1.6276803118908383e-06, 'epoch': 17.08}
{'padding_count': 34, 'loss': 1.1104, 'learning_rate': 1.6188197767145136e-06, 'epoch': 17.1}
{'padding_count': 34, 'loss': 1.65, 'learning_rate': 1.609959241538189e-06, 'epoch': 17.11}
{'padding_count': 34, 'loss': 1.527, 'learning_rate': 1.6010987063618643e-06, 'epoch': 17.13}
{'padding_count': 34, 'loss': 1.4175, 'learning_rate': 1.5922381711855397e-06, 'epoch': 17.15}
{'padding_count': 34, 'loss': 1.16, 'learning_rate': 1.5833776360092152e-06, 'epoch': 17.16}
{'padding_count': 34, 'loss': 1.7627, 'learning_rate': 1.5745171008328906e-06, 'epoch': 17.18}
{'padding_count': 34, 'loss': 1.5772, 'learning_rate': 1.565656565656566e-06, 'epoch': 17.19}
{'padding_count': 34, 'loss': 1.2614, 'learning_rate': 1.5567960304802413e-06, 'epoch': 17.21}
{'padding_count': 34, 'loss': 1.3967, 'learning_rate': 1.5479354953039164e-06, 'epoch': 17.22}
{'padding_count': 34, 'loss': 1.4446, 'learning_rate': 1.5390749601275918e-06, 'epoch': 17.24}
{'padding_count': 34, 'loss': 0.9517, 'learning_rate': 1.5302144249512671e-06, 'epoch': 17.26}
{'padding_count': 34, 'loss': 1.516, 'learning_rate': 1.5213538897749425e-06, 'epoch': 17.27}
{'padding_count': 34, 'loss': 1.1511, 'learning_rate': 1.5124933545986178e-06, 'epoch': 17.29}
{'padding_count': 34, 'loss': 1.4683, 'learning_rate': 1.5036328194222932e-06, 'epoch': 17.3}
{'padding_count': 34, 'loss': 1.3178, 'learning_rate': 1.4947722842459685e-06, 'epoch': 17.32}
{'padding_count': 34, 'loss': 1.4011, 'learning_rate': 1.4859117490696439e-06, 'epoch': 17.34}
{'padding_count': 34, 'loss': 1.4139, 'learning_rate': 1.4770512138933194e-06, 'epoch': 17.35}
{'padding_count': 34, 'loss': 1.4521, 'learning_rate': 1.4681906787169948e-06, 'epoch': 17.37}
{'padding_count': 34, 'loss': 1.1296, 'learning_rate': 1.4593301435406702e-06, 'epoch': 17.38}
{'padding_count': 34, 'loss': 1.5657, 'learning_rate': 1.4504696083643453e-06, 'epoch': 17.4}
{'padding_count': 34, 'loss': 1.1269, 'learning_rate': 1.4416090731880206e-06, 'epoch': 17.42}
{'padding_count': 34, 'loss': 0.9289, 'learning_rate': 1.432748538011696e-06, 'epoch': 17.43}
{'padding_count': 34, 'loss': 1.4255, 'learning_rate': 1.4238880028353713e-06, 'epoch': 17.45}
{'padding_count': 34, 'loss': 1.5378, 'learning_rate': 1.4150274676590467e-06, 'epoch': 17.46}
{'padding_count': 34, 'loss': 1.0666, 'learning_rate': 1.406166932482722e-06, 'epoch': 17.48}
{'padding_count': 34, 'loss': 1.23, 'learning_rate': 1.3973063973063974e-06, 'epoch': 17.5}
{'padding_count': 34, 'loss': 1.3846, 'learning_rate': 1.3884458621300728e-06, 'epoch': 17.51}
{'padding_count': 34, 'loss': 1.4778, 'learning_rate': 1.3795853269537481e-06, 'epoch': 17.53}
{'padding_count': 34, 'loss': 1.2909, 'learning_rate': 1.3707247917774237e-06, 'epoch': 17.54}
{'padding_count': 34, 'loss': 1.5391, 'learning_rate': 1.361864256601099e-06, 'epoch': 17.56}
{'padding_count': 34, 'loss': 1.3869, 'learning_rate': 1.353003721424774e-06, 'epoch': 17.58}
{'padding_count': 34, 'loss': 1.5561, 'learning_rate': 1.3441431862484495e-06, 'epoch': 17.59}
{'padding_count': 34, 'loss': 1.5367, 'learning_rate': 1.3352826510721249e-06, 'epoch': 17.61}
{'padding_count': 34, 'loss': 1.4161, 'learning_rate': 1.3264221158958002e-06, 'epoch': 17.62}
{'padding_count': 34, 'loss': 1.1885, 'learning_rate': 1.3175615807194756e-06, 'epoch': 17.64}
{'padding_count': 34, 'loss': 1.2788, 'learning_rate': 1.308701045543151e-06, 'epoch': 17.66}
{'padding_count': 34, 'loss': 1.0799, 'learning_rate': 1.2998405103668263e-06, 'epoch': 17.67}
{'padding_count': 34, 'loss': 1.3972, 'learning_rate': 1.2909799751905016e-06, 'epoch': 17.69}
{'padding_count': 35, 'loss': 1.0956, 'learning_rate': 1.282119440014177e-06, 'epoch': 17.7}
{'padding_count': 35, 'loss': 1.5825, 'learning_rate': 1.2732589048378523e-06, 'epoch': 17.72}
{'padding_count': 35, 'loss': 1.5686, 'learning_rate': 1.2643983696615279e-06, 'epoch': 17.74}
{'padding_count': 35, 'loss': 1.362, 'learning_rate': 1.2555378344852028e-06, 'epoch': 17.75}
{'padding_count': 35, 'loss': 1.4158, 'learning_rate': 1.2466772993088784e-06, 'epoch': 17.77}
{'padding_count': 35, 'loss': 1.6574, 'learning_rate': 1.2378167641325537e-06, 'epoch': 17.78}
{'padding_count': 35, 'loss': 1.1736, 'learning_rate': 1.228956228956229e-06, 'epoch': 17.8}
{'padding_count': 35, 'loss': 1.4669, 'learning_rate': 1.2200956937799044e-06, 'epoch': 17.81}
{'padding_count': 35, 'loss': 1.44, 'learning_rate': 1.2112351586035798e-06, 'epoch': 17.83}
{'padding_count': 35, 'loss': 1.3557, 'learning_rate': 1.2023746234272551e-06, 'epoch': 17.85}
{'padding_count': 35, 'loss': 1.5833, 'learning_rate': 1.1935140882509305e-06, 'epoch': 17.86}
{'padding_count': 35, 'loss': 1.6892, 'learning_rate': 1.1846535530746058e-06, 'epoch': 17.88}
{'padding_count': 35, 'loss': 1.284, 'learning_rate': 1.1757930178982812e-06, 'epoch': 17.89}
{'padding_count': 36, 'loss': 1.2303, 'learning_rate': 1.1669324827219566e-06, 'epoch': 17.91}
{'padding_count': 36, 'loss': 1.2781, 'learning_rate': 1.158071947545632e-06, 'epoch': 17.93}
{'padding_count': 36, 'loss': 1.3443, 'learning_rate': 1.1492114123693073e-06, 'epoch': 17.94}
{'padding_count': 36, 'loss': 1.2637, 'learning_rate': 1.1403508771929824e-06, 'epoch': 17.96}
{'padding_count': 36, 'loss': 1.3477, 'learning_rate': 1.131490342016658e-06, 'epoch': 17.97}
{'padding_count': 36, 'loss': 1.5415, 'learning_rate': 1.1226298068403333e-06, 'epoch': 17.99}
{'eval_loss': 3.540131092071533, 'eval_runtime': 43.1858, 'eval_samples_per_second': 6.298, 'eval_steps_per_second': 1.575, 'epoch': 18.0}
{'padding_count': 36, 'loss': 1.4176, 'learning_rate': 1.1137692716640087e-06, 'epoch': 18.01}
{'padding_count': 36, 'loss': 1.5418, 'learning_rate': 1.104908736487684e-06, 'epoch': 18.02}
{'padding_count': 36, 'loss': 1.4908, 'learning_rate': 1.0960482013113594e-06, 'epoch': 18.04}
{'padding_count': 36, 'loss': 1.5686, 'learning_rate': 1.0871876661350345e-06, 'epoch': 18.05}
{'padding_count': 36, 'loss': 1.5181, 'learning_rate': 1.07832713095871e-06, 'epoch': 18.07}
{'padding_count': 36, 'loss': 0.987, 'learning_rate': 1.0694665957823854e-06, 'epoch': 18.09}
{'padding_count': 36, 'loss': 1.3165, 'learning_rate': 1.0606060606060608e-06, 'epoch': 18.1}
{'padding_count': 36, 'loss': 1.662, 'learning_rate': 1.0517455254297361e-06, 'epoch': 18.12}
{'padding_count': 36, 'loss': 1.418, 'learning_rate': 1.0428849902534113e-06, 'epoch': 18.13}
{'padding_count': 36, 'loss': 1.3257, 'learning_rate': 1.0340244550770866e-06, 'epoch': 18.15}
{'padding_count': 36, 'loss': 1.333, 'learning_rate': 1.0251639199007622e-06, 'epoch': 18.17}
{'padding_count': 36, 'loss': 1.7038, 'learning_rate': 1.0163033847244375e-06, 'epoch': 18.18}
{'padding_count': 36, 'loss': 1.3441, 'learning_rate': 1.0074428495481129e-06, 'epoch': 18.2}
{'padding_count': 36, 'loss': 1.4538, 'learning_rate': 9.98582314371788e-07, 'epoch': 18.21}
{'padding_count': 36, 'loss': 1.2869, 'learning_rate': 9.897217791954634e-07, 'epoch': 18.23}
{'padding_count': 36, 'loss': 1.3545, 'learning_rate': 9.808612440191387e-07, 'epoch': 18.25}
{'padding_count': 36, 'loss': 0.9932, 'learning_rate': 9.720007088428143e-07, 'epoch': 18.26}
{'padding_count': 36, 'loss': 1.3414, 'learning_rate': 9.631401736664896e-07, 'epoch': 18.28}
{'padding_count': 36, 'loss': 1.1011, 'learning_rate': 9.54279638490165e-07, 'epoch': 18.29}
{'padding_count': 36, 'loss': 1.4628, 'learning_rate': 9.454191033138401e-07, 'epoch': 18.31}
{'padding_count': 36, 'loss': 1.3066, 'learning_rate': 9.365585681375156e-07, 'epoch': 18.33}
{'padding_count': 36, 'loss': 1.3111, 'learning_rate': 9.27698032961191e-07, 'epoch': 18.34}
{'padding_count': 36, 'loss': 1.4256, 'learning_rate': 9.188374977848663e-07, 'epoch': 18.36}
{'padding_count': 36, 'loss': 1.4491, 'learning_rate': 9.099769626085417e-07, 'epoch': 18.37}
{'padding_count': 36, 'loss': 1.0674, 'learning_rate': 9.011164274322169e-07, 'epoch': 18.39}
{'padding_count': 36, 'loss': 1.338, 'learning_rate': 8.922558922558923e-07, 'epoch': 18.41}
{'padding_count': 36, 'loss': 1.1215, 'learning_rate': 8.833953570795677e-07, 'epoch': 18.42}
{'padding_count': 36, 'loss': 1.0963, 'learning_rate': 8.745348219032431e-07, 'epoch': 18.44}
{'padding_count': 36, 'loss': 1.3856, 'learning_rate': 8.656742867269184e-07, 'epoch': 18.45}
{'padding_count': 36, 'loss': 1.3129, 'learning_rate': 8.568137515505938e-07, 'epoch': 18.47}
{'padding_count': 36, 'loss': 1.4266, 'learning_rate': 8.47953216374269e-07, 'epoch': 18.48}
{'padding_count': 36, 'loss': 1.0815, 'learning_rate': 8.390926811979444e-07, 'epoch': 18.5}
{'padding_count': 36, 'loss': 1.4435, 'learning_rate': 8.302321460216198e-07, 'epoch': 18.52}
{'padding_count': 36, 'loss': 1.152, 'learning_rate': 8.213716108452952e-07, 'epoch': 18.53}
{'padding_count': 36, 'loss': 1.4681, 'learning_rate': 8.125110756689705e-07, 'epoch': 18.55}
{'padding_count': 36, 'loss': 1.4322, 'learning_rate': 8.036505404926458e-07, 'epoch': 18.56}
{'padding_count': 36, 'loss': 1.4225, 'learning_rate': 7.947900053163211e-07, 'epoch': 18.58}
{'padding_count': 36, 'loss': 1.5703, 'learning_rate': 7.859294701399965e-07, 'epoch': 18.6}
{'padding_count': 36, 'loss': 1.6666, 'learning_rate': 7.770689349636719e-07, 'epoch': 18.61}
{'padding_count': 36, 'loss': 1.0652, 'learning_rate': 7.682083997873473e-07, 'epoch': 18.63}
{'padding_count': 36, 'loss': 1.2938, 'learning_rate': 7.593478646110225e-07, 'epoch': 18.64}
{'padding_count': 36, 'loss': 1.0883, 'learning_rate': 7.504873294346979e-07, 'epoch': 18.66}
{'padding_count': 36, 'loss': 1.0464, 'learning_rate': 7.416267942583732e-07, 'epoch': 18.68}
{'padding_count': 37, 'loss': 1.4331, 'learning_rate': 7.327662590820486e-07, 'epoch': 18.69}
{'padding_count': 37, 'loss': 1.2486, 'learning_rate': 7.23905723905724e-07, 'epoch': 18.71}
{'padding_count': 37, 'loss': 1.5711, 'learning_rate': 7.150451887293994e-07, 'epoch': 18.72}
{'padding_count': 37, 'loss': 1.6251, 'learning_rate': 7.061846535530746e-07, 'epoch': 18.74}
{'padding_count': 37, 'loss': 1.1618, 'learning_rate': 6.9732411837675e-07, 'epoch': 18.76}
{'padding_count': 37, 'loss': 1.4907, 'learning_rate': 6.884635832004253e-07, 'epoch': 18.77}
{'padding_count': 37, 'loss': 1.6136, 'learning_rate': 6.796030480241007e-07, 'epoch': 18.79}
{'padding_count': 37, 'loss': 1.2273, 'learning_rate': 6.707425128477762e-07, 'epoch': 18.8}
{'padding_count': 37, 'loss': 1.5277, 'learning_rate': 6.618819776714513e-07, 'epoch': 18.82}
{'padding_count': 37, 'loss': 1.115, 'learning_rate': 6.530214424951268e-07, 'epoch': 18.84}
{'padding_count': 37, 'loss': 1.5158, 'learning_rate': 6.441609073188021e-07, 'epoch': 18.85}
{'padding_count': 37, 'loss': 1.7008, 'learning_rate': 6.353003721424775e-07, 'epoch': 18.87}
{'padding_count': 37, 'loss': 1.3993, 'learning_rate': 6.264398369661528e-07, 'epoch': 18.88}
{'padding_count': 38, 'loss': 1.3049, 'learning_rate': 6.175793017898282e-07, 'epoch': 18.9}
{'padding_count': 38, 'loss': 1.2154, 'learning_rate': 6.087187666135035e-07, 'epoch': 18.92}
{'padding_count': 38, 'loss': 1.1409, 'learning_rate': 5.998582314371789e-07, 'epoch': 18.93}
{'padding_count': 38, 'loss': 1.4212, 'learning_rate': 5.909976962608542e-07, 'epoch': 18.95}
{'padding_count': 38, 'loss': 1.3013, 'learning_rate': 5.821371610845296e-07, 'epoch': 18.96}
{'padding_count': 38, 'loss': 1.3942, 'learning_rate': 5.732766259082049e-07, 'epoch': 18.98}
{'padding_count': 38, 'loss': 1.4548, 'learning_rate': 5.644160907318803e-07, 'epoch': 19.0}
{'eval_loss': 3.552457332611084, 'eval_runtime': 43.1415, 'eval_samples_per_second': 6.305, 'eval_steps_per_second': 1.576, 'epoch': 19.0}
{'padding_count': 38, 'loss': 1.4072, 'learning_rate': 5.555555555555555e-07, 'epoch': 19.01}
{'padding_count': 38, 'loss': 1.3807, 'learning_rate': 5.46695020379231e-07, 'epoch': 19.03}
{'padding_count': 38, 'loss': 1.5078, 'learning_rate': 5.378344852029063e-07, 'epoch': 19.04}
{'padding_count': 38, 'loss': 1.5664, 'learning_rate': 5.289739500265816e-07, 'epoch': 19.06}
{'padding_count': 38, 'loss': 1.3325, 'learning_rate': 5.20113414850257e-07, 'epoch': 19.07}
{'padding_count': 38, 'loss': 1.0847, 'learning_rate': 5.112528796739324e-07, 'epoch': 19.09}
{'padding_count': 38, 'loss': 1.4429, 'learning_rate': 5.023923444976076e-07, 'epoch': 19.11}
{'padding_count': 38, 'loss': 1.5496, 'learning_rate': 4.935318093212831e-07, 'epoch': 19.12}
{'padding_count': 38, 'loss': 1.3995, 'learning_rate': 4.846712741449583e-07, 'epoch': 19.14}
{'padding_count': 38, 'loss': 1.0861, 'learning_rate': 4.7581073896863374e-07, 'epoch': 19.15}
{'padding_count': 38, 'loss': 1.6482, 'learning_rate': 4.669502037923091e-07, 'epoch': 19.17}
{'padding_count': 38, 'loss': 1.6557, 'learning_rate': 4.5808966861598444e-07, 'epoch': 19.19}
{'padding_count': 38, 'loss': 1.2515, 'learning_rate': 4.492291334396598e-07, 'epoch': 19.2}
{'padding_count': 38, 'loss': 1.2257, 'learning_rate': 4.4036859826333515e-07, 'epoch': 19.22}
{'padding_count': 38, 'loss': 1.4233, 'learning_rate': 4.315080630870105e-07, 'epoch': 19.23}
{'padding_count': 38, 'loss': 1.197, 'learning_rate': 4.2264752791068585e-07, 'epoch': 19.25}
{'padding_count': 38, 'loss': 1.1567, 'learning_rate': 4.137869927343612e-07, 'epoch': 19.27}
{'padding_count': 38, 'loss': 1.1712, 'learning_rate': 4.0492645755803655e-07, 'epoch': 19.28}
{'padding_count': 38, 'loss': 1.3355, 'learning_rate': 3.960659223817119e-07, 'epoch': 19.3}
{'padding_count': 38, 'loss': 1.2422, 'learning_rate': 3.872053872053872e-07, 'epoch': 19.31}
{'padding_count': 38, 'loss': 1.3441, 'learning_rate': 3.783448520290626e-07, 'epoch': 19.33}
{'padding_count': 38, 'loss': 1.2816, 'learning_rate': 3.6948431685273796e-07, 'epoch': 19.35}
{'padding_count': 38, 'loss': 1.4286, 'learning_rate': 3.6062378167641326e-07, 'epoch': 19.36}
{'padding_count': 38, 'loss': 1.2845, 'learning_rate': 3.5176324650008867e-07, 'epoch': 19.38}
{'padding_count': 38, 'loss': 1.1759, 'learning_rate': 3.42902711323764e-07, 'epoch': 19.39}
{'padding_count': 38, 'loss': 1.2087, 'learning_rate': 3.340421761474393e-07, 'epoch': 19.41}
{'padding_count': 38, 'loss': 1.0052, 'learning_rate': 3.251816409711147e-07, 'epoch': 19.43}
{'padding_count': 38, 'loss': 1.2553, 'learning_rate': 3.1632110579479e-07, 'epoch': 19.44}
{'padding_count': 38, 'loss': 1.5154, 'learning_rate': 3.0746057061846537e-07, 'epoch': 19.46}
{'padding_count': 38, 'loss': 1.3183, 'learning_rate': 2.986000354421407e-07, 'epoch': 19.47}
{'padding_count': 38, 'loss': 1.2021, 'learning_rate': 2.897395002658161e-07, 'epoch': 19.49}
{'padding_count': 38, 'loss': 1.2897, 'learning_rate': 2.8087896508949143e-07, 'epoch': 19.51}
{'padding_count': 38, 'loss': 1.4369, 'learning_rate': 2.720184299131668e-07, 'epoch': 19.52}
{'padding_count': 38, 'loss': 1.2054, 'learning_rate': 2.6315789473684213e-07, 'epoch': 19.54}
{'padding_count': 38, 'loss': 1.4323, 'learning_rate': 2.542973595605175e-07, 'epoch': 19.55}
{'padding_count': 38, 'loss': 1.437, 'learning_rate': 2.4543682438419284e-07, 'epoch': 19.57}
{'padding_count': 38, 'loss': 1.4186, 'learning_rate': 2.3657628920786816e-07, 'epoch': 19.59}
{'padding_count': 38, 'loss': 1.5666, 'learning_rate': 2.2771575403154351e-07, 'epoch': 19.6}
{'padding_count': 38, 'loss': 1.5482, 'learning_rate': 2.188552188552189e-07, 'epoch': 19.62}
{'padding_count': 38, 'loss': 0.9919, 'learning_rate': 2.0999468367889422e-07, 'epoch': 19.63}
{'padding_count': 38, 'loss': 1.3232, 'learning_rate': 2.0113414850256957e-07, 'epoch': 19.65}
{'padding_count': 38, 'loss': 1.1193, 'learning_rate': 1.922736133262449e-07, 'epoch': 19.67}
{'padding_count': 38, 'loss': 1.0317, 'learning_rate': 1.8341307814992027e-07, 'epoch': 19.68}
{'padding_count': 39, 'loss': 1.3481, 'learning_rate': 1.7455254297359563e-07, 'epoch': 19.7}
{'padding_count': 39, 'loss': 1.2955, 'learning_rate': 1.6569200779727095e-07, 'epoch': 19.71}
{'padding_count': 39, 'loss': 1.5732, 'learning_rate': 1.5683147262094633e-07, 'epoch': 19.73}
{'padding_count': 39, 'loss': 1.5693, 'learning_rate': 1.4797093744462168e-07, 'epoch': 19.74}
{'padding_count': 39, 'loss': 1.0777, 'learning_rate': 1.39110402268297e-07, 'epoch': 19.76}
{'padding_count': 39, 'loss': 1.5662, 'learning_rate': 1.3024986709197238e-07, 'epoch': 19.78}
{'padding_count': 39, 'loss': 1.5357, 'learning_rate': 1.213893319156477e-07, 'epoch': 19.79}
{'padding_count': 39, 'loss': 1.2229, 'learning_rate': 1.1252879673932308e-07, 'epoch': 19.81}
{'padding_count': 39, 'loss': 1.295, 'learning_rate': 1.0366826156299841e-07, 'epoch': 19.82}
{'padding_count': 39, 'loss': 1.3247, 'learning_rate': 9.480772638667377e-08, 'epoch': 19.84}
{'padding_count': 39, 'loss': 1.6501, 'learning_rate': 8.59471912103491e-08, 'epoch': 19.86}
{'padding_count': 39, 'loss': 1.6004, 'learning_rate': 7.708665603402446e-08, 'epoch': 19.87}
{'padding_count': 39, 'loss': 1.1924, 'learning_rate': 6.822612085769981e-08, 'epoch': 19.89}
{'padding_count': 40, 'loss': 1.3637, 'learning_rate': 5.9365585681375154e-08, 'epoch': 19.9}
{'padding_count': 40, 'loss': 1.0951, 'learning_rate': 5.050505050505051e-08, 'epoch': 19.92}
{'padding_count': 40, 'loss': 1.163, 'learning_rate': 4.1644515328725864e-08, 'epoch': 19.94}
{'padding_count': 40, 'loss': 1.4038, 'learning_rate': 3.278398015240121e-08, 'epoch': 19.95}
{'padding_count': 40, 'loss': 1.31, 'learning_rate': 2.3923444976076555e-08, 'epoch': 19.97}
{'padding_count': 40, 'loss': 1.2179, 'learning_rate': 1.594896331738437e-08, 'epoch': 19.98}
{'padding_count': 40, 'loss': 1.5921, 'learning_rate': 7.088428141059721e-09, 'epoch': 20.0}
{'eval_loss': 3.5514867305755615, 'eval_runtime': 43.3894, 'eval_samples_per_second': 6.269, 'eval_steps_per_second': 1.567, 'epoch': 20.0}
{'train_runtime': 24581.1114, 'train_samples_per_second': 1.023, 'train_steps_per_second': 0.51, 'train_loss': 2.011363101499883, 'epoch': 20.0}
***** train metrics *****
  epoch                    =       20.0
  train_loss               =     2.0114
  train_runtime            = 6:49:41.11
  train_samples            =       1257
  train_samples_per_second =      1.023
  train_steps_per_second   =       0.51
***** eval metrics *****
  epoch                   =       20.0
  eval_loss               =     3.5515
  eval_runtime            = 0:00:42.98
  eval_samples            =        272
  eval_samples_per_second =      6.328
  eval_steps_per_second   =      1.582
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                                   | 0/12540 [00:00<?, ?it/s][INFO|trainer.py:1411] 2022-06-08 13:14:17,828 >> Unused parameters:
/home/s1970716/miniconda3/envs/scrolls_venv/lib/python3.8/site-packages/transformers/trainer.py:1444: FutureWarning:
Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  0%|                                                                                        | 1/12540 [00:04<14:26:13,  4.14s/it]2022-06-08 13:14:18 | INFO | root | Reducer buckets have been rebuilt in this iteration.
  0%|                                                                                       | 24/12540 [00:49<6:15:43,  1.80s/it][INFO|modeling_led.py:1663] 2022-06-08 13:15:03,438 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
  1%|                                                                                       | 72/12540 [02:18<6:19:53,  1.83s/it][INFO|modeling_led.py:1663] 2022-06-08 13:16:32,709 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
  1%|                                                                                       | 90/12540 [02:52<6:33:37,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 13:17:06,740 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
  1%|                                                                                      | 157/12540 [04:59<6:38:43,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 13:19:12,919 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
  1%|                                                                                     | 177/12540 [05:36<6:16:25,  1.83s/it][INFO|modeling_led.py:1663] 2022-06-08 13:19:50,118 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
  1%|                                                                                     | 182/12540 [05:45<6:33:49,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 13:19:59,631 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
  2%|                                                                                     | 190/12540 [06:00<6:22:41,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 13:20:14,583 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
  2%|                                                                                     | 199/12540 [06:17<6:30:42,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 13:20:31,423 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
  2%|                                                                                     | 236/12540 [07:27<6:21:43,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 13:21:40,994 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
  2%|                                                                                     | 272/12540 [08:34<6:32:35,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 13:22:48,772 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
  2%|                                                                                    | 313/12540 [09:51<6:21:38,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 13:24:05,738 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
  3%|                                                                                    | 319/12540 [10:03<6:31:59,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 13:24:17,178 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
  3%|                                                                                    | 332/12540 [10:28<6:31:49,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 13:24:41,980 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
  3%|                                                                                    | 334/12540 [10:31<6:25:44,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 13:24:45,709 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
  3%|                                                                                    | 375/12540 [11:50<6:29:38,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 13:26:03,910 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
  4%|                                                                                   | 458/12540 [14:26<6:21:01,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 13:28:40,809 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
  4%|                                                                                   | 502/12540 [15:49<6:10:48,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 13:30:03,428 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
  4%|                                                                                   | 563/12540 [17:44<6:09:12,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 13:31:58,479 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
  5%|                                                                                  | 605/12540 [19:04<6:18:08,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 13:33:18,106 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
  5%|                                                                                  | 609/12540 [19:11<6:12:39,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 13:33:25,598 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
  5%|                                                                                  | 614/12540 [19:20<6:06:10,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 13:33:34,782 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
  5%|                                                                                  | 616/12540 [19:24<6:13:02,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 13:33:38,568 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
  5%|                                                                                  | 627/12540 [19:45<6:17:56,  1.90s/it][INFO|trainer.py:528] 2022-06-08 13:33:59,678 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 13:33:59,683 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 13:33:59,683 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 13:33:59,684 >>   Batch size = 2
  5%|                                                                                  | 651/12540 [21:14<6:14:08,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 13:35:28,293 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
  6%|                                                                                  | 699/12540 [22:45<6:10:46,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 13:36:59,190 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
  6%|                                                                                  | 717/12540 [23:18<6:13:04,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 13:37:32,820 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
  6%|                                                                                 | 784/12540 [25:24<6:09:54,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 13:39:38,446 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
  6%|                                                                                 | 804/12540 [26:03<6:19:07,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 13:40:16,874 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
  6%|                                                                                 | 809/12540 [26:12<6:04:34,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 13:40:26,039 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
  7%|                                                                                 | 817/12540 [26:27<6:04:20,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 13:40:40,987 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
  7%|                                                                                 | 826/12540 [26:44<6:13:38,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 13:40:58,043 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
  7%|                                                                                 | 863/12540 [27:53<6:11:26,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 13:42:07,884 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
  7%|                                                                                | 899/12540 [29:02<6:10:34,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 13:43:15,989 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
  7%|                                                                                | 940/12540 [30:19<6:12:09,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 13:44:33,502 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
  8%|                                                                                | 946/12540 [30:30<5:58:59,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 13:44:44,709 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
  8%|                                                                                | 959/12540 [30:55<6:08:16,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 13:45:09,332 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
  8%|                                                                                | 961/12540 [30:59<6:05:07,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 13:45:13,094 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
  8%|                                                                               | 1002/12540 [32:16<6:13:09,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 13:46:30,534 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
  9%|                                                                              | 1085/12540 [34:53<6:12:59,  1.95s/it][INFO|modeling_led.py:1663] 2022-06-08 13:49:07,371 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
  9%|                                                                              | 1129/12540 [36:17<6:02:54,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 13:50:30,937 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
  9%|                                                                             | 1190/12540 [38:12<5:44:59,  1.82s/it][INFO|modeling_led.py:1663] 2022-06-08 13:52:25,916 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 10%|                                                                             | 1232/12540 [39:31<5:52:44,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 13:53:44,970 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 10%|                                                                             | 1236/12540 [39:38<6:03:29,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 13:53:52,728 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 10%|                                                                             | 1241/12540 [39:48<5:57:34,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 13:54:02,216 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 10%|                                                                             | 1243/12540 [39:52<5:57:59,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 13:54:06,019 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 10%|                                                                             | 1254/12540 [40:13<6:00:44,  1.92s/it][INFO|trainer.py:528] 2022-06-08 13:54:26,899 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 13:54:26,916 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 13:54:26,916 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 13:54:26,917 >>   Batch size = 2
 10%|                                                                             | 1278/12540 [41:42<6:00:22,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 13:55:56,304 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 11%|                                                                             | 1326/12540 [43:12<5:52:32,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 13:57:26,690 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 11%|                                                                            | 1344/12540 [43:46<5:49:18,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 13:58:00,192 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 11%|                                                                            | 1411/12540 [45:52<5:52:44,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 14:00:06,855 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 11%|                                                                            | 1431/12540 [46:31<6:01:30,  1.95s/it][INFO|modeling_led.py:1663] 2022-06-08 14:00:44,931 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 11%|                                                                            | 1436/12540 [46:40<5:53:28,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 14:00:54,367 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 12%|                                                                            | 1444/12540 [46:55<5:53:36,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 14:01:09,375 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 12%|                                                                            | 1453/12540 [47:12<5:43:48,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 14:01:26,197 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 12%|                                                                           | 1490/12540 [48:22<5:46:11,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 14:02:36,793 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 12%|                                                                           | 1526/12540 [49:30<5:43:24,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:03:44,912 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 12%|                                                                           | 1567/12540 [50:48<5:44:55,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:05:02,609 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 13%|                                                                           | 1573/12540 [51:00<5:45:33,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:05:13,986 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 13%|                                                                           | 1586/12540 [51:24<5:45:57,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 14:05:38,851 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 13%|                                                                           | 1588/12540 [51:28<5:37:35,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 14:05:42,456 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 13%|                                                                          | 1629/12540 [52:46<5:43:39,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:07:00,327 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 14%|                                                                          | 1712/12540 [55:24<5:45:33,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 14:09:38,141 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 14%|                                                                          | 1756/12540 [56:47<5:35:03,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 14:11:00,941 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 14%|                                                                         | 1817/12540 [58:42<5:39:24,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 14:12:56,470 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 15%|                                                                       | 1859/12540 [1:00:01<5:34:43,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 14:14:15,787 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 15%|                                                                       | 1863/12540 [1:00:09<5:41:20,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 14:14:23,490 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 15%|                                                                       | 1868/12540 [1:00:18<5:34:47,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 14:14:32,847 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 15%|                                                                       | 1870/12540 [1:00:22<5:33:26,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 14:14:36,596 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 15%|                                                                       | 1881/12540 [1:00:43<5:38:01,  1.90s/it][INFO|trainer.py:528] 2022-06-08 14:14:57,472 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 14:14:57,490 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 14:14:57,490 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 14:14:57,491 >>   Batch size = 2
 15%|                                                                       | 1905/12540 [1:02:12<5:39:28,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 14:16:26,658 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 16%|                                                                       | 1953/12540 [1:03:43<5:28:04,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 14:17:57,029 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 16%|                                                                      | 1971/12540 [1:04:17<5:35:20,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 14:18:30,907 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 16%|                                                                      | 2038/12540 [1:06:22<5:32:10,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 14:20:36,833 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 16%|                                                                      | 2058/12540 [1:07:00<5:36:27,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 14:21:14,498 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 16%|                                                                      | 2063/12540 [1:07:10<5:38:11,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 14:21:24,167 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 17%|                                                                      | 2071/12540 [1:07:25<5:28:45,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 14:21:39,222 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 17%|                                                                      | 2080/12540 [1:07:42<5:25:13,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:21:56,110 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 17%|                                                                     | 2117/12540 [1:08:51<5:25:05,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:23:05,635 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 17%|                                                                     | 2153/12540 [1:09:59<5:18:40,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 14:24:13,457 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 17%|                                                                     | 2194/12540 [1:11:17<5:26:03,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:25:30,976 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 18%|                                                                     | 2200/12540 [1:11:28<5:30:11,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 14:25:42,436 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 18%|                                                                     | 2213/12540 [1:11:53<5:37:35,  1.96s/it][INFO|modeling_led.py:1663] 2022-06-08 14:26:07,424 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 18%|                                                                     | 2215/12540 [1:11:57<5:32:14,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 14:26:11,218 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 18%|                                                                     | 2256/12540 [1:13:15<5:30:15,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 14:27:29,039 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 19%|                                                                    | 2339/12540 [1:15:51<5:17:58,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:30:05,884 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 19%|                                                                    | 2383/12540 [1:17:15<5:17:15,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:31:29,339 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 19%|                                                                   | 2444/12540 [1:19:10<5:14:08,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:33:24,267 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 20%|                                                                   | 2486/12540 [1:20:29<5:20:21,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 14:34:43,891 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 20%|                                                                   | 2490/12540 [1:20:37<5:15:36,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 14:34:51,379 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 20%|                                                                   | 2495/12540 [1:20:46<5:17:11,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:35:00,860 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 20%|                                                                   | 2497/12540 [1:20:50<5:17:43,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 14:35:04,644 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 20%|                                                                   | 2508/12540 [1:21:11<5:16:58,  1.90s/it][INFO|trainer.py:528] 2022-06-08 14:35:25,518 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 14:35:25,523 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 14:35:25,536 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 14:35:25,536 >>   Batch size = 2
 20%|                                                                   | 2532/12540 [1:22:41<5:12:39,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:36:55,098 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 21%|                                                                  | 2580/12540 [1:24:11<5:14:28,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:38:25,580 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 21%|                                                                  | 2598/12540 [1:24:45<5:13:12,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:38:59,895 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 21%|                                                                  | 2665/12540 [1:26:52<5:14:14,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 14:41:06,178 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 21%|                                                                  | 2685/12540 [1:27:30<5:15:13,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 14:41:44,421 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 21%|                                                                  | 2690/12540 [1:27:40<5:19:05,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 14:41:54,237 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 22%|                                                                  | 2698/12540 [1:27:55<5:16:52,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 14:42:09,571 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 22%|                                                                 | 2707/12540 [1:28:12<5:09:46,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:42:26,512 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 22%|                                                                 | 2744/12540 [1:29:22<5:07:59,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:43:36,310 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 22%|                                                                 | 2780/12540 [1:30:30<5:04:08,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:44:44,301 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 22%|                                                                 | 2821/12540 [1:31:47<5:04:13,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 14:46:01,478 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 23%|                                                                 | 2827/12540 [1:31:58<4:58:03,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 14:46:12,624 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 23%|                                                                 | 2840/12540 [1:32:23<5:03:45,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 14:46:37,384 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 23%|                                                                 | 2842/12540 [1:32:27<5:05:28,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:46:41,190 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 23%|                                                                | 2883/12540 [1:33:44<5:07:05,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 14:47:58,508 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 24%|                                                                | 2966/12540 [1:36:21<5:02:11,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:50:35,074 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 24%|                                                               | 3010/12540 [1:37:44<4:53:38,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 14:51:57,986 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 24%|                                                               | 3071/12540 [1:39:38<4:54:52,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:53:52,390 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 25%|                                                               | 3113/12540 [1:40:57<4:57:07,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:55:11,203 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 25%|                                                               | 3117/12540 [1:41:04<4:53:14,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 14:55:18,628 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 25%|                                                               | 3122/12540 [1:41:14<4:51:12,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 14:55:27,981 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 25%|                                                               | 3124/12540 [1:41:17<4:50:48,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 14:55:31,663 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 25%|                                                               | 3135/12540 [1:41:38<4:59:05,  1.91s/it][INFO|trainer.py:528] 2022-06-08 14:55:52,495 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 14:55:52,512 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 14:55:52,512 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 14:55:52,513 >>   Batch size = 2
 25%|                                                              | 3159/12540 [1:43:07<4:50:36,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 14:57:21,444 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 26%|                                                              | 3207/12540 [1:44:37<4:54:11,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 14:58:51,105 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 26%|                                                              | 3225/12540 [1:45:11<4:49:02,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 14:59:25,321 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 26%|                                                              | 3292/12540 [1:47:18<4:52:15,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:01:32,005 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 26%|                                                             | 3312/12540 [1:47:55<4:53:48,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:02:09,499 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 26%|                                                             | 3317/12540 [1:48:05<4:52:39,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:02:18,944 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 27%|                                                             | 3325/12540 [1:48:20<4:50:31,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:02:33,963 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 27%|                                                             | 3334/12540 [1:48:36<4:48:16,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 15:02:50,878 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 27%|                                                             | 3371/12540 [1:49:47<4:48:04,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:04:01,337 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 27%|                                                             | 3407/12540 [1:50:55<4:47:12,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:05:09,614 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 27%|                                                             | 3448/12540 [1:52:13<4:45:44,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:06:27,505 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 28%|                                                            | 3454/12540 [1:52:24<4:47:24,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:06:38,831 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 28%|                                                            | 3467/12540 [1:52:49<4:45:32,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:07:03,426 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 28%|                                                            | 3469/12540 [1:52:53<4:43:55,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 15:07:07,153 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 28%|                                                            | 3510/12540 [1:54:11<4:41:40,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 15:08:25,243 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 29%|                                                            | 3593/12540 [1:56:47<4:44:04,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:11:01,783 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 29%|                                                           | 3637/12540 [1:58:11<4:41:41,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:12:25,098 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 29%|                                                           | 3698/12540 [2:00:06<4:35:48,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 15:14:20,794 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 30%|                                                           | 3740/12540 [2:01:26<4:38:14,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:15:40,271 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 30%|                                                           | 3744/12540 [2:01:34<4:40:05,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:15:48,047 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 30%|                                                           | 3749/12540 [2:01:43<4:36:41,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:15:57,501 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 30%|                                                          | 3751/12540 [2:01:47<4:40:23,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:16:01,391 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 30%|                                                          | 3762/12540 [2:02:08<4:38:50,  1.91s/it][INFO|trainer.py:528] 2022-06-08 15:16:22,163 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 15:16:22,168 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 15:16:22,168 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 15:16:22,169 >>   Batch size = 2
 30%|                                                          | 3786/12540 [2:03:37<4:39:37,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 15:17:51,698 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 31%|                                                          | 3834/12540 [2:05:07<4:31:45,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 15:19:21,217 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 31%|                                                          | 3852/12540 [2:05:41<4:35:41,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:19:54,980 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 31%|                                                         | 3919/12540 [2:07:47<4:32:04,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:22:01,777 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 31%|                                                         | 3939/12540 [2:08:25<4:30:15,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:22:39,441 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 31%|                                                         | 3944/12540 [2:08:35<4:34:06,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:22:48,995 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 32%|                                                         | 3952/12540 [2:08:50<4:31:55,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:23:04,078 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 32%|                                                         | 3961/12540 [2:09:07<4:30:03,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:23:20,903 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 32%|                                                         | 3998/12540 [2:10:17<4:30:08,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:24:31,464 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 32%|                                                         | 4034/12540 [2:11:25<4:29:35,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:25:39,821 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 32%|                                                        | 4075/12540 [2:12:43<4:29:29,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:26:57,146 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 33%|                                                        | 4081/12540 [2:12:54<4:21:58,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 15:27:08,402 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 33%|                                                        | 4094/12540 [2:13:19<4:35:33,  1.96s/it][INFO|modeling_led.py:1663] 2022-06-08 15:27:33,450 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 33%|                                                        | 4096/12540 [2:13:23<4:28:12,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:27:37,167 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 33%|                                                        | 4137/12540 [2:14:40<4:29:01,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 15:28:54,854 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 34%|                                                       | 4220/12540 [2:17:18<4:22:04,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:31:32,429 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 34%|                                                       | 4264/12540 [2:18:42<4:24:53,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 15:32:55,929 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 34%|                                                       | 4325/12540 [2:20:37<4:16:10,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 15:34:51,528 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 35%|                                                      | 4367/12540 [2:21:56<4:17:19,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:36:10,373 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 35%|                                                      | 4371/12540 [2:22:04<4:21:39,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 15:36:18,061 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 35%|                                                      | 4376/12540 [2:22:13<4:18:02,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:36:27,559 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 35%|                                                      | 4378/12540 [2:22:17<4:19:34,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:36:31,411 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 35%|                                                      | 4389/12540 [2:22:38<4:21:04,  1.92s/it][INFO|trainer.py:528] 2022-06-08 15:36:52,393 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 15:36:52,397 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 15:36:52,397 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 15:36:52,398 >>   Batch size = 2
 35%|                                                      | 4413/12540 [2:24:07<4:07:45,  1.83s/it][INFO|modeling_led.py:1663] 2022-06-08 15:38:21,009 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 36%|                                                      | 4461/12540 [2:25:37<4:11:02,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 15:39:50,974 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 36%|                                                      | 4479/12540 [2:26:11<4:09:40,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 15:40:24,963 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 36%|                                                     | 4546/12540 [2:28:16<4:17:38,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 15:42:30,614 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 36%|                                                     | 4566/12540 [2:28:54<4:10:14,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 15:43:08,437 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 36%|                                                     | 4571/12540 [2:29:03<4:09:58,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 15:43:17,709 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 37%|                                                     | 4579/12540 [2:29:18<4:06:32,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 15:43:32,683 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 37%|                                                     | 4588/12540 [2:29:35<4:09:34,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 15:43:49,489 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 37%|                                                     | 4625/12540 [2:30:45<4:10:39,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:44:59,390 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 37%|                                                    | 4661/12540 [2:31:53<4:07:33,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:46:07,366 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 37%|                                                    | 4702/12540 [2:33:10<4:07:35,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:47:24,869 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 38%|                                                    | 4708/12540 [2:33:22<4:06:32,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:47:36,230 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 38%|                                                    | 4721/12540 [2:33:46<4:07:24,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 15:48:00,576 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 38%|                                                    | 4723/12540 [2:33:50<4:09:46,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 15:48:04,436 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 38%|                                                    | 4764/12540 [2:35:08<4:04:23,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:49:22,344 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 39%|                                                   | 4847/12540 [2:37:45<4:02:14,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:51:59,296 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 39%|                                                   | 4891/12540 [2:39:08<3:59:20,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 15:53:22,035 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 39%|                                                  | 4952/12540 [2:41:03<3:50:00,  1.82s/it][INFO|modeling_led.py:1663] 2022-06-08 15:55:17,007 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 40%|                                                  | 4994/12540 [2:42:22<3:57:25,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:56:36,417 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 40%|                                                  | 4998/12540 [2:42:30<3:59:29,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 15:56:44,061 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 40%|                                                  | 5003/12540 [2:42:39<3:57:34,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 15:56:53,440 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 40%|                                                  | 5005/12540 [2:42:43<3:54:30,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 15:56:57,156 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 40%|                                                  | 5016/12540 [2:43:04<3:59:55,  1.91s/it][INFO|trainer.py:528] 2022-06-08 15:57:18,099 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 15:57:18,103 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 15:57:18,104 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 15:57:18,104 >>   Batch size = 2
 40%|                                                  | 5040/12540 [2:44:33<3:51:55,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 15:58:47,340 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 41%|                                                  | 5088/12540 [2:46:04<4:00:55,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 16:00:18,256 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 41%|                                                 | 5106/12540 [2:46:38<3:53:53,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:00:52,241 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 41%|                                                 | 5173/12540 [2:48:45<3:57:22,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 16:02:59,329 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 41%|                                                 | 5193/12540 [2:49:22<3:49:43,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:03:36,647 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 41%|                                                 | 5198/12540 [2:49:32<3:52:44,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:03:46,043 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 42%|                                                 | 5206/12540 [2:49:47<3:51:01,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:04:01,170 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 42%|                                                 | 5215/12540 [2:50:04<3:49:46,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:04:18,301 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 42%|                                                | 5252/12540 [2:51:14<3:50:24,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:05:28,103 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 42%|                                                | 5288/12540 [2:52:21<3:51:04,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:06:35,896 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 42%|                                                | 5329/12540 [2:53:39<3:46:52,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:07:53,018 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 43%|                                                | 5335/12540 [2:53:50<3:45:22,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:08:04,182 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 43%|                                                | 5348/12540 [2:54:14<3:43:56,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 16:08:28,609 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 43%|                                                | 5350/12540 [2:54:18<3:43:37,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 16:08:32,374 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 43%|                                                | 5391/12540 [2:55:36<3:48:47,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 16:09:50,022 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 44%|                                               | 5474/12540 [2:58:13<3:48:03,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 16:12:27,312 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 44%|                                               | 5518/12540 [2:59:36<3:41:17,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:13:50,517 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 44%|                                              | 5579/12540 [3:01:31<3:39:29,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:15:45,280 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 45%|                                              | 5621/12540 [3:02:50<3:40:53,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 16:17:04,847 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 45%|                                              | 5625/12540 [3:02:58<3:34:45,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 16:17:12,367 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 45%|                                              | 5630/12540 [3:03:07<3:31:55,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 16:17:21,589 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 45%|                                              | 5632/12540 [3:03:11<3:31:21,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 16:17:25,219 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 45%|                                              | 5643/12540 [3:03:32<3:37:00,  1.89s/it][INFO|trainer.py:528] 2022-06-08 16:17:46,074 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 16:17:46,079 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 16:17:46,092 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 16:17:46,092 >>   Batch size = 2
 45%|                                              | 5667/12540 [3:05:01<3:35:10,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:19:15,542 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 46%|                                             | 5715/12540 [3:06:31<3:32:45,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 16:20:45,343 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 46%|                                             | 5733/12540 [3:07:05<3:29:11,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 16:21:19,448 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 46%|                                             | 5800/12540 [3:09:12<3:28:55,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 16:23:25,903 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 46%|                                             | 5820/12540 [3:09:50<3:33:06,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:24:03,953 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 46%|                                             | 5825/12540 [3:09:59<3:33:39,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:24:13,509 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 47%|                                             | 5833/12540 [3:10:14<3:31:56,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:24:28,436 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 47%|                                            | 5842/12540 [3:10:31<3:25:47,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 16:24:45,163 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 47%|                                            | 5879/12540 [3:11:41<3:32:20,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:25:55,251 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 47%|                                            | 5915/12540 [3:12:49<3:32:10,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 16:27:03,100 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 47%|                                            | 5956/12540 [3:14:06<3:27:52,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:28:20,193 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 48%|                                            | 5962/12540 [3:14:17<3:28:02,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:28:31,540 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 48%|                                            | 5975/12540 [3:14:42<3:25:42,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:28:56,179 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 48%|                                            | 5977/12540 [3:14:46<3:29:39,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 16:29:00,064 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 48%|                                           | 6018/12540 [3:16:03<3:26:17,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:30:17,599 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 49%|                                           | 6101/12540 [3:18:40<3:25:08,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:32:54,475 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 49%|                                          | 6145/12540 [3:20:03<3:23:19,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:34:17,866 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 49%|                                          | 6206/12540 [3:21:58<3:16:39,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 16:36:12,424 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 50%|                                          | 6248/12540 [3:23:17<3:16:16,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 16:37:31,295 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 50%|                                          | 6252/12540 [3:23:25<3:18:31,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:37:38,946 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 50%|                                          | 6257/12540 [3:23:34<3:15:13,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 16:37:48,367 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 50%|                                          | 6259/12540 [3:23:38<3:18:55,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:37:52,221 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 50%|                                          | 6270/12540 [3:23:59<3:21:34,  1.93s/it][INFO|trainer.py:528] 2022-06-08 16:38:13,125 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 16:38:13,129 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 16:38:13,129 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 16:38:13,130 >>   Batch size = 2
 50%|                                         | 6294/12540 [3:25:28<3:19:12,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:39:42,798 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 51%|                                         | 6342/12540 [3:26:58<3:16:48,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:41:12,850 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 51%|                                         | 6360/12540 [3:27:32<3:17:25,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 16:41:46,866 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 51%|                                         | 6427/12540 [3:29:38<3:13:39,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:43:52,863 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 51%|                                        | 6447/12540 [3:30:16<3:11:15,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:44:30,446 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 51%|                                        | 6452/12540 [3:30:26<3:13:34,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:44:39,912 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 52%|                                        | 6460/12540 [3:30:41<3:11:09,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:44:54,982 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 52%|                                        | 6469/12540 [3:30:58<3:11:02,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:45:12,158 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 52%|                                        | 6506/12540 [3:32:08<3:10:20,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:46:22,654 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 52%|                                        | 6542/12540 [3:33:17<3:11:11,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:47:31,271 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 52%|                                        | 6583/12540 [3:34:35<3:09:46,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 16:48:49,155 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 53%|                                       | 6589/12540 [3:34:46<3:10:24,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 16:49:00,767 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 53%|                                       | 6602/12540 [3:35:11<3:04:17,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 16:49:24,964 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 53%|                                       | 6604/12540 [3:35:14<3:05:35,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:49:28,804 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 53%|                                       | 6645/12540 [3:36:32<3:06:19,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:50:45,923 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 54%|                                       | 6728/12540 [3:39:09<3:00:48,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 16:53:23,478 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 54%|                                      | 6772/12540 [3:40:32<3:01:46,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 16:54:46,516 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 54%|                                      | 6833/12540 [3:42:26<2:54:48,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 16:56:40,832 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 55%|                                      | 6875/12540 [3:43:45<2:57:32,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:57:59,787 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 55%|                                      | 6879/12540 [3:43:53<2:59:21,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 16:58:07,391 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 55%|                                      | 6884/12540 [3:44:02<2:57:40,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 16:58:16,730 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 55%|                                     | 6886/12540 [3:44:06<2:56:29,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 16:58:20,438 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 55%|                                     | 6897/12540 [3:44:27<3:00:29,  1.92s/it][INFO|trainer.py:528] 2022-06-08 16:58:41,284 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 16:58:41,289 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 16:58:41,300 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 16:58:41,301 >>   Batch size = 2
 55%|                                     | 6921/12540 [3:45:55<2:57:46,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:00:09,567 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 56%|                                     | 6969/12540 [3:47:25<2:51:48,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 17:01:39,369 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 56%|                                     | 6987/12540 [3:47:59<2:56:56,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:02:13,450 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 56%|                                    | 7054/12540 [3:50:05<2:51:14,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:04:19,774 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 56%|                                    | 7074/12540 [3:50:43<2:53:16,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:04:57,619 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 56%|                                    | 7079/12540 [3:50:53<2:51:16,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:05:06,937 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 57%|                                    | 7087/12540 [3:51:07<2:50:07,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:05:21,810 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 57%|                                    | 7096/12540 [3:51:24<2:49:54,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:05:38,621 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 57%|                                    | 7133/12540 [3:52:34<2:51:30,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:06:48,513 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 57%|                                    | 7169/12540 [3:53:42<2:48:38,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:07:56,051 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 57%|                                   | 7210/12540 [3:55:00<2:49:48,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:09:14,070 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 58%|                                   | 7216/12540 [3:55:11<2:45:56,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:09:25,295 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 58%|                                   | 7229/12540 [3:55:35<2:46:34,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:09:49,645 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 58%|                                   | 7231/12540 [3:55:39<2:50:22,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 17:09:53,573 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 58%|                                   | 7272/12540 [3:56:57<2:45:57,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 17:11:10,924 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 59%|                                  | 7355/12540 [3:59:33<2:43:41,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 17:13:47,775 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 59%|                                  | 7399/12540 [4:00:56<2:45:58,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 17:15:10,605 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 59%|                                  | 7460/12540 [4:02:51<2:40:38,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:17:05,595 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 60%|                                 | 7502/12540 [4:04:10<2:36:38,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:18:24,682 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 60%|                                 | 7506/12540 [4:04:18<2:37:16,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:18:32,294 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 60%|                                 | 7511/12540 [4:04:27<2:36:49,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:18:41,712 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 60%|                                 | 7513/12540 [4:04:31<2:39:24,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:18:45,551 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 60%|                                 | 7524/12540 [4:04:52<2:34:18,  1.85s/it][INFO|trainer.py:528] 2022-06-08 17:19:06,009 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 17:19:06,018 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 17:19:06,030 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 17:19:06,030 >>   Batch size = 2
 60%|                                 | 7548/12540 [4:06:20<2:35:10,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:20:34,793 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 61%|                                 | 7596/12540 [4:07:50<2:34:22,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:22:04,802 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 61%|                                 | 7614/12540 [4:08:24<2:35:06,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 17:22:38,434 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 61%|                                | 7681/12540 [4:10:31<2:33:36,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:24:45,373 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 61%|                                | 7701/12540 [4:11:09<2:33:26,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:25:23,027 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 61%|                                | 7706/12540 [4:11:18<2:31:25,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:25:32,368 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 62%|                                | 7714/12540 [4:11:33<2:33:18,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:25:47,593 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 62%|                                | 7723/12540 [4:11:50<2:28:35,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 17:26:04,275 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 62%|                                | 7760/12540 [4:13:00<2:32:00,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:27:14,036 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 62%|                               | 7796/12540 [4:14:08<2:31:52,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 17:28:21,995 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 62%|                               | 7837/12540 [4:15:25<2:27:27,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:29:39,408 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 63%|                               | 7843/12540 [4:15:36<2:28:45,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:29:50,755 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 63%|                               | 7856/12540 [4:16:01<2:28:37,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:30:15,439 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 63%|                               | 7858/12540 [4:16:05<2:27:10,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 17:30:19,164 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 63%|                               | 7899/12540 [4:17:22<2:29:33,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 17:31:36,433 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 64%|                              | 7982/12540 [4:19:59<2:25:03,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:34:13,165 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 64%|                              | 8026/12540 [4:21:22<2:23:50,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:35:36,118 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 64%|                             | 8087/12540 [4:23:17<2:20:34,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 17:37:31,345 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 65%|                             | 8129/12540 [4:24:36<2:19:29,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:38:50,847 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 65%|                             | 8133/12540 [4:24:44<2:17:45,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:38:58,322 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 65%|                             | 8138/12540 [4:24:53<2:14:49,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 17:39:07,511 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 65%|                             | 8140/12540 [4:24:57<2:18:29,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 17:39:11,336 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 65%|                             | 8151/12540 [4:25:17<2:16:05,  1.86s/it][INFO|trainer.py:528] 2022-06-08 17:39:31,865 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 17:39:31,869 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 17:39:31,870 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 17:39:31,870 >>   Batch size = 2
 65%|                             | 8175/12540 [4:26:47<2:20:01,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 17:41:01,247 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 66%|                             | 8223/12540 [4:28:16<2:12:08,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 17:42:30,427 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 66%|                            | 8241/12540 [4:28:50<2:15:52,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:43:04,316 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 66%|                            | 8308/12540 [4:30:57<2:13:25,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 17:45:10,942 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 66%|                            | 8328/12540 [4:31:34<2:11:33,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:45:48,412 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 66%|                            | 8333/12540 [4:31:43<2:09:14,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 17:45:57,649 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 67%|                            | 8341/12540 [4:31:58<2:13:31,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:46:12,858 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 67%|                            | 8350/12540 [4:32:15<2:11:23,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:46:29,647 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 67%|                           | 8387/12540 [4:33:25<2:12:14,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:47:39,291 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 67%|                           | 8423/12540 [4:34:33<2:10:21,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:48:46,948 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 67%|                           | 8464/12540 [4:35:49<2:07:38,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:50:03,451 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 68%|                           | 8470/12540 [4:36:01<2:09:05,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:50:14,988 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 68%|                           | 8483/12540 [4:36:25<2:09:35,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 17:50:39,662 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 68%|                           | 8485/12540 [4:36:29<2:09:10,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 17:50:43,487 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 68%|                           | 8526/12540 [4:37:47<2:09:25,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 17:52:01,523 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 69%|                          | 8609/12540 [4:40:24<2:02:04,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 17:54:38,708 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 69%|                          | 8653/12540 [4:41:48<2:02:50,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 17:56:02,129 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 69%|                         | 8714/12540 [4:43:43<1:59:06,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 17:57:57,280 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 70%|                         | 8756/12540 [4:45:02<1:57:02,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 17:59:16,194 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 70%|                         | 8760/12540 [4:45:09<1:57:00,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 17:59:23,660 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 70%|                         | 8765/12540 [4:45:18<1:55:28,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 17:59:32,777 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 70%|                         | 8767/12540 [4:45:22<1:58:18,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 17:59:36,642 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 70%|                         | 8778/12540 [4:45:43<1:58:00,  1.88s/it][INFO|trainer.py:528] 2022-06-08 17:59:57,092 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 17:59:57,096 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 17:59:57,109 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 17:59:57,109 >>   Batch size = 2
 70%|                         | 8802/12540 [4:47:12<1:55:58,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 18:01:26,576 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 71%|                        | 8850/12540 [4:48:42<1:54:40,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 18:02:56,873 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 71%|                        | 8868/12540 [4:49:16<1:54:22,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 18:03:30,438 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 71%|                        | 8935/12540 [4:51:22<1:54:10,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 18:05:36,528 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 71%|                        | 8955/12540 [4:52:00<1:49:39,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 18:06:14,037 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 71%|                        | 8960/12540 [4:52:09<1:52:34,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 18:06:23,567 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 72%|                        | 8968/12540 [4:52:24<1:48:35,  1.82s/it][INFO|modeling_led.py:1663] 2022-06-08 18:06:38,316 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 72%|                       | 8977/12540 [4:52:41<1:51:10,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 18:06:55,247 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 72%|                       | 9014/12540 [4:53:51<1:50:11,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:08:05,261 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 72%|                       | 9050/12540 [4:54:58<1:47:50,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 18:09:12,828 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 72%|                       | 9091/12540 [4:56:16<1:47:49,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:10:30,082 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 73%|                       | 9097/12540 [4:56:27<1:47:36,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:10:41,513 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 73%|                       | 9110/12540 [4:56:51<1:44:40,  1.83s/it][INFO|modeling_led.py:1663] 2022-06-08 18:11:05,634 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 73%|                       | 9112/12540 [4:56:55<1:45:43,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 18:11:09,397 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 73%|                      | 9153/12540 [4:58:13<1:45:45,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 18:12:27,415 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 74%|                      | 9236/12540 [5:00:48<1:41:35,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 18:15:02,817 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 74%|                     | 9280/12540 [5:02:12<1:43:42,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 18:16:26,181 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 74%|                     | 9341/12540 [5:04:06<1:37:42,  1.83s/it][INFO|modeling_led.py:1663] 2022-06-08 18:18:20,479 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 75%|                     | 9383/12540 [5:05:26<1:40:42,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 18:19:39,891 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 75%|                     | 9387/12540 [5:05:33<1:39:33,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 18:19:47,541 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 75%|                     | 9392/12540 [5:05:43<1:40:21,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 18:19:57,052 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 75%|                     | 9394/12540 [5:05:46<1:39:44,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 18:20:00,832 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 75%|                     | 9405/12540 [5:06:07<1:39:29,  1.90s/it][INFO|trainer.py:528] 2022-06-08 18:20:21,567 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 18:20:21,571 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 18:20:21,584 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 18:20:21,584 >>   Batch size = 2
 75%|                    | 9429/12540 [5:07:36<1:38:15,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 18:21:50,376 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 76%|                    | 9477/12540 [5:09:05<1:36:56,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 18:23:19,761 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 76%|                    | 9495/12540 [5:09:39<1:33:30,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 18:23:53,280 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 76%|                    | 9562/12540 [5:11:44<1:31:49,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 18:25:58,030 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 76%|                   | 9582/12540 [5:12:21<1:34:14,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 18:26:35,682 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 76%|                   | 9587/12540 [5:12:31<1:31:32,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 18:26:44,951 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 77%|                   | 9595/12540 [5:12:46<1:32:36,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 18:26:59,909 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 77%|                   | 9604/12540 [5:13:02<1:29:28,  1.83s/it][INFO|modeling_led.py:1663] 2022-06-08 18:27:16,425 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 77%|                   | 9641/12540 [5:14:11<1:29:13,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 18:28:25,894 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 77%|                   | 9677/12540 [5:15:19<1:30:41,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 18:29:33,802 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 77%|                   | 9718/12540 [5:16:37<1:29:20,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 18:30:51,528 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 78%|                  | 9724/12540 [5:16:49<1:29:35,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 18:31:02,975 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 78%|                  | 9737/12540 [5:17:13<1:27:54,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:31:27,619 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 78%|                  | 9739/12540 [5:17:17<1:27:58,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:31:31,361 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 78%|                  | 9780/12540 [5:18:34<1:25:02,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 18:32:48,514 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 79%|                  | 9863/12540 [5:21:11<1:25:37,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 18:35:25,360 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 79%|                 | 9907/12540 [5:22:33<1:22:29,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:36:47,792 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 79%|                 | 9968/12540 [5:24:29<1:20:34,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:38:42,968 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 80%|                | 10010/12540 [5:25:48<1:19:23,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:40:01,965 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 80%|                | 10014/12540 [5:25:55<1:18:16,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 18:40:09,392 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 80%|                | 10019/12540 [5:26:04<1:18:54,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:40:18,862 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 80%|                | 10021/12540 [5:26:08<1:17:42,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 18:40:22,505 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 80%|                | 10032/12540 [5:26:29<1:20:42,  1.93s/it][INFO|trainer.py:528] 2022-06-08 18:40:43,436 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 18:40:43,454 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 18:40:43,454 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 18:40:43,455 >>   Batch size = 2
 80%|                | 10056/12540 [5:27:58<1:18:36,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 18:42:12,315 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 81%|                | 10104/12540 [5:29:28<1:17:15,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 18:43:42,459 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 81%|                | 10122/12540 [5:30:02<1:17:16,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 18:44:16,177 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 81%|               | 10189/12540 [5:32:08<1:13:01,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 18:46:22,221 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 81%|               | 10209/12540 [5:32:46<1:13:02,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:47:00,122 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 81%|               | 10214/12540 [5:32:55<1:13:07,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 18:47:09,358 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 82%|               | 10222/12540 [5:33:10<1:12:41,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:47:24,356 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 82%|               | 10231/12540 [5:33:27<1:12:21,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:47:41,165 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 82%|               | 10268/12540 [5:34:36<1:10:05,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 18:48:50,839 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 82%|              | 10304/12540 [5:35:45<1:10:28,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 18:49:59,010 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 82%|              | 10345/12540 [5:37:02<1:08:44,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 18:51:16,334 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 83%|              | 10351/12540 [5:37:13<1:09:54,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 18:51:27,821 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 83%|              | 10364/12540 [5:37:37<1:06:53,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 18:51:51,889 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 83%|              | 10366/12540 [5:37:42<1:10:17,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 18:51:55,995 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 83%|              | 10407/12540 [5:38:59<1:06:04,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 18:53:13,185 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 84%|             | 10490/12540 [5:41:35<1:03:54,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 18:55:49,062 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 84%|             | 10534/12540 [5:42:57<1:02:15,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 18:57:11,856 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 84%|            | 10595/12540 [5:44:52<1:01:45,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 18:59:06,308 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 85%|             | 10637/12540 [5:46:11<59:33,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:00:25,195 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 85%|            | 10641/12540 [5:46:18<59:57,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:00:32,735 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 85%|            | 10646/12540 [5:46:28<1:00:17,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 19:00:42,174 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 85%|            | 10648/12540 [5:46:31<58:54,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 19:00:45,856 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 85%|            | 10659/12540 [5:46:52<59:27,  1.90s/it][INFO|trainer.py:528] 2022-06-08 19:01:06,531 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 19:01:06,535 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 19:01:06,542 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 19:01:06,542 >>   Batch size = 2
 85%|            | 10683/12540 [5:48:21<58:09,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:02:35,827 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 86%|            | 10731/12540 [5:49:51<56:44,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:04:05,774 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 86%|            | 10749/12540 [5:50:25<56:20,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:04:39,703 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 86%|           | 10816/12540 [5:52:31<54:44,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 19:06:45,307 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 86%|           | 10836/12540 [5:53:08<52:22,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 19:07:22,750 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 86%|           | 10841/12540 [5:53:18<53:27,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:07:32,161 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 87%|           | 10849/12540 [5:53:33<52:34,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 19:07:46,954 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 87%|           | 10858/12540 [5:53:50<53:09,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:08:03,899 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 87%|           | 10895/12540 [5:54:59<50:23,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 19:09:13,474 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 87%|           | 10931/12540 [5:56:06<50:21,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:10:20,810 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 87%|          | 10972/12540 [5:57:24<50:08,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 19:11:38,614 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 88%|          | 10978/12540 [5:57:36<49:42,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 19:11:50,077 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 88%|          | 10991/12540 [5:58:00<47:55,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 19:12:14,416 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 88%|          | 10993/12540 [5:58:04<48:10,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 19:12:18,176 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 88%|          | 11034/12540 [5:59:21<46:26,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 19:13:35,849 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 89%|         | 11117/12540 [6:01:58<43:48,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 19:16:12,378 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 89%|         | 11161/12540 [6:03:21<43:13,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:17:34,958 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 89%|         | 11222/12540 [6:05:15<40:04,  1.82s/it][INFO|modeling_led.py:1663] 2022-06-08 19:19:29,519 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 90%|        | 11264/12540 [6:06:34<40:19,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:20:48,728 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 90%|        | 11268/12540 [6:06:42<39:15,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 19:20:56,025 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 90%|        | 11273/12540 [6:06:51<39:23,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 19:21:05,392 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 90%|        | 11275/12540 [6:06:55<39:48,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:21:09,221 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 90%|        | 11286/12540 [6:07:16<39:12,  1.88s/it][INFO|trainer.py:528] 2022-06-08 19:21:30,095 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 19:21:30,112 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 19:21:30,112 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 19:21:30,113 >>   Batch size = 2
 90%|        | 11310/12540 [6:08:44<38:21,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 19:22:58,661 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 91%|        | 11358/12540 [6:10:15<36:59,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:24:29,228 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 91%|        | 11376/12540 [6:10:48<35:46,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 19:25:02,810 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 91%|       | 11443/12540 [6:12:54<34:29,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:27:08,740 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 91%|       | 11463/12540 [6:13:32<34:02,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:27:46,444 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 91%|       | 11468/12540 [6:13:41<33:09,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 19:27:55,724 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 92%|       | 11476/12540 [6:13:56<32:24,  1.83s/it][INFO|modeling_led.py:1663] 2022-06-08 19:28:10,543 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 92%|       | 11485/12540 [6:14:13<33:11,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:28:27,750 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 92%|       | 11522/12540 [6:15:23<32:00,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:29:37,699 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 92%|      | 11558/12540 [6:16:31<31:10,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 19:30:45,747 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 92%|      | 11599/12540 [6:17:49<29:51,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:32:03,111 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 93%|      | 11605/12540 [6:18:00<29:37,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:32:14,434 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 93%|      | 11618/12540 [6:18:25<29:24,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 19:32:39,324 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 93%|      | 11620/12540 [6:18:29<29:05,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:32:43,063 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 93%|      | 11661/12540 [6:19:46<27:51,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:34:00,072 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 94%|     | 11744/12540 [6:22:21<24:38,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 19:36:35,590 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 94%|     | 11788/12540 [6:23:45<23:36,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:37:58,910 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 94%|    | 11849/12540 [6:25:40<21:27,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 19:39:54,473 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
 95%|    | 11891/12540 [6:26:59<20:28,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:41:13,401 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
 95%|    | 11895/12540 [6:27:06<19:48,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 19:41:20,740 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
 95%|    | 11900/12540 [6:27:16<20:20,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 19:41:30,281 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
 95%|    | 11902/12540 [6:27:20<19:52,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 19:41:33,957 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 95%|    | 11913/12540 [6:27:40<19:37,  1.88s/it][INFO|trainer.py:528] 2022-06-08 19:41:54,247 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 19:41:54,251 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 19:41:54,251 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 19:41:54,252 >>   Batch size = 2
 95%|    | 11937/12540 [6:29:09<19:13,  1.91s/it][INFO|modeling_led.py:1663] 2022-06-08 19:43:22,962 >> Input ids are automatically padded from 2669 to 3072 to be a multiple of `config.attention_window`: 1024
 96%|   | 11985/12540 [6:30:39<17:24,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:44:53,073 >> Input ids are automatically padded from 3592 to 4096 to be a multiple of `config.attention_window`: 1024
 96%|   | 12003/12540 [6:31:12<16:18,  1.82s/it][INFO|modeling_led.py:1663] 2022-06-08 19:45:26,704 >> Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024
 96%|   | 12070/12540 [6:33:18<15:00,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 19:47:32,831 >> Input ids are automatically padded from 4074 to 4096 to be a multiple of `config.attention_window`: 1024
 96%|   | 12090/12540 [6:33:56<14:12,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:48:10,508 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
 96%|   | 12095/12540 [6:34:06<13:55,  1.88s/it][INFO|modeling_led.py:1663] 2022-06-08 19:48:19,978 >> Input ids are automatically padded from 3487 to 4096 to be a multiple of `config.attention_window`: 1024
 97%|   | 12103/12540 [6:34:21<13:50,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:48:35,371 >> Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024
 97%|   | 12112/12540 [6:34:38<13:50,  1.94s/it][INFO|modeling_led.py:1663] 2022-06-08 19:48:52,497 >> Input ids are automatically padded from 3014 to 3072 to be a multiple of `config.attention_window`: 1024
 97%|  | 12149/12540 [6:35:47<11:59,  1.84s/it][INFO|modeling_led.py:1663] 2022-06-08 19:50:01,723 >> Input ids are automatically padded from 2673 to 3072 to be a multiple of `config.attention_window`: 1024
 97%|  | 12185/12540 [6:36:56<11:03,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 19:51:10,014 >> Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
 97%|  | 12226/12540 [6:38:13<09:45,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 19:52:26,990 >> Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024
 98%|  | 12232/12540 [6:38:24<09:22,  1.83s/it][INFO|modeling_led.py:1663] 2022-06-08 19:52:38,062 >> Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024
 98%|  | 12245/12540 [6:38:48<09:09,  1.86s/it][INFO|modeling_led.py:1663] 2022-06-08 19:53:02,533 >> Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
 98%|  | 12247/12540 [6:38:52<09:08,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 19:53:06,317 >> Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024
 98%| | 12288/12540 [6:40:10<07:55,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 19:54:23,972 >> Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024
 99%| | 12371/12540 [6:42:46<05:24,  1.92s/it][INFO|modeling_led.py:1663] 2022-06-08 19:57:00,174 >> Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
 99%|| 12415/12540 [6:44:09<03:57,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 19:58:23,328 >> Input ids are automatically padded from 3010 to 3072 to be a multiple of `config.attention_window`: 1024
 99%|| 12476/12540 [6:46:04<01:58,  1.85s/it][INFO|modeling_led.py:1663] 2022-06-08 20:00:18,635 >> Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024
100%|| 12518/12540 [6:47:23<00:41,  1.90s/it][INFO|modeling_led.py:1663] 2022-06-08 20:01:37,611 >> Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
100%|| 12522/12540 [6:47:31<00:34,  1.89s/it][INFO|modeling_led.py:1663] 2022-06-08 20:01:45,159 >> Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024
100%|| 12527/12540 [6:47:40<00:24,  1.87s/it][INFO|modeling_led.py:1663] 2022-06-08 20:01:54,493 >> Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
100%|| 12529/12540 [6:47:44<00:21,  1.93s/it][INFO|modeling_led.py:1663] 2022-06-08 20:01:58,473 >> Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024
100%|| 12540/12540 [6:48:05<00:00,  1.92s/it][INFO|trainer.py:528] 2022-06-08 20:02:19,287 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 20:02:19,291 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 20:02:19,292 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 20:02:19,292 >>   Batch size = 2
100%|| 12540/12540 [6:48:48<00:00,  1.92s/it][INFO|trainer.py:1507] 2022-06-08 20:03:02,691 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|| 12540/12540 [6:48:48<00:00,  1.96s/it]
[INFO|trainer.py:2072] 2022-06-08 20:03:02,739 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/allenai-led-base-16384_global_4096_1_1e-05_4096_scrolls_qmsum_demand-mention-28
[INFO|configuration_utils.py:391] 2022-06-08 20:03:02,742 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/allenai-led-base-16384_global_4096_1_1e-05_4096_scrolls_qmsum_demand-mention-28/config.json
[INFO|modeling_utils.py:1001] 2022-06-08 20:03:12,507 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/allenai-led-base-16384_global_4096_1_1e-05_4096_scrolls_qmsum_demand-mention-28/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-08 20:03:12,511 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/allenai-led-base-16384_global_4096_1_1e-05_4096_scrolls_qmsum_demand-mention-28/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-08 20:03:12,513 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/allenai-led-base-16384_global_4096_1_1e-05_4096_scrolls_qmsum_demand-mention-28/special_tokens_map.json
2022-06-08 20:03:13 | INFO | __main__ | *** Evaluate ***
[INFO|trainer.py:528] 2022-06-08 20:03:13,012 >> The following columns in the evaluation set  don't have a corresponding argument in `LEDForConditionalGeneration.forward` and have been ignored: not_valid_for_eval, length.
[INFO|trainer.py:2324] 2022-06-08 20:03:13,016 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-08 20:03:13,017 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-08 20:03:13,017 >>   Batch size = 2
100%|| 68/68 [00:42<00:00,  1.60it/s]
