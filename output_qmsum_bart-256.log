{'padding_count': 0, 'loss': 4.9228, 'learning_rate': 1e-05, 'epoch': 0.25}
{'padding_count': 0, 'loss': 4.2144, 'learning_rate': 2.25e-05, 'epoch': 0.5}
{'padding_count': 0, 'loss': 3.8926, 'learning_rate': 3.5e-05, 'epoch': 0.75}
{'padding_count': 0, 'loss': 3.7254, 'learning_rate': 4.75e-05, 'epoch': 1.0}
{'eval_loss': 3.4034085273742676, 'eval_runtime': 0.9475, 'eval_samples_per_second': 287.084, 'eval_steps_per_second': 14.776, 'epoch': 1.0}
{'padding_count': 0, 'loss': 3.6197, 'learning_rate': 6e-05, 'epoch': 1.25}
{'padding_count': 0, 'loss': 3.5957, 'learning_rate': 7.25e-05, 'epoch': 1.5}
{'padding_count': 0, 'loss': 3.4436, 'learning_rate': 8.5e-05, 'epoch': 1.75}
{'padding_count': 0, 'loss': 3.3653, 'learning_rate': 9.75e-05, 'epoch': 2.0}
{'eval_loss': 3.253948450088501, 'eval_runtime': 0.9485, 'eval_samples_per_second': 286.759, 'eval_steps_per_second': 14.76, 'epoch': 2.0}
{'padding_count': 0, 'loss': 3.2756, 'learning_rate': 9.888888888888889e-05, 'epoch': 2.25}
{'padding_count': 0, 'loss': 3.3025, 'learning_rate': 9.75e-05, 'epoch': 2.5}
{'padding_count': 0, 'loss': 3.131, 'learning_rate': 9.611111111111112e-05, 'epoch': 2.75}
{'padding_count': 0, 'loss': 3.0558, 'learning_rate': 9.472222222222222e-05, 'epoch': 3.0}
{'eval_loss': 3.2026424407958984, 'eval_runtime': 0.9514, 'eval_samples_per_second': 285.881, 'eval_steps_per_second': 14.714, 'epoch': 3.0}
{'padding_count': 0, 'loss': 2.9609, 'learning_rate': 9.333333333333334e-05, 'epoch': 3.25}
{'padding_count': 0, 'loss': 3.0449, 'learning_rate': 9.194444444444445e-05, 'epoch': 3.5}
{'padding_count': 0, 'loss': 2.869, 'learning_rate': 9.055555555555556e-05, 'epoch': 3.75}
{'padding_count': 0, 'loss': 2.829, 'learning_rate': 8.916666666666667e-05, 'epoch': 4.0}
{'eval_loss': 3.2090184688568115, 'eval_runtime': 0.955, 'eval_samples_per_second': 284.814, 'eval_steps_per_second': 14.66, 'epoch': 4.0}
{'padding_count': 0, 'loss': 2.7098, 'learning_rate': 8.777777777777778e-05, 'epoch': 4.25}
{'padding_count': 0, 'loss': 2.8365, 'learning_rate': 8.63888888888889e-05, 'epoch': 4.5}
{'padding_count': 0, 'loss': 2.6874, 'learning_rate': 8.5e-05, 'epoch': 4.75}
{'padding_count': 0, 'loss': 2.6578, 'learning_rate': 8.361111111111111e-05, 'epoch': 5.0}
{'eval_loss': 3.222593069076538, 'eval_runtime': 0.9829, 'eval_samples_per_second': 276.743, 'eval_steps_per_second': 14.244, 'epoch': 5.0}
{'padding_count': 0, 'loss': 2.5199, 'learning_rate': 8.222222222222222e-05, 'epoch': 5.25}
{'padding_count': 0, 'loss': 2.6537, 'learning_rate': 8.083333333333334e-05, 'epoch': 5.5}
{'padding_count': 0, 'loss': 2.5077, 'learning_rate': 7.944444444444444e-05, 'epoch': 5.75}
{'padding_count': 0, 'loss': 2.4903, 'learning_rate': 7.819444444444445e-05, 'epoch': 6.0}
{'eval_loss': 3.2394659519195557, 'eval_runtime': 0.9664, 'eval_samples_per_second': 281.464, 'eval_steps_per_second': 14.487, 'epoch': 6.0}
{'padding_count': 0, 'loss': 2.3576, 'learning_rate': 7.680555555555556e-05, 'epoch': 6.25}
{'padding_count': 0, 'loss': 2.4979, 'learning_rate': 7.541666666666667e-05, 'epoch': 6.5}
{'padding_count': 0, 'loss': 2.3623, 'learning_rate': 7.402777777777779e-05, 'epoch': 6.75}
{'padding_count': 0, 'loss': 2.362, 'learning_rate': 7.263888888888889e-05, 'epoch': 7.0}
{'eval_loss': 3.3015003204345703, 'eval_runtime': 0.9792, 'eval_samples_per_second': 277.766, 'eval_steps_per_second': 14.297, 'epoch': 7.0}
{'padding_count': 0, 'loss': 2.2079, 'learning_rate': 7.125000000000001e-05, 'epoch': 7.25}
{'padding_count': 0, 'loss': 2.371, 'learning_rate': 6.986111111111112e-05, 'epoch': 7.5}
{'padding_count': 0, 'loss': 2.2185, 'learning_rate': 6.847222222222222e-05, 'epoch': 7.75}
{'padding_count': 0, 'loss': 2.235, 'learning_rate': 6.708333333333333e-05, 'epoch': 8.0}
{'eval_loss': 3.3406667709350586, 'eval_runtime': 0.9572, 'eval_samples_per_second': 284.155, 'eval_steps_per_second': 14.626, 'epoch': 8.0}
{'padding_count': 0, 'loss': 2.074, 'learning_rate': 6.569444444444445e-05, 'epoch': 8.25}
{'padding_count': 0, 'loss': 2.2419, 'learning_rate': 6.430555555555557e-05, 'epoch': 8.5}
{'padding_count': 0, 'loss': 2.0991, 'learning_rate': 6.291666666666667e-05, 'epoch': 8.75}
{'padding_count': 0, 'loss': 2.1077, 'learning_rate': 6.152777777777778e-05, 'epoch': 9.0}
{'eval_loss': 3.4073073863983154, 'eval_runtime': 0.9547, 'eval_samples_per_second': 284.905, 'eval_steps_per_second': 14.664, 'epoch': 9.0}
{'padding_count': 0, 'loss': 1.9344, 'learning_rate': 6.013888888888889e-05, 'epoch': 9.25}
{'padding_count': 0, 'loss': 2.1167, 'learning_rate': 5.8750000000000005e-05, 'epoch': 9.5}
{'padding_count': 0, 'loss': 1.9855, 'learning_rate': 5.736111111111111e-05, 'epoch': 9.75}
{'padding_count': 0, 'loss': 1.9941, 'learning_rate': 5.5972222222222224e-05, 'epoch': 10.0}
{'eval_loss': 3.4517085552215576, 'eval_runtime': 0.9889, 'eval_samples_per_second': 275.041, 'eval_steps_per_second': 14.157, 'epoch': 10.0}
{'padding_count': 0, 'loss': 1.8365, 'learning_rate': 5.458333333333333e-05, 'epoch': 10.25}
{'padding_count': 0, 'loss': 1.9978, 'learning_rate': 5.319444444444445e-05, 'epoch': 10.5}
{'padding_count': 0, 'loss': 1.8982, 'learning_rate': 5.180555555555556e-05, 'epoch': 10.75}
{'padding_count': 0, 'loss': 1.8752, 'learning_rate': 5.041666666666667e-05, 'epoch': 11.0}
{'eval_loss': 3.511864185333252, 'eval_runtime': 0.9548, 'eval_samples_per_second': 284.888, 'eval_steps_per_second': 14.663, 'epoch': 11.0}
{'padding_count': 0, 'loss': 1.7306, 'learning_rate': 4.902777777777778e-05, 'epoch': 11.25}
{'padding_count': 0, 'loss': 1.9011, 'learning_rate': 4.7638888888888887e-05, 'epoch': 11.5}
{'padding_count': 0, 'loss': 1.7912, 'learning_rate': 4.6250000000000006e-05, 'epoch': 11.75}
{'padding_count': 0, 'loss': 1.79, 'learning_rate': 4.486111111111111e-05, 'epoch': 12.0}
{'eval_loss': 3.5581531524658203, 'eval_runtime': 0.9729, 'eval_samples_per_second': 279.586, 'eval_steps_per_second': 14.39, 'epoch': 12.0}
{'padding_count': 0, 'loss': 1.6394, 'learning_rate': 4.3472222222222225e-05, 'epoch': 12.25}
{'padding_count': 0, 'loss': 1.8235, 'learning_rate': 4.208333333333334e-05, 'epoch': 12.5}
{'padding_count': 0, 'loss': 1.7126, 'learning_rate': 4.0694444444444444e-05, 'epoch': 12.75}
{'padding_count': 0, 'loss': 1.7182, 'learning_rate': 3.9305555555555556e-05, 'epoch': 13.0}
{'eval_loss': 3.593632936477661, 'eval_runtime': 0.9862, 'eval_samples_per_second': 275.819, 'eval_steps_per_second': 14.197, 'epoch': 13.0}
{'padding_count': 0, 'loss': 1.5638, 'learning_rate': 3.791666666666667e-05, 'epoch': 13.25}
{'padding_count': 0, 'loss': 1.7495, 'learning_rate': 3.6527777777777775e-05, 'epoch': 13.5}
{'padding_count': 0, 'loss': 1.6476, 'learning_rate': 3.513888888888889e-05, 'epoch': 13.75}
{'padding_count': 0, 'loss': 1.6364, 'learning_rate': 3.375000000000001e-05, 'epoch': 14.0}
{'eval_loss': 3.612968921661377, 'eval_runtime': 0.9584, 'eval_samples_per_second': 283.802, 'eval_steps_per_second': 14.607, 'epoch': 14.0}
{'padding_count': 0, 'loss': 1.4886, 'learning_rate': 3.236111111111111e-05, 'epoch': 14.25}
{'padding_count': 0, 'loss': 1.6649, 'learning_rate': 3.0972222222222226e-05, 'epoch': 14.5}
{'padding_count': 0, 'loss': 1.5607, 'learning_rate': 2.9583333333333335e-05, 'epoch': 14.75}
{'padding_count': 0, 'loss': 1.5657, 'learning_rate': 2.8194444444444445e-05, 'epoch': 15.0}
{'eval_loss': 3.655233383178711, 'eval_runtime': 0.9683, 'eval_samples_per_second': 280.904, 'eval_steps_per_second': 14.458, 'epoch': 15.0}
{'padding_count': 0, 'loss': 1.4292, 'learning_rate': 2.6805555555555557e-05, 'epoch': 15.25}
{'padding_count': 0, 'loss': 1.6123, 'learning_rate': 2.5416666666666667e-05, 'epoch': 15.5}
{'padding_count': 0, 'loss': 1.5081, 'learning_rate': 2.402777777777778e-05, 'epoch': 15.75}
{'padding_count': 0, 'loss': 1.5048, 'learning_rate': 2.263888888888889e-05, 'epoch': 16.0}
{'eval_loss': 3.6979854106903076, 'eval_runtime': 0.9701, 'eval_samples_per_second': 280.398, 'eval_steps_per_second': 14.432, 'epoch': 16.0}
{'padding_count': 0, 'loss': 1.3679, 'learning_rate': 2.125e-05, 'epoch': 16.25}
{'padding_count': 0, 'loss': 1.5696, 'learning_rate': 1.986111111111111e-05, 'epoch': 16.5}
{'padding_count': 0, 'loss': 1.4713, 'learning_rate': 1.8472222222222224e-05, 'epoch': 16.75}
{'padding_count': 0, 'loss': 1.4578, 'learning_rate': 1.7083333333333333e-05, 'epoch': 17.0}
{'eval_loss': 3.7203049659729004, 'eval_runtime': 0.9583, 'eval_samples_per_second': 283.848, 'eval_steps_per_second': 14.61, 'epoch': 17.0}
{'padding_count': 0, 'loss': 1.3323, 'learning_rate': 1.5694444444444446e-05, 'epoch': 17.25}
{'padding_count': 0, 'loss': 1.5259, 'learning_rate': 1.4305555555555555e-05, 'epoch': 17.5}
{'padding_count': 0, 'loss': 1.4157, 'learning_rate': 1.2916666666666668e-05, 'epoch': 17.75}
{'padding_count': 0, 'loss': 1.4267, 'learning_rate': 1.1527777777777779e-05, 'epoch': 18.0}
{'eval_loss': 3.7317733764648438, 'eval_runtime': 0.9789, 'eval_samples_per_second': 277.859, 'eval_steps_per_second': 14.302, 'epoch': 18.0}
{'padding_count': 0, 'loss': 1.2872, 'learning_rate': 1.013888888888889e-05, 'epoch': 18.25}
{'padding_count': 0, 'loss': 1.4744, 'learning_rate': 8.75e-06, 'epoch': 18.5}
{'padding_count': 0, 'loss': 1.3902, 'learning_rate': 7.361111111111112e-06, 'epoch': 18.75}
{'padding_count': 0, 'loss': 1.3941, 'learning_rate': 5.972222222222223e-06, 'epoch': 19.0}
{'eval_loss': 3.7390804290771484, 'eval_runtime': 0.9571, 'eval_samples_per_second': 284.206, 'eval_steps_per_second': 14.628, 'epoch': 19.0}
{'padding_count': 0, 'loss': 1.2718, 'learning_rate': 4.583333333333333e-06, 'epoch': 19.25}
{'padding_count': 0, 'loss': 1.4643, 'learning_rate': 3.1944444444444443e-06, 'epoch': 19.5}
{'padding_count': 0, 'loss': 1.3708, 'learning_rate': 1.8055555555555555e-06, 'epoch': 19.75}
{'padding_count': 0, 'loss': 1.3708, 'learning_rate': 4.1666666666666667e-07, 'epoch': 20.0}
{'eval_loss': 3.7241744995117188, 'eval_runtime': 0.9609, 'eval_samples_per_second': 283.07, 'eval_steps_per_second': 14.57, 'epoch': 20.0}
{'train_runtime': 751.4181, 'train_samples_per_second': 33.457, 'train_steps_per_second': 1.065, 'train_loss': 2.1839250242710113, 'epoch': 20.0}
***** train metrics *****
  epoch                    =       20.0
  train_loss               =     2.1839
  train_runtime            = 0:12:31.41
  train_samples            =       1257
  train_samples_per_second =     33.457
  train_steps_per_second   =      1.065
***** eval metrics *****
  epoch                   =       20.0
  eval_loss               =     3.2026
  eval_runtime            = 0:00:00.99
  eval_samples            =        272
  eval_samples_per_second =    273.428
  eval_steps_per_second   =     14.073
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                                     | 0/800 [00:00<?, ?it/s][INFO|trainer.py:1411] 2022-06-06 16:30:19,984 >> Unused parameters:
/home/s1970716/miniconda3/envs/scrolls_venv/lib/python3.8/site-packages/transformers/trainer.py:1444: FutureWarning:
Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  0%|                                                                                             | 1/800 [00:01<21:09,  1.59s/it]2022-06-06 16:30:20 | INFO | root | Reducer buckets have been rebuilt in this iteration.
  5%|████▌                                                                                       | 40/800 [00:18<04:50,  2.62it/s][INFO|trainer.py:528] 2022-06-06 16:30:37,087 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:30:37,092 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:30:37,092 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:30:37,092 >>   Batch size = 10
  5%|████▌                                                                                       | 40/800 [00:19<04:50,  2.62it/s][INFO|trainer.py:2072] 2022-06-06 16:30:38,045 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-40
[INFO|configuration_utils.py:391] 2022-06-06 16:30:38,053 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-40/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:30:44,484 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-40/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:30:44,487 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:30:44,489 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-40/special_tokens_map.json
 10%|█████████▏                                                                                  | 80/800 [00:52<04:17,  2.80it/s][INFO|trainer.py:528] 2022-06-06 16:31:10,658 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:31:10,662 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:31:10,662 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:31:10,662 >>   Batch size = 10
 10%|█████████▏                                                                                  | 80/800 [00:53<04:17,  2.80it/s][INFO|trainer.py:2072] 2022-06-06 16:31:11,615 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-80
[INFO|configuration_utils.py:391] 2022-06-06 16:31:11,619 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-80/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:31:17,888 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:31:17,892 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:31:17,893 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-80/special_tokens_map.json
 15%|█████████████▋                                                                             | 120/800 [01:25<04:05,  2.77it/s][INFO|trainer.py:528] 2022-06-06 16:31:44,055 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:31:44,059 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:31:44,059 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:31:44,059 >>   Batch size = 10
 15%|█████████████▋                                                                             | 120/800 [01:26<04:05,  2.77it/s][INFO|trainer.py:2072] 2022-06-06 16:31:45,015 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-120
[INFO|configuration_utils.py:391] 2022-06-06 16:31:45,019 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-120/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:31:51,279 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:31:51,282 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:31:51,284 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-120/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:32:03,939 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-40] due to args.save_total_limit
 20%|██████████████████▏                                                                        | 160/800 [01:59<03:44,  2.85it/s][INFO|trainer.py:528] 2022-06-06 16:32:18,053 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:32:18,056 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:32:18,056 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:32:18,056 >>   Batch size = 10
 20%|██████████████████▏                                                                        | 160/800 [02:00<03:44,  2.85it/s][INFO|trainer.py:2072] 2022-06-06 16:32:19,017 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-160
[INFO|configuration_utils.py:391] 2022-06-06 16:32:19,059 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-160/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:32:25,298 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:32:25,303 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:32:25,305 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-160/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:32:37,879 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-80] due to args.save_total_limit
 25%|██████████████████████▊                                                                    | 200/800 [02:33<03:34,  2.80it/s][INFO|trainer.py:528] 2022-06-06 16:32:52,001 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:32:52,005 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:32:52,006 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:32:52,006 >>   Batch size = 10
 25%|██████████████████████▊                                                                    | 200/800 [02:34<03:34,  2.80it/s][INFO|trainer.py:2072] 2022-06-06 16:32:52,995 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-200
[INFO|configuration_utils.py:391] 2022-06-06 16:32:52,998 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-200/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:32:59,217 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-200/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:32:59,223 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:32:59,225 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-200/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:33:11,847 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-160] due to args.save_total_limit
 29%|██████████████████████████▎                                                                | 231/800 [03:04<03:24,  2.78it/s]/home/s1970716/miniconda3/envs/scrolls_venv/lib/python3.8/site-packages/transformers/trainer.py:1444: FutureWarning:
Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
 30%|███████████████████████████▎                                                               | 240/800 [03:07<03:18,  2.82it/s][INFO|trainer.py:528] 2022-06-06 16:33:26,058 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:33:26,061 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:33:26,061 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:33:26,061 >>   Batch size = 10
 30%|███████████████████████████▎                                                               | 240/800 [03:08<03:18,  2.82it/s][INFO|trainer.py:2072] 2022-06-06 16:33:27,031 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-240
[INFO|configuration_utils.py:391] 2022-06-06 16:33:27,034 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-240/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:33:33,236 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:33:33,238 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:33:33,239 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-240/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:33:45,822 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-200] due to args.save_total_limit
 35%|███████████████████████████████▊                                                           | 280/800 [03:41<03:02,  2.85it/s][INFO|trainer.py:528] 2022-06-06 16:33:59,973 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:33:59,977 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:33:59,977 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:33:59,977 >>   Batch size = 10
 35%|███████████████████████████████▊                                                           | 280/800 [03:42<03:02,  2.85it/s][INFO|trainer.py:2072] 2022-06-06 16:34:00,961 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-280
[INFO|configuration_utils.py:391] 2022-06-06 16:34:00,964 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-280/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:34:07,112 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:34:07,115 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:34:07,117 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-280/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:34:20,200 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-240] due to args.save_total_limit
 40%|████████████████████████████████████▍                                                      | 320/800 [04:15<02:51,  2.80it/s][INFO|trainer.py:528] 2022-06-06 16:34:34,413 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:34:34,416 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:34:34,416 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:34:34,416 >>   Batch size = 10
 40%|████████████████████████████████████▍                                                      | 320/800 [04:16<02:51,  2.80it/s][INFO|trainer.py:2072] 2022-06-06 16:34:35,378 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-320
[INFO|configuration_utils.py:391] 2022-06-06 16:34:35,382 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-320/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:34:41,609 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:34:41,612 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:34:41,614 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-320/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:34:54,230 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-280] due to args.save_total_limit
 45%|████████████████████████████████████████▉                                                  | 360/800 [04:49<02:39,  2.76it/s][INFO|trainer.py:528] 2022-06-06 16:35:08,432 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:35:08,435 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:35:08,435 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:35:08,436 >>   Batch size = 10
 45%|████████████████████████████████████████▉                                                  | 360/800 [04:50<02:39,  2.76it/s][INFO|trainer.py:2072] 2022-06-06 16:35:09,393 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-360
[INFO|configuration_utils.py:391] 2022-06-06 16:35:09,397 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-360/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:35:15,571 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-360/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:35:15,574 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:35:15,576 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-360/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:35:28,263 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-320] due to args.save_total_limit
 50%|█████████████████████████████████████████████▌                                             | 400/800 [05:24<02:22,  2.80it/s][INFO|trainer.py:528] 2022-06-06 16:35:42,557 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:35:42,561 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:35:42,561 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:35:42,561 >>   Batch size = 10
 50%|█████████████████████████████████████████████▌                                             | 400/800 [05:25<02:22,  2.80it/s][INFO|trainer.py:2072] 2022-06-06 16:35:43,553 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-400
[INFO|configuration_utils.py:391] 2022-06-06 16:35:43,557 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-400/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:35:50,055 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:35:50,058 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:35:50,059 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:36:03,274 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-360] due to args.save_total_limit
 55%|██████████████████████████████████████████████████                                         | 440/800 [05:58<02:09,  2.79it/s][INFO|trainer.py:528] 2022-06-06 16:36:17,512 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:36:17,515 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:36:17,516 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:36:17,516 >>   Batch size = 10
 55%|██████████████████████████████████████████████████                                         | 440/800 [05:59<02:09,  2.79it/s][INFO|trainer.py:2072] 2022-06-06 16:36:18,479 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-440
[INFO|configuration_utils.py:391] 2022-06-06 16:36:18,495 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-440/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:36:25,140 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-440/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:36:25,143 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-440/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:36:25,145 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-440/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:36:38,514 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-400] due to args.save_total_limit
 60%|██████████████████████████████████████████████████████▌                                    | 480/800 [06:34<01:55,  2.77it/s][INFO|trainer.py:528] 2022-06-06 16:36:52,645 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:36:52,649 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:36:52,649 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:36:52,649 >>   Batch size = 10
 60%|██████████████████████████████████████████████████████▌                                    | 480/800 [06:35<01:55,  2.77it/s][INFO|trainer.py:2072] 2022-06-06 16:36:53,627 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-480
[INFO|configuration_utils.py:391] 2022-06-06 16:36:53,630 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-480/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:37:00,215 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-480/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:37:00,218 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:37:00,220 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-480/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:37:13,522 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-440] due to args.save_total_limit
 65%|███████████████████████████████████████████████████████████▏                               | 520/800 [07:09<01:38,  2.83it/s][INFO|trainer.py:528] 2022-06-06 16:37:27,672 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:37:27,675 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:37:27,675 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:37:27,675 >>   Batch size = 10
 65%|███████████████████████████████████████████████████████████▏                               | 520/800 [07:10<01:38,  2.83it/s][INFO|trainer.py:2072] 2022-06-06 16:37:28,667 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-520
[INFO|configuration_utils.py:391] 2022-06-06 16:37:28,684 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-520/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:37:35,258 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-520/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:37:35,295 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-520/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:37:35,297 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-520/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:37:48,750 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-480] due to args.save_total_limit
 70%|███████████████████████████████████████████████████████████████▋                           | 560/800 [07:44<01:26,  2.79it/s][INFO|trainer.py:528] 2022-06-06 16:38:02,879 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:38:02,882 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:38:02,882 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:38:02,883 >>   Batch size = 10
 70%|███████████████████████████████████████████████████████████████▋                           | 560/800 [07:45<01:26,  2.79it/s][INFO|trainer.py:2072] 2022-06-06 16:38:03,845 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-560
[INFO|configuration_utils.py:391] 2022-06-06 16:38:03,863 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-560/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:38:10,121 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-560/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:38:10,123 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:38:10,125 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-560/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:38:22,841 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-520] due to args.save_total_limit
 75%|████████████████████████████████████████████████████████████████████▎                      | 600/800 [08:18<01:09,  2.86it/s][INFO|trainer.py:528] 2022-06-06 16:38:36,972 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:38:36,976 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:38:36,976 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:38:36,976 >>   Batch size = 10
 75%|████████████████████████████████████████████████████████████████████▎                      | 600/800 [08:19<01:09,  2.86it/s][INFO|trainer.py:2072] 2022-06-06 16:38:37,949 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-600
[INFO|configuration_utils.py:391] 2022-06-06 16:38:37,959 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-600/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:38:44,223 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:38:44,225 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:38:44,227 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:38:56,905 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-560] due to args.save_total_limit
 80%|████████████████████████████████████████████████████████████████████████▊                  | 640/800 [08:52<00:56,  2.85it/s][INFO|trainer.py:528] 2022-06-06 16:39:11,004 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:39:11,007 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:39:11,007 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:39:11,007 >>   Batch size = 10
 80%|████████████████████████████████████████████████████████████████████████▊                  | 640/800 [08:53<00:56,  2.85it/s][INFO|trainer.py:2072] 2022-06-06 16:39:11,983 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-640
[INFO|configuration_utils.py:391] 2022-06-06 16:39:11,986 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-640/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:39:18,152 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-640/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:39:18,154 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-640/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:39:18,156 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-640/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:39:31,174 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-600] due to args.save_total_limit
 85%|█████████████████████████████████████████████████████████████████████████████▎             | 680/800 [09:26<00:41,  2.88it/s][INFO|trainer.py:528] 2022-06-06 16:39:45,266 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:39:45,269 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:39:45,269 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:39:45,269 >>   Batch size = 10
 85%|█████████████████████████████████████████████████████████████████████████████▎             | 680/800 [09:27<00:41,  2.88it/s][INFO|trainer.py:2072] 2022-06-06 16:39:46,233 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-680
[INFO|configuration_utils.py:391] 2022-06-06 16:39:46,236 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-680/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:39:52,460 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-680/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:39:52,463 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:39:52,465 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-680/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:40:05,238 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-640] due to args.save_total_limit
 90%|█████████████████████████████████████████████████████████████████████████████████▉         | 720/800 [10:00<00:27,  2.86it/s][INFO|trainer.py:528] 2022-06-06 16:40:19,327 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:40:19,330 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:40:19,330 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:40:19,330 >>   Batch size = 10
 90%|█████████████████████████████████████████████████████████████████████████████████▉         | 720/800 [10:01<00:27,  2.86it/s][INFO|trainer.py:2072] 2022-06-06 16:40:20,314 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-720
[INFO|configuration_utils.py:391] 2022-06-06 16:40:20,317 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-720/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:40:26,517 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-720/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:40:26,520 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-720/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:40:26,522 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-720/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:40:39,218 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-680] due to args.save_total_limit
 95%|██████████████████████████████████████████████████████████████████████████████████████▍    | 760/800 [10:34<00:14,  2.84it/s][INFO|trainer.py:528] 2022-06-06 16:40:53,326 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:40:53,329 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:40:53,330 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:40:53,330 >>   Batch size = 10
 95%|██████████████████████████████████████████████████████████████████████████████████████▍    | 760/800 [10:35<00:14,  2.84it/s][INFO|trainer.py:2072] 2022-06-06 16:40:54,292 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-760
[INFO|configuration_utils.py:391] 2022-06-06 16:40:54,295 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-760/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:41:00,480 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-760/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:41:00,484 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-760/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:41:00,485 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-760/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:41:13,224 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-720] due to args.save_total_limit
100%|███████████████████████████████████████████████████████████████████████████████████████████| 800/800 [11:08<00:00,  2.80it/s][INFO|trainer.py:528] 2022-06-06 16:41:27,389 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:41:27,393 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:41:27,393 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:41:27,393 >>   Batch size = 10
100%|███████████████████████████████████████████████████████████████████████████████████████████| 800/800 [11:09<00:00,  2.80it/s][INFO|trainer.py:2072] 2022-06-06 16:41:28,359 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-800
[INFO|configuration_utils.py:391] 2022-06-06 16:41:28,363 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-800/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:41:34,624 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-800/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:41:34,627 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:41:34,628 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-800/special_tokens_map.json
[INFO|trainer.py:2148] 2022-06-06 16:41:47,272 >> Deleting older checkpoint [/disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-760] due to args.save_total_limit
[INFO|trainer.py:1507] 2022-06-06 16:41:47,858 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
[INFO|trainer.py:1515] 2022-06-06 16:41:47,859 >> Loading best model from /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/checkpoint-120 (score: 3.2026424407958984).
100%|███████████████████████████████████████████████████████████████████████████████████████████| 800/800 [11:34<00:00,  1.15it/s]
[INFO|trainer.py:2072] 2022-06-06 16:41:53,264 >> Saving model checkpoint to /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10
[INFO|configuration_utils.py:391] 2022-06-06 16:41:53,267 >> Configuration saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/config.json
[INFO|modeling_utils.py:1001] 2022-06-06 16:41:59,437 >> Model weights saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/pytorch_model.bin
[INFO|tokenization_utils_base.py:2020] 2022-06-06 16:41:59,440 >> tokenizer config file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2026] 2022-06-06 16:41:59,441 >> Special tokens file saved in /disk/nfs/ostrom/s1970716/scrolls/baselines/outputs/facebook-bart-base_256_1_0.0001_4096_scrolls_qmsum_credit-party-10/special_tokens_map.json
2022-06-06 16:41:59 | INFO | __main__ | *** Evaluate ***
[INFO|trainer.py:528] 2022-06-06 16:41:59,658 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: length, not_valid_for_eval.
[INFO|trainer.py:2324] 2022-06-06 16:41:59,662 >> ***** Running Evaluation *****
[INFO|trainer.py:2326] 2022-06-06 16:41:59,662 >>   Num examples = 272
[INFO|trainer.py:2329] 2022-06-06 16:41:59,662 >>   Batch size = 10
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 14.94it/s]